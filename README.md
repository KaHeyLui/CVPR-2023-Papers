# CVPR-2023-Papers
![1ad4f8f92d9208b0f4b579e426b2dcd](https://user-images.githubusercontent.com/62801906/225788627-781870be-cc92-4054-b865-e2556b88cefc.jpg)

# ❣❣❣ CVPR 2023 论文分类整理已完成
# :loudspeaker::loudspeaker::loudspeaker:获奖论文
### :trophy:Best Paper
- [ ]  [Planning-oriented Autonomous Driving](https://ar5iv.org/abs/2212.10156)<br>:house:[project](https://opendrivelab.github.io/UniAD/)
- [ ]  [Visual Programming: Compositional visual reasoning without training](https://ar5iv.org/abs/2211.11559)
### :trophy:Best student Paper
- [ ]  [3D Registration with Maximal Cliques](http://ar5iv.org/abs/2305.10854v1)
### :trophy:Honorable Mention
- [ ]  [DynIBaR: Neural Dynamic Image-Based Rendering](https://ar5iv.org/abs/2211.11082)<br>:house:[project](http://dynibar.github.io/)
### :trophy:Honorable Mention(Student)
- [ ]  [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://ar5iv.org/abs/2208.12242)<br>:house:[project](https://dreambooth.github.io/)

## 历年综述论文分类汇总戳这里↘️[CV-Surveys](https://github.com/52CV/CV-Surveys)施工中~~~~~~~~~~

## 2023 年论文分类汇总戳这里
↘️[CVPR-2023-Papers](https://github.com/52CV/CVPR-2023-Papers)
↘️[WACV-2023-Papers](https://github.com/52CV/WACV-2023-Papers)
↘️[ICCV-2023-Papers](https://github.com/52CV/ICCV-2023-Papers)

## 2022 年论文分类汇总戳这里
↘️[CVPR-2022-Papers](https://github.com/52CV/CVPR-2022-Papers)
↘️[WACV-2022-Papers](https://github.com/52CV/WACV-2022-Papers)
↘️[ECCV-2022-Papers](https://github.com/52CV/ECCV-2022-Papers)

## 2021年论文分类汇总戳这里
↘️[ICCV-2021-Papers](https://github.com/52CV/ICCV-2021-Papers)
↘️[CVPR-2021-Papers](https://github.com/52CV/CVPR-2021-Papers)

## 2020 年论文分类汇总戳这里
↘️[CVPR-2020-Papers](https://github.com/52CV/CVPR-2020-Papers)
↘️[ECCV-2020-Papers](https://github.com/52CV/ECCV-2020-Papers)

## 目录

|:cat:|:dog:|:tiger:|:wolf:|
|------|------|------|------|
|[1.其它](#1)|[2.Image Segmentation(图像分割)](#2)|[3.Image Progress(图像处理)](#4)|[4.Image Captioning(图像字幕)](#)|
|[5.Object Detection(目标检测)](#5)|[6.Object Tracking(目标跟踪)](#6)|[7.Point Cloud(点云)](#7)|[8.Action Detection(人体动作检测与识别)](#8)|
|[9.Human Pose Estimation(人体姿态估计)](#9)|[10.3D(三维视觉)](#10)|[11.Face](#11)|[12.Image-to-Image Translation(图像到图像翻译)](#12)|
|[13.GAN](#13)|[14.Video](#14)|[15.Transformer](#15)|[16.Semi/self-supervised learning(半/自监督)](#16)|
|[17.Medical Image(医学影像)](#17)|[18.Person Re-Identification(人员重识别)](#18)|[19.Neural Architecture Search(神经架构搜索)](#19)|[20.Autonomous vehicles(自动驾驶)](#20)|
|[21.UAV/Remote Sensing/Satellite Image(无人机/遥感/卫星图像)](#21)|[22.Image Synthesis/Generation(图像合成)](#22)|[23.Image Retrieval(图像检索)](#23)|[24.Super-Resolution(超分辨率)](#24)|
|[25.Fine-Grained/Image Classification(细粒度/图像分类)](#25)|[26.GCN/GNN](#26)|[27.Pose Estimation(物体姿势估计)](#27)|[28.Style Transfer(风格迁移)](#28)|
|[29.Augmented Reality/Virtual Reality/Robotics(增强/虚拟现实/机器人)](#29)|[30.Visual Answer Questions(视觉问答)](#30)|[31.Vision-Language(视觉语言)](#31)|[32.Data Augmentation(数据增强)](#32)|
|[33.Human-Object Interaction(人物交互)](#33)|[34.Model Compression/Knowledge Distillation/Pruning(模型压缩/知识蒸馏/剪枝)](#34)|[35.OCR](#35)|[36.Optical Flow(光流估计)](#36)|
|[37.Contrastive Learning(对比学习)](#37)|[38.Meta-Learning(元学习)](#38)|[39.Continual Learning(持续学习)](#39)|[40.Adversarial Learning(对抗学习)](#40)|
|[41.Incremental Learning(增量学习)](#41)|[42.Metric Learning(度量学习)](#42)|[43.Multi-Task Learning(多任务学习)](#43)|[44.Federated Learning(联邦学习)](#44)|
|[45.Dense Prediction(密集预测)](#45)|[46.Scene Graph Generation(场景图生成)](#46)|[47.Few/Zero-Shot Learning/DG/Adaptation(小/零样本/域泛化/适应)](#47)|[48.NLP(自然语言处理)](#48)|
|[49.Image Geo-localization(图像地理定位)](#49)|[50.Anomaly Detection(异常检测)](#50)|[51.光学、几何、光场成像](#51)|[52.Human Motion Forecasting(人体运动预测)](#52)|
|[53.Sign Language Translation(手语翻译)](#53)|[54.Benchmark/Dataset(基准/数据集)](#54)|[55.Novel View Synthesis(视图合成)](#55)|[56.Sound](#56)|
|[57.Gaze Estimation(视线估计)](#57)|[58.Neural rendering(神经渲染)](#58)|[59.Image\Video Compression(图像视频压缩)](#59)|[60.Industrial Anomaly Detection(工业缺陷检测)](#60)|
|[61.Object Re-identification(物体重识别)](#61)|[62.Object Counting(物体计数)](#62)|[63.edge detection(边缘检测)](#63)|[64.Motion Retargeting(动作重定向)](#64)|
|[65.Scene flow estimation(场景流估计)](#65)|[66.Clustering(聚类)](#66)|[67.Active Learning(主动学习)](#67)|[68.Lifelong Learning(终身学习)](#68)|
|[69.Reinforcement learning(强化学习)](#69)|[70.Image Forgery Detection](#70)|[71.visual reasoning(视觉推理)](#71)|[72.open-set recognition(开集识别)](#72)
|[73.Neural Radiance Fields(神经辐射场)](#73)|[74.Machine Learning(机器学习)](#74)|[75.Semantic Scene Completion(语义场景补全)](#75)|[76.IP protection(知识产权保护)](#76)|
|[77.sketch(草图)](#77)|[78.Image/Video Editing(图像/视频编辑)](#78)|[79.thermal imaging technology(热敏成像技术)](#79)|[80.计算机图形学](#80)|


<a name="80"/>

## 80.计算机图形学
- [ ]  [Learning Anchor Transformations for 3D Garment Animation](http://ar5iv.org/abs/2304.00761v1)<br>:star:[code](https://semanticdh.github.io/AnchorDEF)
- [ ]  [Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion](http://ar5iv.org/abs/2304.01893v1)<br>:star:[code](https://nv-tlabs.github.io/trace-pace)
- [ ]  [CloSET: Modeling Clothed Humans on Continuous Surface with Explicit Template Decomposition](http://ar5iv.org/abs/2304.03167v1)<br>:house:[project](https://www.liuyebin.com/closet)
- [ ]  [FLEX: Full-Body Grasping Without Full-Body Grasps](https://openaccess.thecvf.com/content/CVPR2023/papers/Tendulkar_FLEX_Full-Body_Grasping_Without_Full-Body_Grasps_CVPR_2023_paper.pdf)<br>:house:[project](flex.cs.columbia.edu)

<a name="79"/>

## 79.thermal imaging technology(热敏成像技术)
- [ ]  [What Happened 3 Seconds Ago? Inferring the Past with Thermal Imaging](https://ar5iv.org/abs/2304.13651)<br>:star:[code](https://github.com/ZitianTang/Thermal-IM)

<a name="78"/>

## 78.Image/Video Editing(图像/视频编辑)
- [ ]  [PREIM3D: 3D Consistent Precise Image Attribute Editing from a Single Image](https://ar5iv.org/abs/2304.10263)<br>:house:[project](https://mybabyyh.github.io/Preim3D/)
- [ ]  文本驱动的视频编辑
  - [ ]  [Shape-aware Text-driven Layered Video Editing](https://ar5iv.org/abs/2301.13173)<br>:house:[project](https://text-video-edit.github.io/)
- [ ]  Image Editing(图像编辑)
  - [ ]  [CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing](https://ar5iv.org/abs/2303.05031)
  - [ ]  [SIEDOB: Semantic Image Editing by Disentangling Object and Background](http://ar5iv.org/abs/2303.13062v1)
  - [ ]  [NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models](https://ar5iv.org/abs/2211.09794)
  - [ ]  [InstructPix2Pix: Learning To Follow Image Editing Instructions](https://ar5iv.org/abs/2211.09800)<br>:house:[project](https://www.timothybrooks.com/instruct-pix2pix)
  - [ ]  [Local 3D Editing via 3D Distillation of CLIP Knowledge](https://openaccess.thecvf.com/content/CVPR2023/papers/Hyung_Local_3D_Editing_via_3D_Distillation_of_CLIP_Knowledge_CVPR_2023_paper.pdf)
  - [ ]  [Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model](http://ar5iv.org/abs/2211.14573)
  - [ ]  [Imagic: Text-Based Real Image Editing With Diffusion Models](http://ar5iv.org/abs/2210.09276)
  - [ ]  基于样本的图像编辑
    - [ ]  [Paint by Example: Exemplar-based Image Editing with Diffusion Models](https://ar5iv.org/abs/2211.13227)<br>:star:[code](https://github.com/Fantasy-Studio/Paint-by-Example)

<a name="77"/>

## 77.sketch(草图)
- [ ]  [Photo Pre-Training, but for Sketch](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Photo_Pre-Training_but_for_Sketch_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/KeLi-SketchX/Photo-Pre-Training-But-for-Sketch)
- [ ]  [Restoration of Hand-Drawn Architectural Drawings Using Latent Space Mapping With Degradation Generator](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Restoration_of_Hand-Drawn_Architectural_Drawings_Using_Latent_Space_Mapping_With_CVPR_2023_paper.pdf)
- [ ]  [SECAD-Net: Self-Supervised CAD Reconstruction by Learning Sketch-Extrude Operations](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SECAD-Net_Self-Supervised_CAD_Reconstruction_by_Learning_Sketch-Extrude_Operations_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/BunnySoCrazy/SECAD-Net)

<a name="76"/>

## 76.IP protection(知识产权保护)
- [ ]  [Model Barrier: A Compact Un-Transferable Isolation Domain for Model Intellectual Property Protection](https://ar5iv.org/abs/2303.11078)
- [ ]  [Effective Ambiguity Attack Against Passport-Based DNN Intellectual Property Protection Schemes Through Fully Connected Layer Substitution](https://ar5iv.org/abs/2303.11595)

<a name="75"/>

## 75.Semantic Scene Completion(语义场景补全)
- [ ]  [Semantic Scene Completion With Cleaner Self](https://ar5iv.org/abs/2303.09977)
- [ ]  [VoxFormer: Sparse Voxel Transformer for Camera-Based 3D Semantic Scene Completion](https://ar5iv.org/abs/2302.12251)<br>:star:[code](https://github.com/NVlabs/VoxFormer)

<a name="74"/>

## 74.Machine Learning(机器学习)
- [ ]  [Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Cooperation_or_Competition_Avoiding_Player_Domination_for_Multi-Target_Robustness_via_CVPR_2023_paper.pdf )
- [ ]  [Multi-Agent Automated Machine Learning](https://ar5iv.org/abs/2210.09084)
- [ ]  [Towards Better Decision Forests: Forest Alternating Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Carreira-Perpinan_Towards_Better_Decision_Forests_Forest_Alternating_Optimization_CVPR_2023_paper.pdf)
- [ ]  [ERM-KTP: Knowledge-Level Machine Unlearning via Knowledge Transfer](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_ERM-KTP_Knowledge-Level_Machine_Unlearning_via_Knowledge_Transfer_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/RUIYUN-ML/ERM-KTP)
- [ ]  [A Whac-a-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/Whac-A-Mole)
- [ ]  新类别发现
  - [ ]  [Bootstrap Your Own Prior: Towards Distribution-Agnostic Novel Class Discovery](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Bootstrap_Your_Own_Prior_Towards_Distribution-Agnostic_Novel_Class_Discovery_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/muliyangm/BYOP)
  - [ ]  [Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery](http://ar5iv.org/abs/2210.03591)
- [ ]  迁移学习
  - [ ]  [Visual Prompt Tuning for Generative Transfer Learning](https://ar5iv.org/abs/2210.00990)
  - [ ]  [A Data-Based Perspective on Transfer Learning](https://ar5iv.org/abs/2207.05739)<br>:star:[code](https://github.com/MadryLab/data-transfer)
  - [ ]  [Manipulating Transfer Learning for Property Inference](https://ar5iv.org/abs/2303.11643)<br>:star:[code](https://github.com/yulongt23/Transfer-Inference)

<a name="73"/>

## 73.Neural Radiance Fields(神经辐射场)
- [ ]  [Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Ref-NPR_Reference-Based_Non-Photorealistic_Radiance_Fields_for_Controllable_Scene_Stylization_CVPR_2023_paper.pdf)
- [ ]  [Discriminating Known From Unknown Objects via Structure-Enhanced Recurrent Variational AutoEncoder](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Discriminating_Known_From_Unknown_Objects_via_Structure-Enhanced_Recurrent_Variational_AutoEncoder_CVPR_2023_paper.pdf)
- [ ]  [Occlusion-Free Scene Recovery via Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Occlusion-Free_Scene_Recovery_via_Neural_Radiance_Fields_CVPR_2023_paper.pdf)
- [ ]  [Grid-guided Neural Radiance Fields for Large Urban Scenes](https://ar5iv.org/abs/2303.14001)<br>:house:[project](https://city-super.github.io/gridnerf/)
- [ ]  [NeRFLight: Fast and Light Neural Radiance Fields using a Shared Feature Grid](https://openaccess.thecvf.com/content/CVPR2023/papers/Rivas-Manzaneque_NeRFLight_Fast_and_Light_Neural_Radiance_Fields_Using_a_Shared_CVPR_2023_paper.pdf)
- [ ]  [GazeNeRF: 3D-Aware Gaze Redirection With Neural Radiance Fields](https://ar5iv.org/abs/2212.04823)<br>:star:[code](https://github.com/AlessandroRuzzi/GazeNeRF)
- [ ]  [SPARF: Neural Radiance Fields from Sparse and Noisy Poses](https://ar5iv.org/abs/2211.11738)<br>:star:[code](https://github.com/google-research/sparf)
- [ ]  [Masked Wavelet Representation for Compact Neural Radiance Fields](https://ar5iv.org/abs/2212.09069)<br>:star:[code](https://github.com/daniel03c1/masked_wavelet_nerf)
- [ ]  [MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures](https://ar5iv.org/abs/2208.00277)<br>:star:[code](https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf)
- [ ]  [AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training](https://ar5iv.org/abs/2211.09682)<br>:house:[project](https://yifanjiang.net/alignerf)
- [ ]  [JacobiNeRF: NeRF Shaping With Mutual Information Gradients](https://ar5iv.org/abs/2304.00341)
- [ ]  [Robust Dynamic Radiance Fields](https://ar5iv.org/abs/2301.02239)<br>:house:[project](https://robust-dynrf.github.io/)
- [ ]  [Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Isaac-Medina_Exact-NeRF_An_Exploration_of_a_Precise_Volumetric_Parameterization_for_Neural_CVPR_2023_paper.pdf)
- [ ]  [PaletteNeRF: Palette-Based Appearance Editing of Neural Radiance Fields](https://ar5iv.org/abs/2212.10699)
- [ ]  [EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points](https://ar5iv.org/abs/2212.04247)<br>:house:[project](https://chengwei-zheng.github.io/EditableNeRF/)
- [ ]  [SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene](https://ar5iv.org/abs/2211.17260)<br>:house:[project](https://www.computationalimaging.org/publications/singraf/)
- [ ]  [ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision](https://ar5iv.org/abs/2211.14086)<br>:star:[code](https://github.com/gerwang/ShadowNeuS)
- [ ]  [Flow supervision for Deformable NeRF](https://ar5iv.org/abs/2303.16333)
- [ ]  [Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields](https://ar5iv.org/abs/2211.11505)<br>:house:[project](https://rover-xingyu.github.io/L2G-NeRF/)
- [ ]  [EventNeRF: Neural Radiance Fields From a Single Colour Event Camera](https://ar5iv.org/abs/2206.11896)<br>:house:[project](https://4dqv.mpi-inf.mpg.de/EventNeRF)
- [ ]  [SeaThru-NeRF: Neural Radiance Fields in Scattering Media](https://openaccess.thecvf.com/content/CVPR2023/papers/Levy_SeaThru-NeRF_Neural_Radiance_Fields_in_Scattering_Media_CVPR_2023_paper.pdf)
- [ ]  [SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory](https://ar5iv.org/abs/2212.08476)
- [ ]  [Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Complementary_Intrinsics_From_Neural_Radiance_Fields_and_CNNs_for_Outdoor_CVPR_2023_paper.pdf)
- [ ]  [Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields](https://ar5iv.org/abs/2303.16482)
- [ ]  [Removing Objects From Neural Radiance Fields](https://ar5iv.org/abs/2212.11966)
- [ ]  [Grid-guided Neural Radiance Fields for Large Urban Scenes](http://ar5iv.org/abs/2303.14001v1)<br>:star:[code](https://city-super.github.io/gridnerf/)
- [ ]  [GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images](http://ar5iv.org/abs/2303.13777v1)
- [ ]  [HandNeRF: Neural Radiance Fields for Animatable Interacting Hands](http://ar5iv.org/abs/2303.13825v1)
- [ ]  [NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects](http://ar5iv.org/abs/2303.14435v1)<br>:star:[code](https://github.com/JokerYan/NeRF-DS)
- [ ]  [JAWS: Just A Wild Shot for Cinematic Transfer in Neural Radiance Fields](http://ar5iv.org/abs/2303.15427v1)<br>:house:[project](http://www.lix.polytechnique.fr/vista/projects/2023_cvpr_wang)
- [ ]  [Multi-Space Neural Radiance Fields](http://ar5iv.org/abs/2305.04268v1)<br>:star:[code](https://zx-yin.github.io/msnerf)
- [ ]  [DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields](https://ar5iv.org/abs/2303.14478)<br>:star:[code](https://aibluefisher.github.io/dbarf)
- [ ]  [StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields](https://ar5iv.org/abs/2303.10598)<br>:house:[project](https://kunhao-liu.github.io/StyleRF/)
- [ ]  [Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields](https://ar5iv.org/abs/2302.09311)<br>:house:[project](https://sungheonpark.github.io/tempinterpnerf)
- [ ]  [SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting With Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Mirzaei_SPIn-NeRF_Multiview_Segmentation_and_Perceptual_Inpainting_With_Neural_Radiance_Fields_CVPR_2023_paper.pdf)
- [ ]  [F2-NeRF: Fast Neural Radiance Field Training With Free Camera Trajectories](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_F2-NeRF_Fast_Neural_Radiance_Field_Training_With_Free_Camera_Trajectories_CVPR_2023_paper.pdf)<br>:house:[project](totoro97.github.io/projects/f2-nerf)
- [ ]  [Clothed Human Performance Capture with a Double-layer Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.pdf)
- [ ]  [DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models](https://ar5iv.org/abs/2302.12231)
- [ ]  去模糊
  - [ ]  [BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BAD-NeRF_Bundle_Adjusted_Deblur_Neural_Radiance_Fields_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WU-CVGL/BAD-NeRF)
  - [ ]  [DP-NeRF: Deblurred Neural Radiance Field With Physical Scene Priors](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_DP-NeRF_Deblurred_Neural_Radiance_Field_With_Physical_Scene_Priors_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dogyoonlee/DP-NeRF)<br>:house:[project](https://dogyoonlee.github.io/dpnerf/)

<a name="72"/>

## 72.open-set recognition(开集识别)
- [ ]  [Glocal Energy-based Learning for Few-Shot Open-Set Recognition](http://ar5iv.org/abs/2304.11855v1)

<a name="71"/>

## 71.visual reasoning(视觉推理)
- [ ]  [Visual Programming: Compositional visual reasoning without training](https://ar5iv.org/abs/2211.11559)<br>:trophy:Best Paper
- [ ]  [Abstract Visual Reasoning: An Algebraic Approach for Solving Raven's Progressive Matrices](https://ar5iv.org/abs/2303.11730)<br>:star:[code](https://github.com/Xu-Jingyi/AlgebraicMR)
- [ ]  [Super-CLEVR: A Virtual Benchmark To Diagnose Domain Robustness in Visual Reasoning](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Super-CLEVR_A_Virtual_Benchmark_To_Diagnose_Domain_Robustness_in_Visual_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Lizw14/Super-CLEVR)
- [ ]  [Unicode Analogies: An Anti-Objectivist Visual Reasoning Challenge](https://openaccess.thecvf.com/content/CVPR2023/papers/Spratley_Unicode_Analogies_An_Anti-Objectivist_Visual_Reasoning_Challenge_CVPR_2023_paper.pdf)

<a name="70"/>

## 70.Image Forgery Detection
- [ ]  [Hierarchical Fine-Grained Image Forgery Detection and Localization](http://ar5iv.org/abs/2303.17111v1)<br>:star:[code](https://github.com/CHELSEA234/HiFi_IFDL)
- [ ]  [Detecting and Grounding Multi-Modal Media Manipulation](http://ar5iv.org/abs/2304.02556v1)<br>:star:[code](https://rshaojimmy.github.io/Projects/MultiModal-DeepFake)<br>:star:[code](https://github.com/rshaojimmy/MultiModal-DeepFake)虚假信息检测
- [ ]  [Evading DeepFake Detectors via Adversarial Statistical Consistency](http://ar5iv.org/abs/2304.11670v1)
- [ ]  [Edge-Aware Regional Message Passing Controller for Image Forgery Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Edge-Aware_Regional_Message_Passing_Controller_for_Image_Forgery_Localization_CVPR_2023_paper.pdf)
- [ ]  [TruFor: Leveraging all-round clues for trustworthy image forgery detection and localization](https://ar5iv.org/abs/2212.10957)<br>:house:[project](https://grip-unina.github.io/TruFor/)
- [ ]  [Towards Universal Fake Image Detectors That Generalize Across Generative Models](http://ar5iv.org/abs/2302.10174)
- [ ]  Deepfake Detection
  - [ ]  [Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization](https://ar5iv.org/abs/2210.14457)<br>:star:[code](https://github.com/megvii-research/CADDM)
  - [ ]  [Dynamic Graph Learning With Content-Guided Spatial-Frequency Relation Reasoning for Deepfake Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dynamic_Graph_Learning_With_Content-Guided_Spatial-Frequency_Relation_Reasoning_for_Deepfake_CVPR_2023_paper.pdf)

<a name="69"/>

## 69.Reinforcement learning(强化学习)
- [ ]  [PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav](https://ar5iv.org/abs/2301.07302)
- [ ]  [Local-Guided Global: Paired Similarity Representation for Visual Reinforcement Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Local-Guided_Global_Paired_Similarity_Representation_for_Visual_Reinforcement_Learning_CVPR_2023_paper.pdf)
- [ ]  [Fusing Pre-Trained Language Models With Multimodal Prompts Through Reinforcement Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/JiwanChung/esper)
- [ ]  [Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-per-Second](https://openaccess.thecvf.com/content/CVPR2023/papers/Berges_Galactic_Scaling_End-to-End_Reinforcement_Learning_for_Rearrangement_at_100k_Steps-per-Second_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/galactic)
- [ ]  [Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning](https://ar5iv.org/abs/2205.14557)<br>:house:[project](https://sites.google.com/view/peer-cvpr2023/)

<a name="68"/>

## 68.Lifelong Learning(终身学习)
- [ ]  [Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning](http://ar5iv.org/abs/2304.05288v1)<br>:star:[code](https://github.com/WenjinW/PAR)

<a name="67"/>

## 67.Active Learning(主动学习)
- [ ]  [Re-thinking Federated Active Learning based on Inter-class Diversity](http://ar5iv.org/abs/2303.12317v1)
- [ ]  [Box-Level Active Detection](http://ar5iv.org/abs/2303.13089v1)<br>:star:[code](https://github.com/lyumengyao/blad)
- [ ]  [Are Binary Annotations Sufficient? Video Moment Retrieval via Hierarchical Uncertainty-Based Active Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Are_Binary_Annotations_Sufficient_Video_Moment_Retrieval_via_Hierarchical_Uncertainty-Based_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/renjie-liang/HUAL)
- [ ]  [Re-Thinking Federated Active Learning Based on Inter-Class Diversity](http://ar5iv.org/abs/2303.12317)

<a name="66"/>

## 66.Clustering(聚类)
- [ ]  [DivClust: Controlling Diversity in Deep Clustering](http://ar5iv.org/abs/2304.01042v1)
- [ ]  MVC
  - [ ]  [On the Effects of Self-supervision and Contrastive Alignment in Deep Multi-view Clustering](https://ar5iv.org/abs/2303.09877)<br>:star:[code](https://github.com/DanielTrosten/DeepMVC)
  - [ ]  [GCFAgg: Global and Cross-View Feature Aggregation for Multi-View Clustering](https://ar5iv.org/abs/2305.06799)
  - [ ]  [Sample-Level Multi-View Graph Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Sample-Level_Multi-View_Graph_Clustering_CVPR_2023_paper.pdf)
  - [ ]  [On the Effects of Self-Supervision and Contrastive Alignment in Deep Multi-View Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Trosten_On_the_Effects_of_Self-Supervision_and_Contrastive_Alignment_in_Deep_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/DanielTrosten/DeepMVC)
  - [ ]  [Deep Incomplete Multi-View Clustering With Cross-View Partial Sample and Prototype Alignment](http://ar5iv.org/abs/2303.15689)
  - [ ]  [Highly Confident Local Structure Based Consensus Graph Learning for Incomplete Multi-View Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_Highly_Confident_Local_Structure_Based_Consensus_Graph_Learning_for_Incomplete_CVPR_2023_paper.pdf)
  

<a name="65"/>

## 65.Scene flow estimation(场景流估计)
- [ ]  [Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision](https://ar5iv.org/pdf/2303.00462.pdf)<br>:star:[code](https://github.com/Toytiny/CMFlow)
- [ ]  [Self-Supervised 3D Scene Flow Estimation Guided by Superpoints](http://ar5iv.org/abs/2305.02528v1)
- [ ]  [Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow](https://ar5iv.org/abs/2303.07564)

<a name="64"/>

## 64.Motion Retargeting(动作重定向)
- [ ]  [Skinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry](https://ar5iv.org/abs/2303.08658)<br>:star:[code](https://github.com/Kebii/R2ET)

<a name="63"/>

## 63.edge detection(边缘检测)
- [ ]  edge detection
  - [ ]  [The Treasure Beneath Multiple Annotations: An Uncertainty-aware Edge Detector](https://ar5iv.org/abs/2303.11828)<br>:star:[code](https://github.com/ZhouCX117/UAED)

<a name="62"/>

## 62.Object Counting(物体计数)
- [ ]  [Zero-shot Object Counting](https://ar5iv.org/abs/2303.02001)<br>:star:[code](https://github.com/cvlab-stonybrook/zero-shot-counting)
- [ ]  [Indiscernible Object Counting in Underwater Scenes](http://ar5iv.org/abs/2304.11677v1)<br>:star:[code](https://github.com/GuoleiSun/Indiscernible-Object-Counting)

<a name="61"/>

## 61.Object Re-identification(物体重识别)
- [ ]  [MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID](https://ar5iv.org/abs/2303.07065)<br>:star:[code](https://github.com/vimar-gu/MSINet)
- [ ]  [Large-scale Training Data Search for Object Re-identification](http://ar5iv.org/abs/2303.16186v1)<br>:star:[code](https://github.com/yorkeyao/SnP)
- [ ]  [Adaptive Sparse Pairwise Loss for Object Re-Identification](http://ar5iv.org/abs/2303.18247v1)<br>:star:[code](https://github.com/Astaxanthin/AdaSP)


<a name="60"/>

## 60.Industrial Anomaly Detection(工业缺陷检测)
- [ ]  缺陷定位
  - [ ]  [PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow](https://ar5iv.org/abs/2303.02595)
- [ ]  工业异常检测
  - [ ]  [Multimodal Industrial Anomaly Detection via Hybrid Fusion](https://ar5iv.org/pdf/2303.00601.pdf)<br>:star:[code](https://github.com/nomewang/M3DM)
  - [ ]  [OmniAL: A Unified CNN Framework for Unsupervised Anomaly Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_OmniAL_A_Unified_CNN_Framework_for_Unsupervised_Anomaly_Localization_CVPR_2023_paper.pdf)
- [ ]  异常分割
  - [ ]  [Winning Solution for the CVPR2023 Visual Anomaly and Novelty Detection Challenge: Multimodal Prompting for Data-centric Anomaly Detection](https://ar5iv.org/pdf/2306.09067.pdf)<br>:star:[code](https://github.com/caoyunkang/Segment-Any-Anomaly)<br>:thumbsup:[CVPR 2023 冠军解决方案，零样本异常分割新突破！](https://mp.weixin.qq.com/s/_mJKn4o_U_VjEqlz7DXUFQ)

<a name="59"/>

## 59.Image\Video Compression(图像视频压缩)
- [ ]  [Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger](https://ar5iv.org/pdf/2302.14677.pdf)
- [ ]  [Context-Based Trit-Plane Coding for Progressive Image Compression](https://ar5iv.org/abs/2303.05715)<br>:star:[code](https://github.com/seungminjeon-github/CTC)
- [ ]  [Learned Image Compression with Mixed Transformer-CNN Architectures](http://ar5iv.org/abs/2303.14978v1)<br>:star:[code](https://github.com/jmliu206/LIC_TCM)
- [ ]  [LVQAC: Lattice Vector Quantization Coupled with Spatially Adaptive Companding for Efficient Learned Image Compression](http://ar5iv.org/abs/2304.12319v1)
- [ ]  [Optimization-Inspired Cross-Attention Transformer for Compressive Sensing](http://ar5iv.org/abs/2304.13986v1)<br>:star:[code](https://github.com/songjiechong/OCTUF)
- [ ]  [Multi-Realism Image Compression With a Conditional Generator](https://ar5iv.org/abs/2212.13824)
- [ ]  [AccelIR: Task-aware Image Compression for Accelerating Neural Restoration](https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_AccelIR_Task-Aware_Image_Compression_for_Accelerating_Neural_Restoration_CVPR_2023_paper.pdf)
- [ ]  视频压缩
  - [ ]  [Towards Scalable Neural Representation for Diverse Videos](http://ar5iv.org/abs/2303.14124v1)
  - [ ]  [HNeRV: A Hybrid Neural Representation for Videos](http://ar5iv.org/abs/2304.02633v1)<br>:star:[code](https://haochen-rye.github.io/HNeRV)<br>:star:[code](https://github.com/haochen-rye/HNeRV) 
  - [ ]  [Video Compression With Entropy-Constrained Neural Representations](https://openaccess.thecvf.com/content/CVPR2023/papers/Gomes_Video_Compression_With_Entropy-Constrained_Neural_Representations_CVPR_2023_paper.pdf)
  - [ ]  [Complexity-Guided Slimmable Decoder for Efficient Deep Video Compression](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Complexity-Guided_Slimmable_Decoder_for_Efficient_Deep_Video_Compression_CVPR_2023_paper.pdf)
  - [ ]  [EfficientSCI: Densely Connected Network with Space-time Factorization for Large-scale Video Snapshot Compressive Imaging](https://ar5iv.org/abs/2305.10006)<br>:star:[code](https://github.com/ucaswangls/EfficientSCI.git)
  - [ ]  [MMVC: Learned Multi-Mode Video Compression with Block-based Prediction Mode Selection and Density-Adaptive Entropy Coding](https://ar5iv.org/abs/2304.02273)
  - [ ]  [Neural Video Compression With Diverse Contexts](https://ar5iv.org/abs/2302.14402)<br>:star:[code](https://github.com/microsoft/DCVC)
  （ [Motion Information Propagation for Neural Video Compression](https://openaccess.thecvf.com/content/CVPR2023/papers/Qi_Motion_Information_Propagation_for_Neural_Video_Compression_CVPR_2023_paper.pdf)
  - [ ]  [Hierarchical B-Frame Video Coding Using Two-Layer CANF Without Motion Coding](https://openaccess.thecvf.com/content/CVPR2023/papers/Alexandre_Hierarchical_B-Frame_Video_Coding_Using_Two-Layer_CANF_Without_Motion_Coding_CVPR_2023_paper.pdf)
- [ ]  矢量量化
  - [ ]  [NVTC: Nonlinear Vector Transform Coding](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_NVTC_Nonlinear_Vector_Transform_Coding_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/USTC-IMCL/NVTC)   

<a name="58"/>

## 58.Neural rendering(神经渲染)
- [ ]  [TMO: Textured Mesh Acquisition of Objects With a Mobile Device by Using Differentiable Rendering](http://ar5iv.org/abs/2303.15060)
- [ ]  [Tensor4D: Efficient Neural 4D Decomposition for High-Fidelity Dynamic Reconstruction and Rendering](http://ar5iv.org/abs/2211.11610)
- [ ]  [Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur](https://ar5iv.org/abs/2304.12652)<br>:house:[project](https://daipengwa.github.io/Hybrid-Rendering-ProjectPage)
- [ ]  [NeUDF: Leaning Neural Unsigned Distance Fields With Volume Rendering](http://ar5iv.org/abs/2304.10080)
- [ ]  [DiffRF: Rendering-Guided 3D Radiance Field Diffusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Muller_DiffRF_Rendering-Guided_3D_Radiance_Field_Diffusion_CVPR_2023_paper.pdf)<br>:house:[project](https://sirwyver.github.io/DiffRF/)
- [ ]  [Unsupervised Continual Semantic Adaptation Through Neural Rendering](https://ar5iv.org/abs/2211.13969)
- [ ]  [Neural Fields Meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes](https://ar5iv.org/abs/2304.03266)<br>:house:[project](https://nv-tlabs.github.io/fegr/)
- [ ]  [UV Volumes for Real-Time Rendering of Editable Free-View Human Performance](https://ar5iv.org/abs/2203.14402)<br>:house:[project](https://fanegg.github.io/UV-Volumes)
- [ ]  [Inverse Rendering of Translucent Objects Using Physical and Neural Renderers](https://ar5iv.org/abs/2305.08336)
- [ ]  [ORCa: Glossy Objects As Radiance-Field Cameras](https://ar5iv.org/abs/2212.04531)<br>:house:[project](https://ktiwary2.github.io/objectsascam/)
- [ ]  [MAIR: Multi-View Attention Inverse Rendering With 3D Spatially-Varying Lighting Estimation](https://ar5iv.org/abs/2303.12368)<br>:house:[project](https://bring728.github.io/mair.project/)
- [ ]  [FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views](https://ar5iv.org/abs/2303.14368)<br>:house:[project](https://flex-nerf.github.io/)
- [ ]  [Learning To Render Novel Views From Wide-Baseline Stereo Pairs](https://ar5iv.org/abs/2304.08463)<br>:house:[project](https://yilundu.github.io/wide_baseline/)
- [ ]  [NeRFLiX: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-viewpoint MiXer](https://ar5iv.org/abs/2303.06919)<br>:house:[project](https://redrock303.github.io/nerflix/)
- [ ]  [FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization](https://ar5iv.org/abs/2303.07418)<br>:house:[project](https://jiawei-yang.github.io/FreeNeRF/)
- [ ]  [Local Implicit Ray Function for Generalizable Radiance Field Representation](http://ar5iv.org/abs/2304.12746v1)<br>:star:[code](https://xhuangcv.github.io/lirf/)
- [ ]  [FitMe: Deep Photorealistic 3D Morphable Model Avatars](http://ar5iv.org/abs/2305.09641v1)<br>:star:[code](https://lattas.github.io/fitme)
- [ ]  [Pointersect: Neural Rendering with Cloud-Ray Intersection](http://ar5iv.org/abs/2304.12390v1)
- [ ]  [Inverse Rendering of Translucent Objects using Physical and Neural Renderers](http://ar5iv.org/abs/2305.08336v1)
- [ ]  [Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention](http://ar5iv.org/abs/2303.13014v1)<br>:star:[code](https://liuff19.github.io/S-Ray/)
- [ ]  [ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for Neural Radiance Field](http://ar5iv.org/abs/2303.13817v1)
- [ ]  [WildLight: In-the-wild Inverse Rendering with a Flashlight](http://ar5iv.org/abs/2303.14190v1)<br>:star:[code](https://junxuan-li.github.io/wildlight-website/)
- [ ]  [FlexNeRF: Photorealistic Free-viewpoint Rendering of Moving Humans from Sparse Views](http://ar5iv.org/abs/2303.14368v1)<br>:star:[code](https://flex-nerf.github.io/)
- [ ]  [NeFII: Inverse Rendering for Reflectance Decomposition with Near-Field Indirect Illumination](http://ar5iv.org/abs/2303.16617v1)
- [ ]  [MonoHuman: Animatable Human Neural Field from Monocular Video](http://ar5iv.org/abs/2304.02001v1)<br>:star:[code](https://yzmblog.github.io/projects/MonoHuman/)
- [ ]  [Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos](http://ar5iv.org/abs/2304.04452v1)<br>:star:[code](https://aoliao12138.github.io/ReRF/)
- [ ]  [PlenVDB: Memory Efficient VDB-Based Radiance Fields for Fast Training and Rendering](https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_PlenVDB_Memory_Efficient_VDB-Based_Radiance_Fields_for_Fast_Training_and_CVPR_2023_paper.pdf)<br>在 iPhone12 手机上达到了对于输出 1280x720 分辨率的画面每秒 30 帧的速率。
- [ ]  [NeFII: Inverse Rendering for Reflectance Decomposition With Near-Field Indirect Illumination](https://ar5iv.org/abs/2303.16617)

<a name="57"/>

## 57.Gaze Estimation(视线估计)
- [ ]  [NeRF-Gaze: A Head-Eye Redirection Parametric Model for Gaze Estimation](https://ar5iv.org/abs/2212.14710)
- [ ]  [Source-free Adaptive Gaze Estimation by Uncertainty Reduction](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Source-Free_Adaptive_Gaze_Estimation_by_Uncertainty_Reduction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/caixin1998/UnReGA)
- [ ]  [ReDirTrans: Latent-to-Latent Translation for Gaze and Head Redirection](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_ReDirTrans_Latent-to-Latent_Translation_for_Gaze_and_Head_Redirection_CVPR_2023_paper.pdf)
  
<a name="56"/>

## 56.Sound + Vision(声音与视觉)
- [ ]  [Conditional Generation of Audio from Video via Foley Analogies](http://ar5iv.org/abs/2304.08490v1)<br>:star:[code](https://xypb.github.io/CondFoleyGen/)
- [ ]  [Vision Transformers Are Parameter-Efficient Audio-Visual Learners](http://ar5iv.org/abs/2212.07983)
- [ ]  扬声器检测
  - [ ]  [A Light Weight Model for Active Speaker Detection](https://ar5iv.org/abs/2303.04439)<br>:star:[code](https://github.com/Junhua-Liao/Light-ASD)
- [ ]  视听语音识别
  - [ ]  [Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring](https://ar5iv.org/abs/2303.08536)<br>:star:[code](https://github.com/joannahong/AV-RelScore)
  - [ ]  [Collecting Cross-Modal Presence-Absence Evidence for Weakly-Supervised Audio-Visual Event Perception](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Collecting_Cross-Modal_Presence-Absence_Evidence_for_Weakly-Supervised_Audio-Visual_Event_Perception_CVPR_2023_paper.pdf)<br>:star:[code](github.com/MengyuanChen21/CVPR2023-CMPAE)
  - [ ]  [AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR](http://ar5iv.org/abs/2303.16501v1)
  - [ ]  [SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision](http://ar5iv.org/abs/2303.17200v1)
- [ ]  视听定位
  - [ ]  [Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning](https://ar5iv.org/abs/2303.11302)<br>:star:[code](https://github.com/weixuansun/FNAC-AVL)
  - [ ]  [Audio-Visual Grouping Network for Sound Localization from Mixtures](http://ar5iv.org/abs/2303.17056v1)<br>:star:[code](https://github.com/stoneMo/AVGN)
- [ ]  音频源分离
  - [ ]  [Language-Guided Audio-Visual Source Separation via Trimodal Consistency](http://ar5iv.org/abs/2303.16342v1)
  - [ ]  [iQuery: Instruments As Queries for Audio-Visual Sound Separation](https://ar5iv.org/abs/2212.03814)
- [ ]  声音合成
  - [ ]  [Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos](http://ar5iv.org/abs/2303.16897v1)<br>:star:[code](https://sukun1045.github.io/video-physics-sound-diffusion/)
  - [ ]  [ReVISE: Self-Supervised Speech Resynthesis With Visual Input for Universal and Generalized Speech Regeneration](https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_ReVISE_Self-Supervised_Speech_Resynthesis_With_Visual_Input_for_Universal_and_CVPR_2023_paper.pdf)
- [ ]  电影音频描述
  - [ ]  [AutoAD: Movie Description in Context](http://ar5iv.org/abs/2303.16899v1)<br>:house:[project](https://www.robots.ox.ac.uk/~vgg/research/autoad/)
- [ ]  从声音中生成场景图像
  - [ ]  [Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment](http://ar5iv.org/abs/2303.17490v1)
- [ ]  视听异常检测
  - [ ]  [Self-Supervised Video Forensics by Audio-Visual Anomaly Detection](https://ar5iv.org/abs/2301.01767)<br>:star:[code](https://cfeng16.github.io/audio-visual-forensics)
- [ ]  电影配音
  - [ ]  [Learning To Dub Movies via Hierarchical Prosody Models](https://ar5iv.org/abs/2212.04054)
- [ ]  舞蹈生成
  - [ ]  [EDGE: Editable Dance Generation From Music](https://ar5iv.org/abs/2211.10658)<br>:house:[project](https://edge-dance.github.io/)
  - [ ]  [Music-Driven Group Choreography](http://ar5iv.org/abs/2303.12337)
- [ ]  视频显著性预测
  - [ ]  [CASP-Net: Rethinking Video Saliency Prediction From an Audio-Visual Consistency Perceptual Perspective](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_CASP-Net_Rethinking_Video_Saliency_Prediction_From_an_Audio-Visual_Consistency_Perceptual_CVPR_2023_paper.pdf)
- [ ]  音频驱动的肖像动画
  - [ ]  [DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation](http://ar5iv.org/abs/2301.03786)
- [ ]  听觉定位
  - [ ]  [Egocentric Auditory Attention Localization in Conversations](http://ar5iv.org/abs/2303.16024)

<a name="55"/>

## 55.Novel View Synthesis(视图合成)
- [ ]  [Neural Pixel Composition for 3D-4D View Synthesis From Multi-Views](https://openaccess.thecvf.com/content/CVPR2023/papers/Bansal_Neural_Pixel_Composition_for_3D-4D_View_Synthesis_From_Multi-Views_CVPR_2023_paper.pdf)
- [ ]  [Consistent View Synthesis With Pose-Guided Diffusion Models](http://ar5iv.org/abs/2303.17598)
- [ ]  [MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs](https://ar5iv.org/abs/2302.08788)<br>:house:[project](https://shawn615.github.io/mixnerf/)
- [ ]  [NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis](https://ar5iv.org/abs/2301.08556)<br>:house:[project](https://bland.website/spartn)
- [ ]  [NeRDi: Single-View NeRF Synthesis With Language-Guided Diffusion As General Image Priors](https://ar5iv.org/abs/2212.03267)
- [ ]  [Novel-View Acoustic Synthesis](https://ar5iv.org/abs/2301.08730)<br>:house:[project](https://vision.cs.utexas.edu/projects/nvas)
- [ ]  [Cross-Guided Optimization of Radiance Fields With Multi-View Image Super-Resolution for High-Resolution Novel View Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Yoon_Cross-Guided_Optimization_of_Radiance_Fields_With_Multi-View_Image_Super-Resolution_for_CVPR_2023_paper.pdf)
- [ ]  [Frequency-Modulated Point Cloud Rendering with Easy Editing](https://ar5iv.org/abs/2303.07596)<br>:star:[code](https://github.com/yizhangphd/FreqPCR)
- [ ]  [Learning Neural Duplex Radiance Fields for Real-Time View Synthesis](http://ar5iv.org/abs/2304.10537v1)<br>:house:[project](http://raywzy.com/NDRF)
- [ ]  [ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects](http://ar5iv.org/abs/2304.10448v1)<br>:star:[code](https://eyecan-ai.github.io/rene)
- [ ]  [Balanced Spherical Grid for Egocentric View Synthesis](http://ar5iv.org/abs/2303.12408v1)
- [ ]  [Progressively Optimized Local Radiance Fields for Robust View Synthesis](http://ar5iv.org/abs/2303.13791v1)<br>:star:[code](https://localrf.github.io/)
- [ ]  [F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories](http://ar5iv.org/abs/2303.15951v1)<br>:star:[code](https://totoro97.github.io/projects/f2-nerf)
- [ ]  [Enhanced Stable View Synthesis](http://ar5iv.org/abs/2303.17094v1)
- [ ]  [Consistent View Synthesis with Pose-Guided Diffusion Models](http://ar5iv.org/abs/2303.17598v1)<br>:star:[code](https://poseguided-diffusion.github.io/)
- [ ]  [Learning to Render Novel Views from Wide-Baseline Stereo Pairs](http://ar5iv.org/abs/2304.08463v1)<br>:star:[code](https://yilundu.github.io/wide_baseline/)
 - [ ]  [Painting 3D Nature in 2D: View Synthesis of Natural Scenes From a Single Semantic Mask](https://ar5iv.org/abs/2302.07224)<br>:house:[project](https://zju3dv.github.io/paintingnature/)
 - [ ]  [NoPe-NeRF: Optimising Neural Radiance Field With No Pose Prior](https://openaccess.thecvf.com/content/CVPR2023/papers/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.pdf)<br>:house:[project](https://nope-nerf.active.vision)
 - [ ]  [Multiscale Tensor Decomposition and Rendering Equation Encoding for View Synthesis](https://ar5iv.org/abs/2303.03808)<br>:star:[code](https://github.com/imkanghan/nrff)
- [ ]  [Efficient View Synthesis and 3D-Based Multi-Frame Denoising With Multiplane Feature Representations](https://ar5iv.org/abs/2303.18139)
- [ ]  [NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds](https://ar5iv.org/abs/2304.06287)
- [ ]  [DINER: Depth-aware Image-based NEural Radiance fields](https://ar5iv.org/abs/2211.16630)<br>:house:[project](https://malteprinzler.github.io/projects/diner/diner.html)
- [ ]  [RefSR-NeRF: Towards High Fidelity and Super Resolution View Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_RefSR-NeRF_Towards_High_Fidelity_and_Super_Resolution_View_Synthesis_CVPR_2023_paper.pdf)<br>:star:[code](https://gitee.com/mindspore/models/tree/master/research/cv/RefSR-NeRF)
- [ ]  [VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence Normalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_VDN-NeRF_Resolving_Shape-Radiance_Ambiguity_via_View-Dependence_Normalization_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/BoifZ/VDN-NeRF)
- [ ]  [DynIBaR: Neural Dynamic Image-Based Rendering](https://ar5iv.org/abs/2211.11082)<br>:house:[project](http://dynibar.github.io/)<br>:trophy:Honorable Mention
- [ ]  [Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories](https://ar5iv.org/abs/2211.03889)

<a name="54"/>

## 54.Benchmark/Dataset(基准/数据集)
- [ ]  [Joint HDR Denoising and Fusion: A Real-World Mobile HDR Image Dataset](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Joint_HDR_Denoising_and_Fusion_A_Real-World_Mobile_HDR_Image_CVPR_2023_paper.pdf)
- [ ]  [A New Dataset Based on Images Taken by Blind People for Testing the Robustness of Image Classification Models Trained for ImageNet Categories](https://openaccess.thecvf.com/content/CVPR2023/papers/Bafghi_A_New_Dataset_Based_on_Images_Taken_by_Blind_People_CVPR_2023_paper.pdf)
- [ ]  [Benchmarking Self-Supervised Learning on Diverse Pathology Datasets](http://ar5iv.org/abs/2212.04690)
- [ ]  [Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline](https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Multispectral_Video_Semantic_Segmentation_A_Benchmark_Dataset_and_Baseline_CVPR_2023_paper.pdf)
- [ ]  [Towards Artistic Image Aesthetics Assessment: A Large-Scale Dataset and a New Method](http://ar5iv.org/abs/2303.15166)
- [ ]  [ScaleDet: A Scalable Multi-Dataset Object Detector](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ScaleDet_A_Scalable_Multi-Dataset_Object_Detector_CVPR_2023_paper.pdf)
- [ ]  [JRDB-Pose: A Large-Scale Dataset for Multi-Person Pose Estimation and Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Vendrow_JRDB-Pose_A_Large-Scale_Dataset_for_Multi-Person_Pose_Estimation_and_Tracking_CVPR_2023_paper.pdf)<br>:sunflower:[dataset](https://jrdb.erc.monash.edu/)
- [ ]  [Architecture, Dataset and Model-Scale Agnostic Data-Free Meta-Learning](https://ar5iv.org/abs/2303.11183)
- [ ]  [DF-Platter: Multi-Face Heterogeneous Deepfake Dataset](https://openaccess.thecvf.com/content/CVPR2023/papers/Narayan_DF-Platter_Multi-Face_Heterogeneous_Deepfake_Dataset_CVPR_2023_paper.pdf)<br>:sunflower:[dataset](http://iab-rubric.org/df-platter-database)
- [ ]  [HandsOff: Labeled Dataset Generation With No Additional Human Annotations](https://ar5iv.org/abs/2212.12645)<br>:sunflower:[dataset](http://austinxu87.github.io/handsoff)
- [ ]  [M6Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis](https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_M6Doc_A_Large-Scale_Multi-Format_Multi-Type_Multi-Layout_Multi-Language_Multi-Annotation_Category_Dataset_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/HCIILAB/M6Doc)
- [ ]  [ShapeTalk: A Language Dataset and Framework for 3D Shape Edits and Deformations](https://openaccess.thecvf.com/content/CVPR2023/papers/Achlioptas_ShapeTalk_A_Language_Dataset_and_Framework_for_3D_Shape_Edits_CVPR_2023_paper.pdf)<br>:sunflower:[dataset](https://changeit3d.github.io/)
- [ ]  [NewsNet: A Novel Dataset for Hierarchical Temporal Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_NewsNet_A_Novel_Dataset_for_Hierarchical_Temporal_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/NewsNet-Benchmark/NewsNet)
- [ ]  [MISC210K: A Large-Scale Dataset for Multi-Instance Semantic Correspondence](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_MISC210K_A_Large-Scale_Dataset_for_Multi-Instance_Semantic_Correspondence_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/YXSUNMADMAX/MISC210K)
- [ ]  [StarCraftImage: A Dataset for Prototyping Spatial Reasoning Methods for Multi-Agent Environments](https://openaccess.thecvf.com/content/CVPR2023/papers/Kulinski_StarCraftImage_A_Dataset_for_Prototyping_Spatial_Reasoning_Methods_for_Multi-Agent_CVPR_2023_paper.pdf)<br>:house:[project](https://starcraftdata.davidinouye.com/)
- [ ]  [Habitat-Matterport 3D Semantics Dataset](https://ar5iv.org/abs/2210.05633)
- [ ]  [CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset](https://openaccess.thecvf.com/content/CVPR2023/papers/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/CNVid/CNVid-3.5M)<br>大规模公共中文视频文本数据集
- [ ]  [FLAG3D: A 3D Fitness Activity Dataset With Language Instruction](https://ar5iv.org/abs/2212.04638)<br>:house:[project](https://andytang15.github.io/FLAG3D)
- [ ]  [Multi-Label Compound Expression Recognition: C-EXPR Database & Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Kollias_Multi-Label_Compound_Expression_Recognition_C-EXPR_Database__Network_CVPR_2023_paper.pdf)
- [ ]  [ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation](https://ar5iv.org/abs/2204.13662)<br>:house:[project](https://arctic.is.tue.mpg.de/)<br>手物体操作的数据集
- [ ]  [xFBD: Focused Building Damage Dataset and Analysis](https://ar5iv.org/abs/2212.13876)<br>建筑物损坏数据集
- [ ]  [Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo](https://ar5iv.org/abs/2303.01943)<br>:sunflower:[dataset](https://spring-benchmark.org/)
- [ ]  [Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes](https://ar5iv.org/abs/2303.02760)
- [ ]  [HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for Single-View 3D Hair Modeling](https://ar5iv.org/abs/2303.02700)<br>:sunflower:[dataset](https://paulyzheng.github.io/research/hairstep/)
- [ ]  [CUDA: Convolution-based Unlearnable Datasets](https://ar5iv.org/abs/2303.04278)<br>:sunflower:[dataset](https://github.com/vinusankars/Convolution-based-Unlearnability)
- [ ]  [MVImgNet: A Large-scale Dataset of Multi-view Images](https://ar5iv.org/abs/2303.06042)<br>:sunflower:[dataset](https://gaplab.cuhk.edu.cn/projects/MVImgNet/)
- [ ]  [V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception](https://ar5iv.org/abs/2303.07601)<br>:sunflower:[dataset](https://github.com/ucla-mobility/V2V4Real)<br>Vehicle-to-Vehicle(V2V)感知
- [ ]  [Polynomial Implicit Neural Representations For Large Diverse Datasets](https://ar5iv.org/abs/2303.11424)<br>:sunflower:[dataset](https://github.com/Rajhans0/Poly_INR)
- [ ]  [MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset](http://ar5iv.org/abs/2303.12756v1)<br>:sunflower:[dataset](https://github.com/MrChenFeng/MaskCon_CVPR2023)
- [ ]  [RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a Topological-consistent Dataset](http://ar5iv.org/abs/2303.12564v1)<br>:sunflower:[dataset](https://gaplab.cuhk.edu.cn/projects/RaBit/)
- [ ]  [Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts](http://ar5iv.org/abs/2303.14152v1)<br>:star:[code](https://terascale-all-sensing-research-studio.github.io/FantasticBreaks)
- [ ]  [ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data](http://ar5iv.org/abs/2303.13885v1)<br>:star:[code](https://arkittrack.github.io)
- [ ]  [CelebV-Text: A Large-Scale Facial Text-Video Dataset](http://ar5iv.org/abs/2303.14717v1)<br>:star:[code](https://celebv-text.github.io/)<br>人脸文本到视频生成
- [ ]  [Towards Artistic Image Aesthetics Assessment: a Large-scale Dataset and a New Method](http://ar5iv.org/abs/2303.15166v1)<br>:star:[code](https://github.com/Dreemurr-T/BAID.git)<br>艺术图像美学评估
- [ ]  [CIMI4D: A Large Multimodal Climbing Motion Dataset under Human-scene Interactions](http://ar5iv.org/abs/2303.17948v1)<br>:house:[project](http://www.lidarhumanmotion.net/cimi4d/)<br>攀爬动作数据集
- [ ]  [Uncurated Image-Text Datasets: Shedding Light on Demographic Bias](http://ar5iv.org/abs/2304.02828v1)
- [ ]  [AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection](http://ar5iv.org/abs/2304.06116v1)<br>:star:[code](https://github.com/wentaozhu/AutoShot.git)<br>:house:[project](https://paperswithcode.com/paper/autoshot-a-short-video-dataset-and-state-of)公共短视频镜头边界检测数据集
- [ ]  [V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting](http://ar5iv.org/abs/2305.05938v1)<br>:star:[code](https://github.com/AIR-THU/DAIR-V2X-Seq)
- [ ]  [WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models](http://ar5iv.org/abs/2305.07528v1)<br>:star:[code](https://infernolia.github.io/WEDGE)用于极端天气条件下的物体检测和天气分类任务的合成数据集
- [ ]  [CLOTH4D: A Dataset for Clothed Human Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_CLOTH4D_A_Dataset_for_Clothed_Human_Reconstruction_CVPR_2023_paper.pdf)<br>:sunflower:[dataset](http://www.github.com/AemikaChow/AiDLab-fAshIon-Data)<br>用于穿衣服人体重建的数据集
- [ ]  [OmniCity: Omnipotent City Understanding With Multi-Level and Multi-View Images](https://ar5iv.org/abs/2208.00928)<br>:sunflower:[dataset](https://city-super.github.io/omnicity)<br>从多层次和多视图图像中获取全能城市理解的新数据集。
- [ ]  [RealImpact: A Dataset of Impact Sound Fields for Real Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Clarke_RealImpact_A_Dataset_of_Impact_Sound_Fields_for_Real_Objects_CVPR_2023_paper.pdf)<br>:star:[code](https://samuelpclarke.com/realimpact/)
- [ ]  [BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion](https://openaccess.thecvf.com/content/CVPR2023/papers/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.pdf)<br>:house:[project](https://bedlam.is.tue.mpg.de/)
- [ ]  [GFIE:A Dataset and Baseline for Gaze-Following From 2D to 3D in Indoor Environments](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_GFIE_A_Dataset_and_Baseline_for_Gaze-Following_From_2D_to_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/gfie)
- [ ]  Benchmark(基准)
  - [ ]  [A Soma Segmentation Benchmark in Full Adult Fly Brain](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_A_Soma_Segmentation_Benchmark_in_Full_Adult_Fly_Brain_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/liuxy1103/EMADS)
  - [ ]  [A New Comprehensive Benchmark for Semi-Supervised Video Anomaly Detection and Anticipation](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_A_New_Comprehensive_Benchmark_for_Semi-Supervised_Video_Anomaly_Detection_and_CVPR_2023_paper.pdf)
  - [ ]  [A Large-Scale Homography Benchmark](http://ar5iv.org/abs/2302.09997)
  - [ ]  [Toward RAW Object Detection: A New Benchmark and a New Model](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Toward_RAW_Object_Detection_A_New_Benchmark_and_a_New_CVPR_2023_paper.pdf)
  - [ ]  [MammalNet: A Large-Scale Video Benchmark for Mammal Recognition and Behavior Understanding](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_MammalNet_A_Large-Scale_Video_Benchmark_for_Mammal_Recognition_and_Behavior_CVPR_2023_paper.pdf)
  - [ ]  [Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild](https://ar5iv.org/abs/2207.10660)<br>:house:[project](https://omni3d.garrickbrazil.com/)
  - [ ]  [Advancing Visual Grounding With Scene Knowledge: Benchmark and Method](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Advancing_Visual_Grounding_With_Scene_Knowledge_Benchmark_and_Method_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhjohnchan/SK-VG)
  - [ ]  [The ObjectFolder Benchmark: Multisensory Learning With Neural and Real Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_The_ObjectFolder_Benchmark_Multisensory_Learning_With_Neural_and_Real_Objects_CVPR_2023_paper.pdf)<br>:star:[code](https://objectfolder.stanford.edu/)
  - [ ]  [Meta Omnium: A Benchmark for General-Purpose Learning-to-Learn](http://ar5iv.org/abs/2305.07625v1)<br>:star:[code](https://edi-meta-learning.github.io/meta-omnium)
  - [ ]  [A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation](https://ar5iv.org/abs/2303.09165)<br>:star:[code](https://github.com/huitangtang/On_the_Utility_of_Synthetic_Data)
  - [ ]  [GeoNet: Benchmarking Unsupervised Adaptation across Geographies](http://ar5iv.org/abs/2303.15443v1)<br>:star:[code](https://tarun005.github.io/GeoNet)
  - [ ]  [PosterLayout: A New Benchmark and Approach for Content-aware Visual-Textual Presentation Layout](http://ar5iv.org/abs/2303.15937v1)<br>:star:[code](https://github.com/PKU-ICST-MIPL/PosterLayout-CVPR2023)
  - [ ]  [Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline](http://ar5iv.org/abs/2303.12930v1)
  - [ ]  [ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos](https://ar5iv.org/abs/2305.02519)<br>:house:[project](https://milvlg.github.io/anetqa/)
  - [ ]  Image Similarity
    - [ ]  [GeneCIS: A Benchmark for General Conditional Image Similarity](https://openaccess.thecvf.com/content/CVPR2023/papers/Vaze_GeneCIS_A_Benchmark_for_General_Conditional_Image_Similarity_CVPR_2023_paper.pdf)<br>:house:[project](sgvaze.github.io/genecis)
  - [ ]  [ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos](http://ar5iv.org/abs/2305.02519v1)<br>:star:[code](https://milvlg.github.io/anetqa/)
  - [ ]  [Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel Benchmark](http://ar5iv.org/abs/2305.10899v1)<br>:star:[code](https://github.com/jankyee/URUR)
  - [ ]  [NewsNet: A Novel Benchmark for Hierarchical Temporal Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_NewsNet_A_Novel_Dataset_for_Hierarchical_Temporal_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/NewsNet-Benchmark/NewsNet)
  - [ ]  [Ultra-High Resolution Segmentation With Ultra-Rich Context: A Novel Benchmark](https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Ultra-High_Resolution_Segmentation_With_Ultra-Rich_Context_A_Novel_Benchmark_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/jankyee/URUR)
  - [ ]  [PosterLayout: A New Benchmark and Approach for Content-Aware Visual-Textual Presentation Layout](https://ar5iv.org/abs/2303.15937)<br>:star:[code](https://github.com/PKU-ICST-MIPL/PosterLayout-CVPR2023)
  - [ ]  [Meta Omnium: A Benchmark for General-Purpose Learning-To-Learn](https://ar5iv.org/abs/2305.07625)<br>:star:[code](https://edi-meta-learning.github.io/meta-omnium)
  - [ ]  [RefTeacher: A Strong Baseline for Semi-Supervised Referring Expression Comprehension](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_RefTeacher_A_Strong_Baseline_for_Semi-Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf)<br>:house:[project](https://refteacher.github.io/)

<a name="53"/>

## 53.Sign Language (手语)
- [ ]  [Ham2Pose: Animating Sign Language Notation Into Pose Sequences](https://openaccess.thecvf.com/content/CVPR2023/papers/Arkushin_Ham2Pose_Animating_Sign_Language_Notation_Into_Pose_Sequences_CVPR_2023_paper.pdf)<br>:house:[project](https://rotem-shalev.github.io/ham-to-pose)
- [ ]  手语翻译
  - [ ]  [Gloss Attention for Gloss-Free Sign Language Translation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_Gloss_Attention_for_Gloss-Free_Sign_Language_Translation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/YinAoXiong/GASLT)
- [ ]  手语识别
  - [ ]  [Continuous Sign Language Recognition with Correlation Network](https://ar5iv.org/abs/2303.03202)<br>:star:[code](https://github.com/hulianyuyy/CorrNet)
  - [ ]  [Reconstructing Signing Avatars From Video Using Linguistic Priors](https://ar5iv.org/abs/2304.10482)<br>:house:[project](http://sgnify.is.tue.mpg.de/)
  - [ ]  [Distilling Cross-Temporal Contexts for Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Distilling_Cross-Temporal_Contexts_for_Continuous_Sign_Language_Recognition_CVPR_2023_paper.pdf)
  - [ ]  [CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition With Variational Alignment](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_CVT-SLR_Contrastive_Visual-Textual_Transformation_for_Sign_Language_Recognition_With_Variational_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/binbinjiang/CVT-SLR)
  - [ ]  [Natural Language-Assisted Sign Language Recognition](https://ar5iv.org/abs/2303.12080)<br>:star:[code](https://github.com/FangyunWei/SLRT)
  - [ ]  [Continuous Sign Language Recognition With Correlation Network](https://ar5iv.org/abs/2303.03202)<br>:star:[code](https://github.com/hulianyuyy/CorrNet)
- [ ]  手语检索
  - [ ]  [CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive Learning](http://ar5iv.org/abs/2303.12793v1)<br>:star:[code](https://github.com/FangyunWei/SLRT)

<a name="52"/>

## 52.Human Motion(人体运动)
- [ ]  [Semi-Weakly Supervised Object Kinematic Motion Prediction](https://ar5iv.org/abs/2303.17774)
- [ ]  [The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction](https://ar5iv.org/abs/2204.13340)
- [ ]  [MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_MotionDiffuser_Controllable_Multi-Agent_Motion_Prediction_Using_Diffusion_CVPR_2023_paper.pdf)
- [ ]  人体运动预测
  - [ ]  [EqMotion: Equivariant Multi-agent Motion Prediction with Invariant Interaction Reasoning](https://ar5iv.org/abs/2303.10876)<br>:star:[code](https://github.com/MediaBrain-SJTU/EqMotion)
  - [ ]  [DeFeeNet: Consecutive 3D Human Motion Prediction with Deviation Feedback](http://ar5iv.org/abs/2304.04496v1)
  - [ ]  [Decompose More and Aggregate Better: Two Closer Looks at Frequency Representation Learning for Human Motion Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Decompose_More_and_Aggregate_Better_Two_Closer_Looks_at_Frequency_CVPR_2023_paper.pdf)
- [ ]  人体运动合成
  - [ ]  [Generating Human Motion From Textual Descriptions With Discrete Representations](https://ar5iv.org/abs/2301.06052)<br>:house:[project](https://mael-zys.github.io/T2M-GPT/)
  - [ ]  [UDE: A Unified Driving Engine for Human Motion Generation](https://ar5iv.org/abs/2211.16016)<br>:star:[code](https://github.com/zixiangzhou916/UDE/)
  - [ ]  [Mofusion: A Framework for Denoising-Diffusion-Based Motion Synthesis](https://ar5iv.org/abs/2212.04495)<br>:house:[project](https://vcai.mpi-inf.mpg.de/projects/MoFusion)
  - [ ]  [MoDi: Unconditional Motion Synthesis From Diverse Data](http://ar5iv.org/abs/2206.08010)
- [ ]  3D HM
  - [ ]  [Generating Holistic 3D Human Motion from Speech](https://ar5iv.org/abs/2212.04420)<br>:house:[project](https://talkshow.is.tue.mpg.de/)

<a name="51"/>

## 51.Computed Imaging(计算成像，如光学、几何、光场成像等)
- [ ]  [Physics-Guided ISO-Dependent Sensor Noise Modeling for Extreme Low-Light Photography](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Physics-Guided_ISO-Dependent_Sensor_Noise_Modeling_for_Extreme_Low-Light_Photography_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/happycaoyue/LLD)
- [ ]  [TRACE: 5D Temporal Regression of Avatars With Dynamic Cameras in 3D Environments](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Arthur151/DynaCam)
- [ ]  [High-Fidelity Event-Radiance Recovery via Transient Event Frequency](https://openaccess.thecvf.com/content/CVPR2023/papers/Han_High-Fidelity_Event-Radiance_Recovery_via_Transient_Event_Frequency_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/hjynwa/TEF)
- [ ]  [Real-Time Neural Light Field on Mobile Devices](https://ar5iv.org/abs/2212.08057)<br>:house:[project](https://snap-research.github.io/MobileR2L/)
- [ ]  [Accidental Light Probes](https://ar5iv.org/abs/2301.05211)<br>:house:[project](https://kovenyu.com/ALP/)
- [ ]  [DyLiN: Making Light Field Networks Dynamic](http://ar5iv.org/abs/2303.14243v1)<br>:star:[code](https://dylin2023.github.io)
- [ ]  [Learning Rotation-Equivariant Features for Visual Correspondence](http://ar5iv.org/abs/2303.15472v1)<br>:house:[project](http://cvlab.postech.ac.kr/research/RELF)
- [ ]  [Role of Transients in Two-Bounce Non-Line-of-Sight Imaging](https://ar5iv.org/abs/2304.01308)
- [ ]  [Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution](https://ar5iv.org/abs/2209.08503)
- [ ]  相机姿势估计
  - [ ]  [SliceMatch: Geometry-Guided Aggregation for Cross-View Pose Estimation](https://ar5iv.org/abs/2211.14651)
  - [ ]  [SparsePose: Sparse-View Camera Pose Regression and Refinement](https://ar5iv.org/abs/2211.16991)
  - [ ]  [Pose Synchronization Under Multiple Pair-Wise Relative Poses](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Pose_Synchronization_Under_Multiple_Pair-Wise_Relative_Poses_CVPR_2023_paper.pdf)
  - [ ]  [Privacy-Preserving Representations Are Not Enough: Recovering Scene Content From Camera Poses](https://openaccess.thecvf.com/content/CVPR2023/papers/Chelani_Privacy-Preserving_Representations_Are_Not_Enough_Recovering_Scene_Content_From_Camera_CVPR_2023_paper.pdf)
- [ ]  快门校正
  - [ ]  [EvShutter: Transforming Events for Unconstrained Rolling Shutter Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Erbach_EvShutter_Transforming_Events_for_Unconstrained_Rolling_Shutter_Correction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/juliuserbach/EvShutter)
- [ ]  相机校准
  - [ ]  [Perspective Fields for Single Image Camera Calibration](https://ar5iv.org/abs/2212.03239)<br>:house:[project](https://jinlinyi.github.io/PerspectiveFields/)
- [ ]  几何估计
  - [ ]  [Adaptive Annealing for Robust Geometric Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Sidhartha_Adaptive_Annealing_for_Robust_Geometric_Estimation_CVPR_2023_paper.pdf)
- [ ]  相机定位
  - [ ] [NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization](https://ar5iv.org/abs/2211.11177)<br>:star:[code](https://github.com/Tangshitao/NeuMap)

<a name="50"/>

## 50.Anomaly Detection(异常检测)
- [ ]  [Revisiting Reverse Distillation for Anomaly Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Tien_Revisiting_Reverse_Distillation_for_Anomaly_Detection_CVPR_2023_paper.pdf)
- [ ]  [SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection](http://ar5iv.org/abs/2111.13495)
- [ ]  [Prototypical Residual Networks for Anomaly Detection and Localization](https://ar5iv.org/abs/2212.02031)
- [ ]  [OpenMix: Exploring Outlier Samples for Misclassification Detection](https://ar5iv.org/abs/2303.17093)<br>:star:[code](https://github.com/Impression2805/OpenMix)
- [ ]  [Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detection](https://ar5iv.org/abs/2207.01463)<br>:star:[code](https://github.com/xcyao00/BGAD)
- [ ]  [Diversity-Measurable Anomaly Detection](https://ar5iv.org/abs/2303.05047)
- [ ]  [SimpleNet: A Simple Network for Image Anomaly Detection and Localization](http://ar5iv.org/abs/2303.15140v1)<br>:star:[code](https://github.com/DonaldRR/SimpleNet)
- [ ]  [DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection](https://ar5iv.org/abs/2211.11317)
- [ ]  [WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation](http://ar5iv.org/abs/2303.14814v1)
- [ ]  OOD
  - [ ]  [Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection](https://ar5iv.org/abs/2303.10449)<br>:star:[code](https://github.com/LuFan31/ET-OOD)
  - [ ]  [Mind the Label Shift of Augmentation-Based Graph OOD Generalization](http://ar5iv.org/abs/2303.14859)
  - [ ]  [Block Selection Method for Using Feature Norm in Out-of-Distribution Detection](https://ar5iv.org/abs/2212.02295)<br>:star:[code](https://github.com/gist-ailab/block-selection-for-OOD-detection)
  - [ ]  [Distribution Shift Inversion for Out-of-Distribution Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Distribution_Shift_Inversion_for_Out-of-Distribution_Prediction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/yu-rp/Distribution-Shift-Iverson)
  - [ ]  [Are Data-Driven Explanations Robust Against Out-of-Distribution Data?](https://ar5iv.org/abs/2303.16390)
  - [ ]  [LINe: Out-of-Distribution Detection by Leveraging Important Neurons](http://ar5iv.org/abs/2303.13995v1)
  - [ ]  [Rethinking Out-of-Distribution (OOD) Detection: Masked Image Modeling Is All You Need](https://ar5iv.org/abs/2302.02615)<br>:star:[code](https://github.com/JulietLJY/MOOD)
  - [ ]  [Balanced Energy Regularization Loss for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Balanced_Energy_Regularization_Loss_for_Out-of-Distribution_Detection_CVPR_2023_paper.pdf)
  - [ ]  [Decoupling MaxLogit for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Decoupling_MaxLogit_for_Out-of-Distribution_Detection_CVPR_2023_paper.pdf)
  - [ ]  [Detection of Out-of-Distribution Samples Using Binary Neuron Activation Patterns](https://ar5iv.org/abs/2212.14268)
  - [ ]  [GEN: Pushing the Limits of Softmax-Based Out-of-Distribution Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GEN_Pushing_the_Limits_of_Softmax-Based_Out-of-Distribution_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/XixiLiu95/GEN)

<a name="49"/>

## 49.Image Geo-localization(图像地理位置识别)
- [ ]  [Where We Are and What We're Looking At: Query Based Worldwide Image Geo-localization Using Hierarchies and Scenes](https://ar5iv.org/abs/2303.04249)

<a name="48"/>

## 48.NLP(自然语言处理)
- [ ]  [Images Speak in Images: A Generalist Painter for In-Context Visual Learning](https://ar5iv.org/abs/2212.02499)<br>:star:[code](https://github.com/baaivision/Painter)
- [ ]  [CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes From Natural Language](https://openaccess.thecvf.com/content/CVPR2023/papers/Sanghi_CLIP-Sculptor_Zero-Shot_Generation_of_High-Fidelity_and_Diverse_Shapes_From_Natural_CVPR_2023_paper.pdf)
- [ ]  反讽检测(检测文本（或图像，如漫画等其他模态）中是否存在讽刺)
  - [ ]  [DIP: Dual Incongruity Perceiving Network for Sarcasm Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_DIP_Dual_Incongruity_Perceiving_Network_for_Sarcasm_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/downdric/MSD)
- [ ]  NLQ
  - [ ]  [NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory](https://ar5iv.org/abs/2301.00746)<br>:star:[code](http://vision.cs.utexas.edu/projects/naq)
- [ ]  Visual Grounding(视觉指代)
  - [ ]  [Language Adaptive Weight Generation for Multi-Task Visual Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Su_Language_Adaptive_Weight_Generation_for_Multi-Task_Visual_Grounding_CVPR_2023_paper.pdf)
- [ ]  Referring Expression Comprehension(指代表达理解)
  - [ ]  [RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf)<br>:house:[project](https://refclip.github.io/)

<a name="47"/>

## 47.Few/Zero-Shot Learning/Domain Generalization/Adaptation(小/零样本/域泛化/域适应)
- [ ]  DG
  - [ ]  [Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View](https://ar5iv.org/abs/2303.01686)
  - [ ]  [Meta-Causal Learning for Single Domain Generalization](http://ar5iv.org/abs/2304.03709)
  - [ ]  [Bi-Level Meta-Learning for Few-Shot Domain Generalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Bi-Level_Meta-Learning_for_Few-Shot_Domain_Generalization_CVPR_2023_paper.pdf)
  - [ ]  [Promoting Semantic Connectivity: Dual Nearest Neighbors Contrastive Learning for Unsupervised Domain Generalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Promoting_Semantic_Connectivity_Dual_Nearest_Neighbors_Contrastive_Learning_for_Unsupervised_CVPR_2023_paper.pdf)
  - [ ]  [Federated Domain Generalization With Generalization Adjustment](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/MediaBrain-SJTU/FedDG-GA)
  - [ ]  [Decompose, Adjust, Compose: Effective Normalization by Playing With Frequency for Domain Generalization](https://ar5iv.org/abs/2303.02328)
  - [ ]  [NICO++: Towards Better Benchmarking for Domain Generalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xxgege/NICO-plus)
  - [ ]  [Improved Test-Time Adaptation for Domain Generalization](http://ar5iv.org/abs/2304.04494v1)<br>:star:[code](https://github.com/liangchen527/ITTA)
  - [ ]  [Modality-Agnostic Debiasing for Single Domain Generalization](https://ar5iv.org/abs/2303.07123)
  - [ ]  [Neuron Structure Modeling for Generalizable Remote Physiological Measurement](https://ar5iv.org/abs/2303.05955)<br>:star:[code](https://github.com/LuPaoPao/NEST)
  - [ ]  [Sharpness-Aware Gradient Matching for Domain Generalization](https://ar5iv.org/abs/2303.10353)<br>:star:[code](https://github.com/Wang-pengfei/SAGM)
  - [ ]  [Improving Generalization with Domain Convex Game](http://ar5iv.org/abs/2303.13297v1)
  - [ ]  [Generalist: Decoupling Natural and Robust Generalization](http://ar5iv.org/abs/2303.13813v1)<br>:star:[code](https://github.com/PKU-ML/Generalist)
  - [ ]  [ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization](https://ar5iv.org/abs/2303.11674)<br>:star:[code](https://github.com/lingeringlight/ALOFT/)
  - [ ]  [Deep Frequency Filtering for Domain Generalization](https://ar5iv.org/abs/2203.12198)
  - [ ]  [Progressive Random Convolutions for Single Domain Generalization](http://ar5iv.org/abs/2304.00424v1)
  - [ ]  [Meta-causal Learning for Single Domain Generalization](http://ar5iv.org/abs/2304.03709v1)
- [ ]  DA
  - [ ]  [Guiding Pseudo-labels with Uncertainty Estimation for Test-Time Adaptation](https://ar5iv.org/abs/2303.03770)<br>:star:[code](https://github.com/MattiaLitrico/Guiding-Pseudo-labels-with-Uncertainty-Estimation-for-Test-Time-Adaptation)
  - [ ]  [Class Relationship Embedded Learning for Source-Free Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Class_Relationship_Embedded_Learning_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2023_paper.pdf)
  - [ ]  [Semi-Supervised Domain Adaptation With Source Label Adaptation](http://ar5iv.org/abs/2302.02335)
  - [ ]  [SCoDA: Domain Adaptive Shape Completion for Real Scans](http://ar5iv.org/abs/2304.10179)
  - [ ]  [Divide and Adapt: Active Domain Adaptation via Customized Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Divide_and_Adapt_Active_Domain_Adaptation_via_Customized_Learning_CVPR_2023_paper.pdf)
  - [ ]  [Source-Free Video Domain Adaptation With Spatial-Temporal-Historical Consistency Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Source-Free_Video_Domain_Adaptation_With_Spatial-Temporal-Historical_Consistency_Learning_CVPR_2023_paper.pdf)
  - [ ]  [DARE-GRAM: Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices](https://ar5iv.org/abs/2303.13325)<br>:star:[code](https://github.com/ismailnejjar/DARE-GRAM)
  - [ ]  [Dual-Bridging With Adversarial Noise Generation for Domain Adaptive rPPG Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Dual-Bridging_With_Adversarial_Noise_Generation_for_Domain_Adaptive_rPPG_Estimation_CVPR_2023_paper.pdf )
  - [ ]  [MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation](https://ar5iv.org/abs/2212.01322)<br>:star:[code](https://github.com/lhoyer/MIC)
  - [ ]  [DATE: Domain Adaptive Product Seeker for E-commerce](http://ar5iv.org/abs/2304.03669v1)<br>:star:[code](https://github.com/Taobao-live/Product-Seeking)
  - [ ]  [Adjustment and Alignment for Unbiased Open Set Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Adjustment_and_Alignment_for_Unbiased_Open_Set_Domain_Adaptation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/CityU-AIM-Group/Anna)
  - [ ]  [Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective](http://ar5iv.org/abs/2303.13434v1)
  - [ ]  [MHPL: Minimum Happy Points Learning for Active Source Free Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MHPL_Minimum_Happy_Points_Learning_for_Active_Source_Free_Domain_CVPR_2023_paper.pdf)
  - [ ]  [COT: Unsupervised Domain Adaptation with Clustering and Optimal Transport](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_COT_Unsupervised_Domain_Adaptation_With_Clustering_and_Optimal_Transport_CVPR_2023_paper.pdf)
  - [ ]  [Upcycling Models under Domain and Category Shift](https://ar5iv.org/abs/2303.07110)<br>:star:[code](https://github.com/ispc-lab/GLC)
  - [ ]  [C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation](http://ar5iv.org/abs/2303.17132v1)
  - [ ]  [TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation](https://ar5iv.org/abs/2303.09870)<br>:star:[code](https://github.com/devavratTomar/TeSLA)
  - [ ]  [OSAN: A One-Stage Alignment Network to Unify Multimodal Alignment and Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_OSAN_A_One-Stage_Alignment_Network_To_Unify_Multimodal_Alignment_and_CVPR_2023_paper.pdf)
  - [ ]  [MOT: Masked Optimal Transport for Partial Domain Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_MOT_Masked_Optimal_Transport_for_Partial_Domain_Adaptation_CVPR_2023_paper.pdf)
  - [ ]  [Feature Alignment and Uniformity for Test Time Adaptation](https://ar5iv.org/abs/2303.10902)
- [ ]  ZSL
  - [ ]  [Bi-Directional Distribution Alignment for Transductive Zero-Shot Learning](https://ar5iv.org/abs/2303.08698)<br>:star:[code](https://github.com/Zhicaiwww/Bi-VAEGAN)
  - [ ]  [Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning](http://ar5iv.org/abs/2303.15322v1)<br>:star:[code](https://github.com/ManLiuCoder/PSVMA)
  - [ ]  [Learning Attention as Disentangler for Compositional Zero-shot Learning](http://ar5iv.org/abs/2303.15111v1)<br>:star:[code](https://haoosz.github.io/ade-czsl/)
  - [ ]  [Zero-shot Model Diagnosis](http://ar5iv.org/abs/2303.15441v1)
  - [ ]  [Learning Conditional Attributes for Compositional Zero-Shot Learning](https://ar5iv.org/abs/2305.17940)<br>:star:[code](https://github.com/wqshmzh/CANet-CZSL)
  - [ ]  [(ML)$^2$P-Encoder: On Exploration of Channel-Class Correlation for Multi-Label Zero-Shot Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_ML2P-Encoder_On_Exploration_of_Channel-Class_Correlation_for_Multi-Label_Zero-Shot_Learning_CVPR_2023_paper.pdf)<br>:star:[code](github.com/simonzmliu/cvpr23_mlzsl)
  - [ ]  [Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning](http://ar5iv.org/abs/2211.10681)
- [ ]  FSL
  - [ ]  [Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement](http://ar5iv.org/abs/2304.11598v1)
  - [ ]  [Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners](https://ar5iv.org/abs/2303.02151)<br>:star:[code](https://github.com/ZrrSkywalker/CaFo)
  - [ ]  [Revisiting Prototypical Network for Cross Domain Few-Shot Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Revisiting_Prototypical_Network_for_Cross_Domain_Few-Shot_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/NWPUZhoufei/LDP-Net)
  - [ ]  [Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning With Multimodal Models](https://ar5iv.org/abs/2301.06267)<br>:house:[project](https://linzhiqiu.github.io/papers/cross_modal/)
  - [ ]  [Open-Set Likelihood Maximization for Few-Shot Learning](https://ar5iv.org/abs/2301.08390)
  - [ ]  [StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning](https://ar5iv.org/abs/2302.09309)<br>:star:[code](https://github.com/lovelyqian/StyleAdv-CDFSL)

<a name="46"/>

## 46.Scene Graph Generation(场景图生成)
- [ ]  [Unbiased Scene Graph Generation in Videos](http://ar5iv.org/abs/2304.00733)
- [ ]  [Prototype-Based Embedding Network for Scene Graph Generation](http://ar5iv.org/abs/2303.07096)
- [ ]  [IS-GGT: Iterative Scene Graph Generation With Generative Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Kundu_IS-GGT_Iterative_Scene_Graph_Generation_With_Generative_Transformers_CVPR_2023_paper.pdf)
- [ ]  [Prototype-based Embedding Network for Scene Graph Generation](https://ar5iv.org/abs/2303.07096)<br>:star:[code](https://github.com/VL-Group/PENET)
- [ ]  [Devil's on the Edges: Selective Quad Attention for Scene Graph Generation](http://ar5iv.org/abs/2304.03495v1)<br>:house:[project](https://cvlab.postech.ac.kr/research/SQUAT/)
- [ ]  [Learning To Generate Language-Supervised and Open-Vocabulary Scene Graph Using Pre-Trained Visual-Semantic Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_To_Generate_Language-Supervised_and_Open-Vocabulary_Scene_Graph_Using_Pre-Trained_CVPR_2023_paper.pdf)
- [ ]  [Panoptic Video Scene Graph Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf)
- [ ]  [Fast Contextual Scene Graph Generation With Unbiased Context Augmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Fast_Contextual_Scene_Graph_Generation_With_Unbiased_Context_Augmentation_CVPR_2023_paper.pdf)

<a name="45"/>

## 45.Dense Prediction(密集预测)
- [ ]  [Ensemble-Based Blackbox Attacks on Dense Prediction](https://ar5iv.org/abs/2303.14304)<br>:star:[code](https://github.com/CSIPlab/EBAD)
- [ ]  [DejaVu: Conditional Regenerative Learning to Enhance Dense Prediction](https://ar5iv.org/abs/2303.01573)
- [ ]  [Ensemble-based Blackbox Attacks on Dense Prediction](http://ar5iv.org/abs/2303.14304v1)<br>:star:[code](https://github.com/CSIPlab/EBAD)
- [ ]  [Probabilistic Prompt Learning for Dense Prediction](http://ar5iv.org/abs/2304.00779v1)
- [ ]  [1% VS 100%: Parameter-Efficient Low Rank Adapter for Dense Predictions](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_1_VS_100_Parameter-Efficient_Low_Rank_Adapter_for_Dense_Predictions_CVPR_2023_paper.pdf)
- [ ]  [DPF: Learning Dense Prediction Fields With Weak Supervision](https://ar5iv.org/abs/2303.16890)<br>:star:[code](https://github.com/cxx226/DPF)
- [ ]  密集检测
  - [ ]  [One-to-Few Label Assignment for End-to-End Dense Detection](https://ar5iv.org/abs/2303.11567)<br>:star:[code](https://github.com/strongwolf/o2f)
- [ ]  密集目标定位
  - [ ]  [Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense Object Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Multi-Modal_Class-Specific_Tokens_for_Weakly_Supervised_Dense_Object_Localization_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xulianuwa/MMCST)

<a name="44"/>

## 44.Federated Learning(联邦学习)
- [ ]  [Confidence-Aware Personalized Federated Learning via Variational Expectation Maximization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Confidence-Aware_Personalized_Federated_Learning_via_Variational_Expectation_Maximization_CVPR_2023_paper.pdf)
- [ ]  [Federated Learning With Data-Agnostic Distribution Fusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Duan_Federated_Learning_With_Data-Agnostic_Distribution_Fusion_CVPR_2023_paper.pdf)
- [ ]  [How To Prevent the Poor Performance Clients for Personalized Federated Learning?](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_How_To_Prevent_the_Poor_Performance_Clients_for_Personalized_Federated_CVPR_2023_paper.pdf)
- [ ]  [GradMA: A Gradient-Memory-Based Accelerated Federated Learning With Alleviated Catastrophic Forgetting](https://ar5iv.org/abs/2302.14307)
- [ ]  [Bias-Eliminating Augmentation Learning for Debiased Federated Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Bias-Eliminating_Augmentation_Learning_for_Debiased_Federated_Learning_CVPR_2023_paper.pdf)
- [ ]  [Make Landscape Flatter in Differentially Private Federated Learning](https://ar5iv.org/abs/2303.11242)
- [ ]  [The Resource Problem of Using Linear Layer Leakage Attack in Federated Learning](http://ar5iv.org/abs/2303.14868v1)
- [ ]  [Rethinking Federated Learning With Domain Shift: A Prototype View](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WenkeHuang/RethinkFL)
- [ ]  [On the Effectiveness of Partial Variance Reduction in Federated Learning With Heterogeneous Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_On_the_Effectiveness_of_Partial_Variance_Reduction_in_Federated_Learning_CVPR_2023_paper.pdf)
- [ ]  [Elastic Aggregation for Federated Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Elastic_Aggregation_for_Federated_Optimization_CVPR_2023_paper.pdf)
- [ ]  [FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning](https://ar5iv.org/abs/2207.09653)
- [ ]  [Adaptive Channel Sparsity for Federated Learning Under System Heterogeneity](https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Adaptive_Channel_Sparsity_for_Federated_Learning_Under_System_Heterogeneity_CVPR_2023_paper.pdf)
- [ ]  [ScaleFL: Resource-Adaptive Federated Learning With Heterogeneous Clients](https://openaccess.thecvf.com/content/CVPR2023/papers/Ilhan_ScaleFL_Resource-Adaptive_Federated_Learning_With_Heterogeneous_Clients_CVPR_2023_paper.pdf)
- [ ]  [Reliable and Interpretable Personalized Federated Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Reliable_and_Interpretable_Personalized_Federated_Learning_CVPR_2023_paper.pdf)

<a name="43"/>

## 43.Multi-Task Learning(多任务学习)
- [ ]  [Independent Component Alignment for Multi-Task Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Senushkin_Independent_Component_Alignment_for_Multi-Task_Learning_CVPR_2023_paper.pdf)
- [ ]  [Dynamic Neural Network for Multi-Task Learning Searching Across Diverse Network Topologies](https://ar5iv.org/abs/2303.06856)
- [ ]  [AdaMTL: Adaptive Input-dependent Inference for Efficient Multi-Task Learning](http://ar5iv.org/abs/2304.08594v1)<br>:star:[code](https://github.com/scale-lab/AdaMTL.git)
- [ ]  [Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Mod-Squad_Designing_Mixtures_of_Experts_As_Modular_Multi-Task_Learners_CVPR_2023_paper.pdf)<br>:house:[project](https://vis-www.cs.umass.edu/mod-squad/)
- [ ]  [Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing With Non-Learnable Primitives](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Mitigating_Task_Interference_in_Multi-Task_Learning_via_Explicit_Task_Routing_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhichao-lu/etr-nlp-mtl)
- [ ]  [Hierarchical Prompt Learning for Multi-Task Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.pdf)

<a name="42"/>

## 42.Metric Learning(度量学习)
- [ ]  [Advancing Deep Metric Learning Through Multiple Batch Norms And Multi-Targeted Adversarial Examples](https://ar5iv.org/abs/2211.16253)
- [ ]  [Deep Factorized Metric Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Factorized_Metric_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/wangck20/DFML)
- [ ]  [Deep Semi-Supervised Metric Learning With Mixed Label Propagation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuang_Deep_Semi-Supervised_Metric_Learning_With_Mixed_Label_Propagation_CVPR_2023_paper.pdf)
- [ ]  [Cross-Image-Attention for Conditional Embeddings in Deep Metric Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Kotovenko_Cross-Image-Attention_for_Conditional_Embeddings_in_Deep_Metric_Learning_CVPR_2023_paper.pdf)

<a name="41"/>

## 41.Incremental Learning(增量学习)
- [ ]  [Decoupling Learning and Remembering: A Bilevel Memory Framework With Knowledge Projection for Task-Incremental Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Decoupling_Learning_and_Remembering_A_Bilevel_Memory_Framework_With_Knowledge_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/SunWenJu123/BMKP)
- [ ]  [AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning](https://ar5iv.org/abs/2305.11488)<br>:star:[code](https://github.com/bhrqw/AttriCLIP)
- [ ]  [GKEAL: Gaussian Kernel Embedded Analytic Learning for Few-Shot Class Incremental Task](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuang_GKEAL_Gaussian_Kernel_Embedded_Analytic_Learning_for_Few-Shot_Class_Incremental_CVPR_2023_paper.pdf)
- [ ]  类增量学习
  - [ ]  [Dense Network Expansion for Class Incremental Learning](http://ar5iv.org/abs/2303.12696v1)
  - [ ]  [Class-Incremental Exemplar Compression for Class-Incremental Learning](http://ar5iv.org/abs/2303.14042v1)<br>:star:[code](https://github.com/xfflzl/CIM-CIL)
  - [ ]  [Rebalancing Batch Normalization for Exemplar-based Class-Incremental Learning](https://ar5iv.org/abs/2201.12559)
  - [ ]  [Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning](http://ar5iv.org/abs/2304.00426v1)<br>:star:[code](https://github.com/zysong0113/SAVC)
  - [ ]  [On the Stability-Plasticity Dilemma of Class-Incremental Learning](http://ar5iv.org/abs/2304.01663v1)
  - [ ]  [Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Few-Shot_Class-Incremental_Learning_via_Class-Aware_Bilateral_Distillation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/LinglanZhao/BiDistFSCIL)
  - [ ]  [Multi-Centroid Task Descriptor for Dynamic Class Incremental Inference](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Multi-Centroid_Task_Descriptor_for_Dynamic_Class_Incremental_Inference_CVPR_2023_paper.pdf)
  - [ ]  [DKT: Diverse Knowledge Transfer Transformer for Class Incremental Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_DKT_Diverse_Knowledge_Transfer_Transformer_for_Class_Incremental_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/MIV-XJTU/DKT)
  - [ ]  [Learning With Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning](http://ar5iv.org/abs/2304.00426)
  - [ ]  [CafeBoost: Causal Feature Boost To Eliminate Task-Induced Bias for Class Incremental Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_CafeBoost_Causal_Feature_Boost_To_Eliminate_Task-Induced_Bias_for_Class_CVPR_2023_paper.pdf)
  
<a name="40"/>

## 40.Adversarial Learning(对抗学习)
- [ ]  [Adversarial Robustness via Random Projection Filters](https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Adversarial_Robustness_via_Random_Projection_Filters_CVPR_2023_paper.pdf)
- [ ]  [Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts](https://ar5iv.org/abs/2302.10164)
- [ ]  [Dynamic Generative Targeted Attacks With Pattern Injection](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Dynamic_Generative_Targeted_Attacks_With_Pattern_Injection_CVPR_2023_paper.pdf)
- [ ]  [FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits](https://ar5iv.org/abs/2304.10306)
- [ ]  [Enhancing the Self-Universality for Transferable Targeted Attacks](https://ar5iv.org/abs/2209.03716)<br>:star:[code](https://github.com/zhipeng-wei/Self-Universality)
- [ ]  [Exploring the Relationship Between Architectural Design and Adversarially Robust Generalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Exploring_the_Relationship_Between_Architectural_Design_and_Adversarially_Robust_Generalization_CVPR_2023_paper.pdf)<br>:house:[project](http://robust.art/)
- [ ]  [Revisiting Residual Networks for Adversarial Robustness](https://ar5iv.org/abs/2212.11005)<br>:star:[code](https://github.com/zhichao-lu/robust-residual-network)
- [ ]  [Feature Separation and Recalibration for Adversarial Robustness](http://ar5iv.org/abs/2303.13846v1)<br>:star:[code](https://github.com/wkim97/FSR)
- [ ]  [CFA: Class-wise Calibrated Fair Adversarial Training](http://ar5iv.org/abs/2303.14460v1)<br>:star:[code](https://github.com/PKU-ML/CFA)
- [ ]  [Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations](https://ar5iv.org/abs/2202.04235)<br>:house:[project](https://hsiung.cc/CARBEN/)
- [ ]  [Efficient Loss Function by Minimizing the Detrimental Effect of Floating-Point Errors on Gradient-Based Attacks](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Efficient_Loss_Function_by_Minimizing_the_Detrimental_Effect_of_Floating-Point_CVPR_2023_paper.pdf)
- [ ]  黑盒
  - [ ]  [BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning](http://ar5iv.org/abs/2303.14773v1)<br>:star:[code](https://github.com/changdaeoh/BlackVIP)
  - [ ]  [Black-Box Sparse Adversarial Attack via Multi-Objective Optimisation](https://openaccess.thecvf.com/content/CVPR2023/papers/Williams_Black-Box_Sparse_Adversarial_Attack_via_Multi-Objective_Optimisation_CVPR_2023_paper.pdf)
  - [ ]  [Reinforcement Learning-Based Black-Box Model Inversion Attacks](http://ar5iv.org/abs/2304.04625v1)
  - [ ]  [Minimizing Maximum Model Discrepancy for Transferable Black-Box Targeted Attacks](https://ar5iv.org/abs/2212.09035)
- [ ]  对抗样本
  - [ ]  [Demystifying Causal Features on Adversarial Examples and Causal Inoculation for Robust Network by Adversarial Instrumental Variable Regression](https://ar5iv.org/abs/2303.01052)
  - [ ]  [Introducing Competition To Boost the Transferability of Targeted Adversarial Examples Through Clean Feature Mixup](https://openaccess.thecvf.com/content/CVPR2023/papers/Byun_Introducing_Competition_To_Boost_the_Transferability_of_Targeted_Adversarial_Examples_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dreamflake/CFM)
  - [ ]  [Towards Transferable Targeted Adversarial Examples](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Towards_Transferable_Targeted_Adversarial_Examples_CVPR_2023_paper.pdf)
  - [ ]  [Improving the Transferability of Adversarial Samples by Path-Augmented Method](http://ar5iv.org/abs/2303.15735v1)
  - [ ]  [Increasing the Latency of LiDAR-Based Detection Using Adversarial Examples](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SlowLiDAR_Increasing_the_Latency_of_LiDAR-Based_Detection_Using_Adversarial_Examples_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WUSTL-CSPL/SlowLiDAR)
- [ ]  后门攻击
  - [ ]  [Single Image Backdoor Inversion via Robust Smoothed Classifiers](https://ar5iv.org/pdf/2303.00215.pdf)<br>:star:[code](https://ar5iv.org/pdf/2303.00215.pdf)
  - [ ]  [Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency](http://ar5iv.org/abs/2303.18191)
  - [ ]  [You Are Catching My Attention: Are Vision Transformers Bad Learners Under Backdoor Attacks?](https://openaccess.thecvf.com/content/CVPR2023/papers/Yuan_You_Are_Catching_My_Attention_Are_Vision_Transformers_Bad_Learners_CVPR_2023_paper.pdf)
  - [ ]  [MEDIC: Remove Model Backdoors via Importance Driven Cloning](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_MEDIC_Remove_Model_Backdoors_via_Importance_Driven_Cloning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/qiulingxu/MEDIC)
  - [ ]  [Backdoor Defense via Adaptively Splitting Poisoned Dataset](http://ar5iv.org/abs/2303.12993v1)<br>:star:[code](https://github.com/KuofengGao/ASD)
  - [ ]  [Detecting Backdoors in Pre-trained Encoders](http://ar5iv.org/abs/2303.15180v1)<br>:star:[code](https://github.com/GiantSeaweed/DECREE)
  - [ ]  [Color Backdoor: A Robust Poisoning Attack in Color Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Color_Backdoor_A_Robust_Poisoning_Attack_in_Color_Space_CVPR_2023_paper.pdf)
  - [ ]  [Detecting Backdoors in Pre-Trained Encoders](https://ar5iv.org/abs/2303.15180)<br>:star:[code](https://github.com/GiantSeaweed/DECREE)
- [ ]  对抗攻击
  - [ ]  [Adversarial Attack with Raindrops](https://ar5iv.org/pdf/2302.14267.pdf)
  - [ ]  [Progressive Backdoor Erasing via Connecting Backdoor and Adversarial Attacks](http://ar5iv.org/abs/2202.06312)
  - [ ]  [Towards Benchmarking and Assessing Visual Naturalness of Physical World Adversarial Attacks](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Towards_Benchmarking_and_Assessing_Visual_Naturalness_of_Physical_World_Adversarial_CVPR_2023_paper.pdf)
  - [ ]  [The Best Defense Is a Good Offense: Adversarial Augmentation Against Adversarial Attacks](https://openaccess.thecvf.com/content/CVPR2023/papers/Frosio_The_Best_Defense_Is_a_Good_Offense_Adversarial_Augmentation_Against_CVPR_2023_paper.pdf)
  - [ ]  [Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization](http://ar5iv.org/abs/2303.15754v1)
  - [ ]  [Robust Single Image Reflection Removal Against Adversarial Attacks](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Robust_Single_Image_Reflection_Removal_Against_Adversarial_Attacks_CVPR_2023_paper.pdf)
  - [ ]  [Transferable Adversarial Attacks on Vision Transformers With Token Gradient Regularization](https://ar5iv.org/abs/2303.15754)<br>:star:[code](https://github.com/jpzhang1810/TGR)
  - [ ]  [StyLess: Boosting the Transferability of Adversarial Examples](http://ar5iv.org/abs/2304.11579v1)
  - [ ]  [Re-thinking Model Inversion Attacks Against Deep Neural Networks](http://ar5iv.org/abs/2304.01669v1)<br>:star:[code](https://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/)
  - [ ]  [Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning](http://ar5iv.org/abs/2304.01482v1)<br>:star:[code](https://github.com/UCDvision/PatchSearch)
  - [ ]  [Jedi: Entropy-based Localization and Removal of Adversarial Patches](http://ar5iv.org/abs/2304.10029v1)
- [ ]  后门防御
  - [ ]  [Backdoor Defense via Deconfounded Representation Learning](https://ar5iv.org/abs/2303.06818)<br>:star:[code](https://github.com/zaixizhang/CBD)
- [ ]  对抗训练
  - [ ]  [The Enemy of My Enemy Is My Friend: Exploring Inverse Adversaries for Improving Adversarial Training](https://ar5iv.org/abs/2211.00525)
  - [ ]  [Randomized Adversarial Training via Taylor Expansion](https://ar5iv.org/abs/2303.10653)<br>:star:[code](https://github.com/Alexkael/Randomized-Adversarial-Training)
  - [ ]  [AGAIN: Adversarial Training With Attribution Span Enlargement and Hybrid Feature Fusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_AGAIN_Adversarial_Training_With_Attribution_Span_Enlargement_and_Hybrid_Feature_CVPR_2023_paper.pdf)

<a name="39"/>

## 39.Continual Learning(持续学习)
- [ ]  [Dealing With Cross-Task Class Discrimination in Online Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Dealing_With_Cross-Task_Class_Discrimination_in_Online_Continual_Learning_CVPR_2023_paper.pdf)
- [ ]  [Heterogeneous Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Madaan_Heterogeneous_Continual_Learning_CVPR_2023_paper.pdf)
- [ ]  [Batch Model Consolidation: A Multi-Task Model Consolidation Framework](https://openaccess.thecvf.com/content/CVPR2023/papers/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.pdf)
- [ ]  [CODA-Prompt: COntinual Decomposed Attention-Based Prompting for Rehearsal-Free Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_CODA-Prompt_COntinual_Decomposed_Attention-Based_Prompting_for_Rehearsal-Free_Continual_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/GT-RIPL/CODA-Prompt)
- [ ]  [Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning](https://ar5iv.org/abs/2303.09483)<br>:star:[code](https://github.com/kim-sanghwan/ANCL)
- [ ]  [Computationally Budgeted Continual Learning: What Does Matter?](https://ar5iv.org/abs/2303.11165)<br>:star:[code](https://github.com/drimpossible/BudgetCL)
- [ ]  [Achieving a Better Stability-Plasticity Trade-Off via Auxiliary Networks in Continual Learning](https://ar5iv.org/abs/2303.09483)
- [ ]  [Preserving Linear Separability in Continual Learning by Backward Feature Projection](http://ar5iv.org/abs/2303.14595v1)
- [ ]  [Regularizing Second-Order Influences for Continual Learning](http://ar5iv.org/abs/2304.10177v1)<br>:star:[code](https://github.com/feifeiobama/InfluenceCL)
- [ ]  [Rethinking Gradient Projection Continual Learning: Stability / Plasticity Feature Space Decoupling](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Rethinking_Gradient_Projection_Continual_Learning_Stability__Plasticity_Feature_Space_CVPR_2023_paper.pdf)
- [ ]  [MetaMix: Towards Corruption-Robust Continual Learning With Temporally Self-Adaptive Data Transformation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MetaMix_Towards_Corruption-Robust_Continual_Learning_With_Temporally_Self-Adaptive_Data_Transformation_CVPR_2023_paper.pdf)
- [ ]  [Exploring Data Geometry for Continual Learning](http://ar5iv.org/abs/2304.03931v1)
- [ ]  [PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning](http://ar5iv.org/abs/2304.04408v1)
- [ ]  [Bilateral Memory Consolidation for Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Nie_Bilateral_Memory_Consolidation_for_Continual_Learning_CVPR_2023_paper.pdf)
- [ ]  [Adaptive Plasticity Improvement for Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Adaptive_Plasticity_Improvement_for_Continual_Learning_CVPR_2023_paper.pdf)
- [ ]  [Real-Time Evaluation in Online Continual Learning: A New Hope](https://ar5iv.org/abs/2302.01047)
- [ ]  [PIVOT: Prompting for Video Continual Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Villa_PIVOT_Prompting_for_Video_Continual_Learning_CVPR_2023_paper.pdf)

<a name="38"/>

## 38.Meta-Learning(元学习)
- [ ]  [Meta-Learning with a Geometry-Adaptive Preconditioner](http://ar5iv.org/abs/2304.01552v1)<br>:star:[code](https://github.com/Suhyun777/CVPR23-GAP)元学习
- [ ]  [Improving Generalization of Meta-Learning With Inverted Regularization at Inner-Level](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Improving_Generalization_of_Meta-Learning_With_Inverted_Regularization_at_Inner-Level_CVPR_2023_paper.pdf)
- [ ]  [Ground-Truth Free Meta-Learning for Deep Compressive Sampling](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Ground-Truth_Free_Meta-Learning_for_Deep_Compressive_Sampling_CVPR_2023_paper.pdf)
- [ ]  [HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization](http://ar5iv.org/abs/2212.14258)
- [ ]  [Panoptic Compositional Feature Field for Editable Scene Rendering With Network-Inferred Labels via Metric Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_Panoptic_Compositional_Feature_Field_for_Editable_Scene_Rendering_With_Network-Inferred_CVPR_2023_paper.pdf)

<a name="37"/>

## 37.Contrastive Learning(对比学习)
- [ ]  [Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning](http://ar5iv.org/abs/2210.09452)
- [ ]  [Difficulty-Based Sampling for Debiased Contrastive Representation Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Jang_Difficulty-Based_Sampling_for_Debiased_Contrastive_Representation_Learning_CVPR_2023_paper.pdf)
- [ ]  [MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining](https://ar5iv.org/abs/2208.12262)<br>:star:[code](https://github.com/LightDXY/MaskCLIP)
- [ ]  [Twin Contrastive Learning with Noisy Labels](https://ar5iv.org/abs/2303.06930)<br>:star:[code](https://github.com/Hzzone/TCL)
- [ ]  [Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens](http://ar5iv.org/abs/2303.14865v1)
- [ ]  [Best of Both Worlds: Multimodal Contrastive Learning With Tabular and Imaging Data](https://ar5iv.org/abs/2303.14080)
- [ ]  [CLAMP: Prompt-Based Contrastive Learning for Connecting Language and Animal Pose](https://ar5iv.org/abs/2206.11752)
- [ ]  [ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-Real Novel View Synthesis via Contrastive Learning](https://ar5iv.org/abs/2303.11052)
- [ ]  [Hyperbolic Contrastive Learning for Visual Representations beyond Objects](https://ar5iv.org/abs/2212.00653)<br>:star:[code](https://github.com/shlokk/HCL/tree/main/HCL)
- [ ]  非对比学习
  - [ ]  [Non-Contrastive Learning Meets Language-Image Pre-Training](https://ar5iv.org/abs/2210.09304)

<a name="36"/>

## 36.Optical Flow(光流估计)
- [ ]  [Rethinking Optical Flow from Geometric Matching Consistent Perspective](https://ar5iv.org/abs/2303.08384)<br>:star:[code](https://github.com/DQiaole/MatchFlow)
- [ ]  [DistractFlow: Improving Optical Flow Estimation via Realistic Distractions and Pseudo-Labeling](http://ar5iv.org/abs/2303.14078v1)
- [ ]  [AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation](http://ar5iv.org/abs/2303.16493v1)
- [ ]  [TransFlow: Transformer as Flow Learner](http://ar5iv.org/abs/2304.11523v1)
- [ ]  [Tangentially Elongated Gaussian Belief Propagation for Event-Based Incremental Optical Flow Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Nagata_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_Optical_Flow_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/DensoITLab/tegbp/)
- [ ]  [FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_FlowFormer_Masked_Cost_Volume_Autoencoding_for_Pretraining_Optical_Flow_Estimation_CVPR_2023_paper.pdf)

<a name="35"/>

## 35.OCR
- [ ]  文本识别
  - [ ]  [Self-Supervised Implicit Glyph Attention for Text Recognition](https://ar5iv.org/abs/2203.03382)
- [ ]  场景文本检测
  - [ ]  [Turning a CLIP Model into a Scene Text Detector](https://ar5iv.org/pdf/2302.14338.pdf)<br>:star:[code](https://github.com/wenwenyu/TCM)
- [ ]  表格结构识别
  - [ ]  [Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling](https://ar5iv.org/abs/2303.06949)
- [ ]  字体生成
  - [ ]  [CF-Font: Content Fusion for Few-shot Font Generation](http://ar5iv.org/abs/2303.14017v1)<br>:star:[code](https://github.com/wangchi95/CF-Font)
  - [ ]  [Neural Transformation Fields for Arbitrary-Styled Font Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Neural_Transformation_Fields_for_Arbitrary-Styled_Font_Generation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/fubinfb/NTF)
  - [ ]  [DualVector: Unsupervised Vector Font Synthesis with Dual-Part Representation](http://ar5iv.org/abs/2305.10462v1)<br>:star:[code](https://github.com/thuliu-yt16/dualvector)
- [ ]  手写文本生成
  - [ ]  [Disentangling Writer and Character Styles for Handwriting Generation](http://ar5iv.org/abs/2303.14736v1)<br>:star:[code](https://github.com/dailenson/SDT)
  - [ ]  [Handwritten Text Generation from Visual Archetypes](http://ar5iv.org/abs/2303.15269v1)
- [ ]  矢量字体合成
  - [ ]  [DeepVecFont-v2: Exploiting Transformers to Synthesize Vector Fonts with Higher Quality](http://ar5iv.org/abs/2303.14585v1)<br>:star:[code](https://github.com/yizhiwang96/deepvecfont-v2)
- [ ]  生成图形文档
  - [ ]  [Towards Flexible Multi-modal Document Models](http://ar5iv.org/abs/2303.18248v1)<br>:star:[code](https://cyberagentailab.github.io/flex-dm)
- [ ]  文本检测
  - [ ]  [Towards Robust Tampered Text Detection in Document Image: New Dataset and New Solution](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Towards_Robust_Tampered_Text_Detection_in_Document_Image_New_Dataset_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/qcf-568/DocTamper)
- [ ]  文档处理
  - [ ]  [Unifying Vision, Text, and Layout for Universal Document Processing](https://ar5iv.org/abs/2212.02623)
- [ ]  Scene Text Spotting
  - [ ]  [Towards Unified Scene Text Spotting based on Sequence Generation](https://ar5iv.org/abs/2304.03435)<br>:star:[code](https://github.com/clovaai/units)
  - [ ]  [Towards Unified Scene Text Spotting based on Sequence Generation](http://ar5iv.org/abs/2304.03435v1)<br>:star:[code](https://github.com/clovaai/units)
  - [ ]  [DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting](https://ar5iv.org/abs/2211.10772)<br>:star:[code](https://github.com/ViTAE-Transformer/DeepSolo)

<a name="34"/>

## 34.Model Compression/Knowledge Distillation/Pruning(模型压缩/知识蒸馏/剪枝)
- [ ]  [Network Expansion for Practical Training Acceleration](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Network_Expansion_for_Practical_Training_Acceleration_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/huawei-noah/Efficient-Computing/tree/master/TrainingAcceleration/NetworkExpansion)
- [ ]  [Accelerating Dataset Distillation via Model Augmentation](https://ar5iv.org/abs/2212.06152)
- [ ]  [Run, Don’t Walk: Chasing Higher FLOPS for Faster Neural Networks](https://ar5iv.org/abs/2303.03667)<br>:star:[code](https://github.com/JierunChen/FasterNet)
- [ ]  量化
  - [ ]  [Solving Oscillation Problem in Post-Training Quantization Through a Theoretical Perspective](https://ar5iv.org/abs/2303.11906)<br>:star:[code](https://github.com/bytedance/MRECG)
  - [ ]  [Adaptive Data-Free Quantization](https://ar5iv.org/abs/2303.06869)<br>:star:[code](https://github.com/hfutqian/AdaDFQ)
  - [ ]  [Defining and Quantifying the Emergence of Sparse Concepts in DNNs](http://ar5iv.org/abs/2111.06206)
  - [ ]  [NIPQ: Noise Proxy-Based Integrated Pseudo-Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_NIPQ_Noise_Proxy-Based_Integrated_Pseudo-Quantization_CVPR_2023_paper.pdf)
  - [ ]  [Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Bit-Shrinking_Limiting_Instantaneous_Sharpness_for_Improving_Post-Training_Quantization_CVPR_2023_paper.pdf)
  - [ ]  [Genie: Show Me the Data for Quantization](https://ar5iv.org/abs/2212.04780)
  - [ ]  [One-Shot Model for Mixed-Precision Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2023_paper.pdf)
  - [ ]  [Post-training Quantization on Diffusion Models](https://ar5iv.org/abs/2211.15736)<br>:star:[code](https://github.com/42Shawn/PTQ4DM)
  - [ ]  [Q-DETR: An Efficient Low-Bit Quantized Detection Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Q-DETR_An_Efficient_Low-Bit_Quantized_Detection_Transformer_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/SteveTsui/Q-DETR)
  - [ ]  [NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers](https://ar5iv.org/abs/2211.16056)
  - [ ]  [PD-Quant: Post-Training Quantization Based on Prediction Difference Metric](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PD-Quant_Post-Training_Quantization_Based_on_Prediction_Difference_Metric_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/hustvl/PD-Quant)
  - [ ]  [Boost Vision Transformer with GPU-Friendly Sparsity and Quantization](http://ar5iv.org/abs/2305.10727v1)
- [ ]  剪枝
  - [ ]  [CP$^3$: Channel Pruning Plug-in for Point-based Networks](http://ar5iv.org/abs/2303.13097v1)
  - [ ]  [Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures](http://ar5iv.org/abs/2304.12622v1)
  - [ ]  [Global Vision Transformer Pruning With Hessian-Aware Saliency](https://ar5iv.org/abs/2110.04869)
  - [ ]  [X-Pruner: eXplainable Pruning for Vision Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_X-Pruner_eXplainable_Pruning_for_Vision_Transformers_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/vickyyu90/XPruner)
  - [ ]  [DepGraph: Towards Any Structural Pruning](https://ar5iv.org/abs/2301.12900)
  - [ ]  [Progressive Neighbor Consistency Mining for Correspondence Pruning](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Progressive_Neighbor_Consistency_Mining_for_Correspondence_Pruning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xinliu29/NCMNet)
  - [ ]  [Training Debiased Subnetworks With Contrastive Weight Pruning](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Training_Debiased_Subnetworks_With_Contrastive_Weight_Pruning_CVPR_2023_paper.pdf)
- [ ]  MC
  - [ ]  [Hard Sample Matters a Lot in Zero-Shot Quantization](http://ar5iv.org/abs/2303.13826v1)
- [ ]  KD
  - [ ]  [DisWOT: Student Architecture Search for Distillation WithOut Training](http://ar5iv.org/abs/2303.15678v1)<br>:star:[code](https://lilujunai.github.io/DisWOT-CVPR2023/)
  - [ ]  [Data-Free Knowledge Distillation via Feature Exchange and Activation Region Constraint](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Data-Free_Knowledge_Distillation_via_Feature_Exchange_and_Activation_Region_Constraint_CVPR_2023_paper.pdf)
  - [ ]  [Supervised Masked Knowledge Distillation for Few-Shot Transformers](https://ar5iv.org/abs/2303.15466)<br>:star:[code](https://github.com/HL-hanlin/SMKD)
  - [ ]  [Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation](https://ar5iv.org/abs/2303.14666)<br>:star:[code](https://github.com/tianlizhang/OKDPH)
  - [ ]  [KD-DLGAN: Data Limited Image Generation via Knowledge Distillation](http://ar5iv.org/abs/2303.17158v1)
  - [ ]  [TinyMIM: An Empirical Study of Distilling MIM Pre-Trained Models](https://ar5iv.org/abs/2301.01296)(https://github.com/OliverRensu/TinyMIM)
  - [ ]  [Masked Autoencoders Enable Efficient Knowledge Distillers](https://ar5iv.org/abs/2208.12256)<br>:star:[code]<br>:star:[code](https://github.com/UCSC-VLAA/DMAE)
  - [ ]  [Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning](http://ar5iv.org/abs/2304.06461v1)
  - [ ]  [Class Attention Transfer Based Knowledge Distillation](http://ar5iv.org/abs/2304.12777v1)<br>:star:[code](https://github.com/GzyAftermath/CAT-KD)
  - [ ]  [DaFKD: Domain-Aware Federated Knowledge Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DaFKD_Domain-Aware_Federated_Knowledge_Distillation_CVPR_2023_paper.pdf)
  - [ ]  [Multi-Level Logit Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Multi-Level_Logit_Distillation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Jin-Ying/Multi-Level-Logit-Distillation)
  - [ ]  [A Unified Knowledge Distillation Framework for Deep Directed Graphical Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_A_Unified_Knowledge_Distillation_Framework_for_Deep_Directed_Graphical_Models_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/YizhuoChen99/KD4DGM-CVPR)
  - [ ]  [Enhanced Multimodal Representation Learning with Cross-modal KD](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Enhanced_Multimodal_Representation_Learning_With_Cross-Modal_KD_CVPR_2023_paper.pdf)
  - [ ]  [Constructing Deep Spiking Neural Networks From Artificial Neural Networks With Knowledge Distillation](http://ar5iv.org/abs/2304.05627)
  - [ ]  [Learning To Retain While Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation](http://ar5iv.org/abs/2302.14290)
  - [ ]  对抗性蒸馏
    - [ ]  [Boosting Accuracy and Robustness of Student Models via Adaptive Adversarial Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Boosting_Accuracy_and_Robustness_of_Student_Models_via_Adaptive_Adversarial_CVPR_2023_paper.pdf) 
- [ ]  轻量级网络
  - [ ]  [FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network](https://ar5iv.org/abs/2211.15069)<br>:star:[code](http://github.com/SJTU-ViSYS/FeatureBooster)
- [ ]  去量化
  - [ ]  [ABCD: Arbitrary Bitwise Coefficient for De-Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Han_ABCD_Arbitrary_Bitwise_Coefficient_for_De-Quantization_CVPR_2023_paper.pdf)

<a name="33"/>

## 33.Human-Object Interaction(人物交互)
- [ ]  [Visibility Aware Human-Object Interaction Tracking From Single RGB Camera](http://ar5iv.org/abs/2303.16479)
- [ ]  [Affordance Diffusion: Synthesizing Hand-Object Interactions](http://ar5iv.org/abs/2303.12538)
- [ ]  [HOICLIP: Efficient Knowledge Transfer for HOI Detection With Vision-Language Models](https://ar5iv.org/abs/2303.15786)<br>:star:[code](https://github.com/Artanic30/HOICLIP)
- [ ]  [ViPLO: Vision Transformer Based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection](https://ar5iv.org/abs/2304.08114)<br>:star:[code](https://github.com/Jeeseung-Park/ViPLO)
- [ ]  [Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf)
- [ ]  [Detecting Human-Object Contact in Images](https://ar5iv.org/abs/2303.03373)<br>:house:[project](https://hot.is.tue.mpg.de/)
- [ ]  [Category Query Learning for Human-Object Interaction Classification](http://ar5iv.org/abs/2303.14005v1)
- [ ]  [Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention](http://ar5iv.org/abs/2303.15274v1)
- [ ]  [Relational Context Learning for Human-Object Interaction Detection](http://ar5iv.org/abs/2304.04997v1)
- [ ]  [HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models](http://ar5iv.org/abs/2303.15786v1)<br>:star:[code](https://github.com/Artanic30/HOICLIP)
- [ ]  [ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection](http://ar5iv.org/abs/2304.08114v1)<br>:star:[code](https://github.com/Jeeseung-Park/ViPLO)
- [ ]  [Visibility Aware Human-Object Interaction Tracking from Single RGB Camera](http://ar5iv.org/abs/2303.16479v1)<br>:house:[project](https://virtualhumans.mpi-inf.mpg.de/VisTracker)
- [ ]  [Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream](http://ar5iv.org/abs/2304.03184v1)
- [ ]  [A Neural Modeling Pipeline on Multi-View Human-Object Interactions](https://ar5iv.org/abs/2212.07626)
- [ ]  双手交互
  - [ ]  [Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes](https://ar5iv.org/pdf/2302.14348.pdf)<br>:star:[code](https://github.com/jyunlee/Im2Hands)
- [ ]  手物交互
  - [ ]  [Visual-Tactile Sensing for In-Hand Object Reconstruction](http://ar5iv.org/abs/2303.14498v1)<br>:house:[project](https://sites.google.com/view/vtaco/)
  - [ ]  [CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis](http://ar5iv.org/abs/2303.15469v1)<br>:star:[code](https://cams-hoi.github.io/)
  - [ ]  [Transformer-Based Unified Recognition of Two Hands Manipulating Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Transformer-Based_Unified_Recognition_of_Two_Hands_Manipulating_Objects_CVPR_2023_paper.pdf)

<a name="32"/>

## 32.Data Augmentation(数据增强)
- [ ]  [Full or Weak annotations? An adaptive strategy for budget-constrained annotation campaigns](https://ar5iv.org/abs/2303.11678)
- [ ]  [SLACK: Stable Learning of Augmentations With Cold-Start and KL Regularization](https://openaccess.thecvf.com/content/CVPR2023/papers/Marrie_SLACK_Stable_Learning_of_Augmentations_With_Cold-Start_and_KL_Regularization_CVPR_2023_paper.pdf)<br>:star:[code](https://europe.naverlabs.com/slack)
- [ ]  学习库
  - [ ]  [PyPose: A Library for Robot Learning With Physics-Based Optimization](http://ar5iv.org/abs/2209.15428)
- [ ]  关键点定位
  - [ ]  [Few-shot Geometry-Aware Keypoint Localization](http://ar5iv.org/abs/2303.17216v1)<br>:star:[code](https://xingzhehe.github.io/FewShot3DKP/)
- [ ]  关键点检测
  - [ ]  [Continuous Landmark Detection With 3D Queries](https://openaccess.thecvf.com/content/CVPR2023/papers/Chandran_Continuous_Landmark_Detection_With_3D_Queries_CVPR_2023_paper.pdf)

<a name="31"/>

## 31.Vision-Language(视觉语言)
- [ ]  [Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.pdf)
- [ ]  [InternImage: Exploring Large-Scale Vision Foundation Models With Deformable Convolutions](http://ar5iv.org/abs/2211.05778)
- [ ]  [GIVL: Improving Geographical Inclusivity of Vision-Language Models With Pre-Training Methods](http://ar5iv.org/abs/2301.01893)
- [ ]  [Learning To Exploit Temporal Structure for Biomedical Vision-Language Processing](http://ar5iv.org/abs/2301.04558)
- [ ]  [REVEAL: Retrieval-Augmented Visual-Language Pre-Training With Multi-Source Multimodal Knowledge Memory](http://ar5iv.org/abs/2212.05221)
- [ ]  [Policy Adaptation from Foundation Model Feedback](https://ar5iv.org/abs/2212.07398)<br>:house:[project](https://geyuying.github.io/PAFF/)
- [ ]  [Learning Visual Representations via Language-Guided Sampling](http://ar5iv.org/abs/2302.12248)
- [ ]  [LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models](http://ar5iv.org/abs/2210.01115)
- [ ]  [Scaling Language-Image Pre-Training via Masking](https://ar5iv.org/abs/2212.00794)
- [ ]  [MAP: Multimodal Uncertainty-Aware Vision-Language Pre-Training Model](https://ar5iv.org/abs/2210.05335)
- [ ]  [Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles](https://ar5iv.org/abs/2211.16504)<br>:star:[code](https://github.com/pleaseconnectwifi/DANCE)
- [ ]  [Few-Shot Learning With Visual Distribution Calibration and Cross-Modal Distribution Alignment](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_Alignment_CVPR_2023_paper.pdf)<br>:star:[code](https://gitee.com/mindspore/models/tree/master/research/cv/SADA)
- [ ]  [ConStruct-VL: Data-Free Continual Structured VL Concepts Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_ConStruct-VL_Data-Free_Continual_Structured_VL_Concepts_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/jamessealesmith/ConStruct-VL)
- [ ]  [Teaching Structured Vision & Language Concepts to Vision & Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/SivanDoveh/TSVLC)
- [ ]  [Leveraging per Image-Token Consistency for Vision-Language Pre-Training](https://ar5iv.org/abs/2211.15398)
- [ ]  [Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf)<br>:house:[project](https://aka.ms/beit-3)
- [ ]  [CREPE: Can Vision-Language Foundation Models Reason Compositionally?](https://ar5iv.org/abs/2212.07796)
- [ ]  [Open-vocabulary Attribute Detection](https://ar5iv.org/abs/2211.12914)<br>:house:[project](https://ovad-benchmark.github.io/)
- [ ]  [Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training](https://ar5iv.org/abs/2301.02280)<br>:star:[code](https://github.com/facebookresearch/diht)
- [ ]  [FashionSAP: Symbols and Attributes Prompt for Fine-Grained Fashion Vision-Language Pre-Training](https://ar5iv.org/abs/2304.05051)
- [ ]  [Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Exploring_the_Effect_of_Primitives_for_Compositional_Generalization_in_Vision-and-Language_CVPR_2023_paper.pdf)
- [ ]  [Task Residual for Tuning Vision-Language Models](https://ar5iv.org/abs/2211.10277)<br>:star:[code](https://github.com/geekyutao/TaskRes)
- [ ]  [Masked Autoencoding Does Not Help Natural Language Supervision at Scale](https://ar5iv.org/abs/2301.07836)
- [ ]  [Open-Set Fine-Grained Retrieval via Prompting Vision-Language Evaluator](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Open-Set_Fine-Grained_Retrieval_via_Prompting_Vision-Language_Evaluator_CVPR_2023_paper.pdf)
- [ ]  [Visual-Language Prompt Tuning With Knowledge-Guided Context Optimization](https://ar5iv.org/abs/2303.13283)
- [ ]  [Position-Guided Text Prompt for Vision-Language Pre-Training](https://ar5iv.org/abs/2212.09737)<br>:star:[code](https://github.com/sail-sg/ptp)
- [ ]  [RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.pdf)
- [ ]  [FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks](https://ar5iv.org/abs/2303.02483)<br>:star:[code](https://github.com/BrandonHanx/FAME-ViL)
- [ ]  [Seeing What You Miss: Vision-Language Pre-Training With Semantic Completion Learning](https://ar5iv.org/abs/2211.13437)
- [ ]  [You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model](https://ar5iv.org/abs/2211.11152)
- [ ]  [DeAR: Debiasing Vision-Language Models with Additive Residuals](https://ar5iv.org/abs/2303.10431)
- [ ]  [Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning](https://ar5iv.org/abs/2303.05952)
- [ ]  [Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding](http://ar5iv.org/abs/2303.12513v1)<br>:star:[code](https://isbertblind.github.io/)
- [ ]  [VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining](http://ar5iv.org/abs/2303.14302v1)
- [ ]  [MAGVLT: Masked Generative Vision-and-Language Transformer](http://ar5iv.org/abs/2303.12208v1)
- [ ]  [Visual-Language Prompt Tuning with Knowledge-guided Context Optimization](http://ar5iv.org/abs/2303.13283v1)
- [ ]  [Top-Down Visual Attention from Analysis by Synthesis](http://ar5iv.org/abs/2303.13043v1)<br>:house:[project](https://sites.google.com/view/absvit)
- [ ]  [Accelerating Vision-Language Pretraining with Free Language Modeling](http://ar5iv.org/abs/2303.14038v1)<br>:star:[code](https://github.com/TencentARC/FLM)
- [ ]  [Multi-Modal Representation Learning with Text-Driven Soft Masks](http://ar5iv.org/abs/2304.00719v1)
- [ ]  [Fine-tuned CLIP models are efficient video learners](https://ar5iv.org/abs/2212.03640)<br>:star:[code](https://github.com/muzairkhattak/ViFi-CLIP)
- [ ]  [MaPLe: Multi-modal Prompt Learning](https://ar5iv.org/abs/2210.03117)<br>:star:[code](https://github.com/muzairkhattak/multimodal-prompt-learning)
- [ ]  [Learning to Name Classes for Vision and Language Models](http://ar5iv.org/abs/2304.01830v1)
- [ ]  [Dynamic Inference With Grounding Based Vision and Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Uzkent_Dynamic_Inference_With_Grounding_Based_Vision_and_Language_Models_CVPR_2023_paper.pdf)
- [ ]  [Connecting Vision and Language with Video Localized Narratives](https://ar5iv.org/abs/2302.11217)<br>:house:[project](https://google.github.io/video-localized-narratives/)
- [ ]  [Bidirectional Cross-Modal Knowledge Exploration for Video Recognition With Pre-Trained Vision-Language Models](https://ar5iv.org/abs/2301.00182)<br>:star:[code](https://github.com/whwu95/BIKE)
- [ ]  [Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks](https://ar5iv.org/abs/2211.09808)<br>:star:[code](https://github.com/fundamentalvision/Uni-Perceiver)
- [ ]  [VILA: Learning Image Aesthetics From User Comments With Vision-Language Pretraining](https://ar5iv.org/abs/2303.14302)<br>:star:[code](https://github.com/google-research/google-research/tree/master/vila)
- [ ]  VLN
  - [ ]  [Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation Using Scene Object Spectrum Grounding](https://ar5iv.org/abs/2303.04077)<br>:house:[project](https://rllab-snu.github.io/projects/Meta-Explore/doc.html)
  - [ ]  [Lana: A Language-Capable Navigator for Instruction Following and Generation](https://ar5iv.org/abs/2303.08409)<br>:star:[code](https://github.com/wxh1996/LANA-VLN)
  - [ ]  [LANA: A Language-Capable Navigator for Instruction Following and Generation](https://ar5iv.org/abs/2303.08409)
  - [ ]  [KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation](http://ar5iv.org/abs/2303.15796v1)<br>:star:[code](https://github.com/XiangyangLi20/KERM)
  - [ ]  [Improving Vision-and-Language Navigation by Generating Future-View Image Semantics](http://ar5iv.org/abs/2304.04907v1)<br>:star:[code](https://jialuli-luka.github.io/VLN-SIG)
  - [ ]  [Iterative Vision-and-Language Navigation](https://ar5iv.org/abs/2210.03087)
  - [ ]  [Behavioral Analysis of Vision-and-Language Navigation Agents](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Behavioral_Analysis_of_Vision-and-Language_Navigation_Agents_CVPR_2023_paper.pdf)
  - [ ]  [Adaptive Zone-Aware Hierarchical Planner for Vision-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf)
  - [ ]  [GeoVLN: Learning Geometry-Enhanced Visual Representation With Slot Attention for Vision-and-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huo_GeoVLN_Learning_Geometry-Enhanced_Visual_Representation_With_Slot_Attention_for_Vision-and-Language_CVPR_2023_paper.pdf)
  - [ ]  [A New Path: Scaling Vision-and-Language Navigation With Synthetic Instructions and Imitation Learning](https://ar5iv.org/abs/2210.03112)
  - [ ]  [Layout-Based Causal Inference for Object Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Layout-Based_Causal_Inference_for_Object_Navigation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sx-zhang/Layout-based-sTDE.git)
  - [ ]  [KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation](http://ar5iv.org/abs/2303.15796)
- [ ]  视频语言
  - [ ]  [Test of Time: Instilling Video-Language Models with a Sense of Time](https://ar5iv.org/abs/2301.02074)<br>:house:[project](https://bpiyush.github.io/testoftime-website/index.html)
  - [ ]  [All in One: Exploring Unified Video-Language Pre-Training](https://ar5iv.org/abs/2203.07303)<br>:star:[code](https://github.com/showlab/all-in-one)
  - [ ]  [HierVL: Learning Hierarchical Video-Language Embeddings](https://ar5iv.org/abs/2301.02311)
  - [ ]  [An Empirical Study of End-to-End Video-Language Transformers With Masked Visual Modeling](https://ar5iv.org/abs/2209.01540)<br>:star:[code](https://github.com/tsujuifu/pytorch_empirical-mvm)
  - [ ]  [Clover: Towards A Unified Video-Language Alignment and Fusion Model](https://ar5iv.org/abs/2207.07885)<br>:star:[code](https://github.com/LeeYN-43/Clover)<br>Clover 视频-文本预训练模型在 DiDeMo、MSRVTT 和 LSMDC 三个文本-视频检索任务上取得了 zero-shot 及 finetune performance 的最佳表现；在 8 个主流的视频问答 benchmark 上也达到了新的 state-of-the-art。
  - [ ]  [VindLU: A Recipe for Effective Video-and-Language Pretraining](https://ar5iv.org/abs/2212.05051)<br>:star:[code](https://github.com/klauscc/VindLU)
- [ ]  LLM
  - [ ]  [Learning Video Representations From Large Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Learning_Video_Representations_From_Large_Language_Models_CVPR_2023_paper.pdf)
- [ ]  visual grounding
  - [ ]  [EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding](https://ar5iv.org/abs/2209.14941)<br>:star:[code](https://github.com/yanmin-wu/EDA)
- [ ]  视觉对话
  - [ ]  [The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training](http://ar5iv.org/abs/2205.12502)


<a name="30"/>

## 30.Visual Answer Questions(视觉问答)
- [ ]  VQA
  - [ ]  [SimVQA: Exploring Simulated Environments for Visual Question Answering](https://ar5iv.org/abs/2203.17219)<br>:house:[project](https://simvqa.github.io)
  - [ ]  [From Images to Textual Prompts: Zero-Shot Visual Question Answering With Frozen Large Language Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_From_Images_to_Textual_Prompts_Zero-Shot_Visual_Question_Answering_With_CVPR_2023_paper.pdf)
  - [ ]  [Logical Implications for Visual Question Answering Consistency](https://openaccess.thecvf.com/content/CVPR2023/papers/Tascon-Morales_Logical_Implications_for_Visual_Question_Answering_Consistency_CVPR_2023_paper.pdf)
  - [ ]  [S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_S3C_Semi-Supervised_VQA_Natural_Language_Explanation_via_Self-Critical_Learning_CVPR_2023_paper.pdf)
  - [ ]  [RMLVQA: A Margin Loss Approach for Visual Question Answering With Language Biases](https://openaccess.thecvf.com/content/CVPR2023/papers/Basu_RMLVQA_A_Margin_Loss_Approach_for_Visual_Question_Answering_With_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/val-iisc/RMLVQA)
  - [ ]  [VQACL: A Novel Visual Question Answering Continual Learning Setting](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_VQACL_A_Novel_Visual_Question_Answering_Continual_Learning_Setting_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhangxi1997/VQACL)
  - [ ]  [Q: How To Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!](https://openaccess.thecvf.com/content/CVPR2023/papers/Khan_Q_How_To_Specialize_Large_Vision-Language_Models_to_Data-Scarce_VQA_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/codezakh/SelTDA)
  - [ ]  [Improving Selective Visual Question Answering by Learning From Your Peers](https://openaccess.thecvf.com/content/CVPR2023/papers/Dancette_Improving_Selective_Visual_Question_Answering_by_Learning_From_Your_Peers_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/selective-vqa_ood)
  - [ ]  [MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering](https://ar5iv.org/pdf/2303.01239.pdf)
  - [ ]  [Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering](https://ar5iv.org/abs/2303.01903)<br>:star:[code](https://github.com/MILVLG/prophet)
  - [ ]  [MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos](http://ar5iv.org/abs/2303.14933v1)
  - [ ]  [Divide and Conquer: Answering Questions with Object Factorization and Compositional Reasoning](https://ar5iv.org/abs/2303.10482)<br>:star:[code](https://github.com/szzexpoi/POEM)
  - [ ]  [Prompting Large Language Models With Answer Heuristics for Knowledge-Based Visual Question Answering](https://ar5iv.org/abs/2303.01903)<br>:star:[code](https://github.com/MILVLG/prophet)
   - [ ]  [Generative Bias for Robust Visual Question Answering](https://ar5iv.org/abs/2208.00690)
- [ ]  Video-QA
  - [ ]  [Learning Situation Hyper-Graphs for Video Question Answering](https://ar5iv.org/abs/2304.08682)
  - [ ]  [Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering](https://openaccess.thecvf.com/content/CVPR2023/papers/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Chuanqi-Zang/Discovering-the-Real-Association)
  - [ ]  [MIST: Multi-Modal Iterative Spatial-Temporal Transformer for Long-Form Video Question Answering](http://ar5iv.org/abs/2212.09522)

<a name="29"/>

## 29.SLAM/Augmented Reality/Virtual Reality/Robotics(增强/虚拟现实/机器人)
- [ ]  机器人
  - [ ]  [PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations](http://ar5iv.org/abs/2303.16958v1)
  - [ ]  [Affordances From Human Videos as a Versatile Representation for Robotics](http://ar5iv.org/abs/2304.08488)
  - [ ]  [Markerless Camera-to-Robot Pose Estimation via Self-supervised Sim-to-Real Transfer](https://ar5iv.org/abs/2302.14332)
  - [ ]  [Robot Structure Prior Guided Temporal Attention for Camera-to-Robot Pose Estimation From Image Sequence](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Robot_Structure_Prior_Guided_Temporal_Attention_for_Camera-to-Robot_Pose_Estimation_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/sgtapose)
  - [ ]  [Phone2Proc: Bringing Robust Robots Into Our Chaotic World](https://ar5iv.org/abs/2212.04819)<br>:house:[project](https://allenai.org/project/phone2proc)
  - [ ]  [DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects](http://ar5iv.org/abs/2305.05706v1)<br>:house:[project](https://www.chenbao.tech/dexart/)
  - [ ]  [Learning Human-to-Robot Handovers from Point Clouds](http://ar5iv.org/abs/2303.17592v1)<br>:star:[code](https://handover-sim2real.github.io)
  - [ ]  [Neural Volumetric Memory for Visual Locomotion Control](http://ar5iv.org/abs/2304.01201v1)<br>:star:[code](https://rchalyang.github.io/NVM)
  - [ ]  [Affordances from Human Videos as a Versatile Representation for Robotics](http://ar5iv.org/abs/2304.08488v1)<br>:star:[code](https://robo-affordances.github.io/)
  - [ ]  [NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models](http://ar5iv.org/abs/2304.09787v1)机器人
  - [ ]  机器手抓取
    - [ ]  [UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy](https://ar5iv.org/abs/2303.00938)<br>:house:[project](https://pku-epic.github.io/UniDexGrasp/)
    - [ ]  [UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning](http://ar5iv.org/abs/2304.00464v1)
    - [ ]  [Target-Referenced Reactive Grasping for Dynamic Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Target-Referenced_Reactive_Grasping_for_Dynamic_Objects_CVPR_2023_paper.pdf)<br>:house:[project](https://graspnet.net/reactive)
  - [ ]  Visual Navigation(视觉导航)
    - [ ]  [Renderable Neural Radiance Map for Visual Navigation](https://ar5iv.org/pdf/2303.00304.pdf)
    - [ ]  [Object-Goal Visual Navigation via Effective Exploration of Relations Among Historical Navigation States](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Object-Goal_Visual_Navigation_via_Effective_Exploration_of_Relations_Among_Historical_CVPR_2023_paper.pdf)
- [ ]  SLAM
  - [ ]  [Efficient Map Sparsification Based on 2D and 3D Discretized Grids](https://ar5iv.org/abs/2303.10882)<br>:star:[code](https://github.com/fishmarch/SLAM_Map_Compression)
  - [ ]  [Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM](http://ar5iv.org/abs/2304.14377v1)<br>:star:[code](https://hengyiwang.github.io/projects/CoSLAM)
  - [ ]  [ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields](https://ar5iv.org/abs/2211.11704)<br>:house:[project](https://www.idiap.ch/paper/eslam/)
  - [ ]  [ObjectMatch: Robust Registration Using Canonical Object Correspondences](https://openaccess.thecvf.com/content/CVPR2023/papers/Gumeli_ObjectMatch_Robust_Registration_Using_Canonical_Object_Correspondences_CVPR_2023_paper.pdf)<br>:house:[project](https://cangumeli.github.io/ObjectMatch/)
  - [ ]  [vMAP: Vectorised Object Mapping for Neural Field SLAM](https://ar5iv.org/abs/2302.01838)<br>:house:[project](https://kxhit.github.io/vMAP)
- [ ]  虚拟试穿
  - [ ]  [GP-VTON: Towards General Purpose Virtual Try-on via Collaborative Local-Flow Global-Parsing Learning](https://ar5iv.org/abs/2303.13756)<br>:star:[code](https://github.com/xiezhy6/GP-VTON)
  - [ ]  [TryOnDiffusion: A Tale of Two UNets](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf)
  - [ ]  [Linking Garment With Person via Semantically Associated Landmarks for Virtual Try-On](https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Linking_Garment_With_Person_via_Semantically_Associated_Landmarks_for_Virtual_CVPR_2023_paper.pdf)<br>:house:[project](https://modelscope.cn/datasets/damo/SAL-HG/summary)
  - [ ]  [Synthesizing Photorealistic Virtual Humans Through Cross-Modal Disentanglement](https://ar5iv.org/abs/2209.01320)
- [ ]  AR/VR
  - [ ]  [Affordance Grounding from Demonstration Video to Target Image](http://ar5iv.org/abs/2303.14644v1)<br>:star:[code](https://github.com/showlab/afformer)
  - [ ]  [GarmentTracking: Category-Level Garment Pose Tracking](https://ar5iv.org/abs/2303.13913)<br>:house:[project](https://garment-tracking.robotflow.ai/)
  - [ ]  [Object Pop-Up: Can We Infer 3D Objects and Their Poses From Human Interactions Alone?](https://openaccess.thecvf.com/content/CVPR2023/papers/Petrov_Object_Pop-Up_Can_We_Infer_3D_Objects_and_Their_Poses_CVPR_2023_paper.pdf)
  - [ ]  [Learning to Zoom and Unzoom](http://ar5iv.org/abs/2303.15390v1)<br>:star:[code](https://tchittesh.github.io/lzu/)
  - [ ]  [Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-time Mobile Telepresence](http://ar5iv.org/abs/2304.11835v1)
  - [ ]  [Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model](http://ar5iv.org/abs/2304.08577v1)<br>:star:[code](https://dulucas.github.io/agrol/)VR/AR
  - [ ]  [Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-Time Mobile Telepresence](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Auto-CARD_Efficient_and_Robust_Codec_Avatar_Driving_for_Real-Time_Mobile_CVPR_2023_paper.pdf)
  - [ ]  [Affordance Grounding From Demonstration Video To Target Image](https://ar5iv.org/abs/2303.14644)<br>:star:[code](https://github.com/showlab/afformer)
  - [ ]  [Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video](https://ar5iv.org/abs/2211.12782)<br>:house:[project](https://seanchenxy.github.io/HandAvatarWeb)
- [ ]  混合现实
  - [ ]  [MixSim: A Hierarchical Framework for Mixed Reality Traffic Simulation](https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_MixSim_A_Hierarchical_Framework_for_Mixed_Reality_Traffic_Simulation_CVPR_2023_paper.pdf)<br>:house:[project](https://waabi.ai/research/mixsim/)
- [ ]  Visual Localization(视觉定位)
  - [ ]  [OrienterNet: Visual Localization in 2D Public Maps with Neural Matching](http://ar5iv.org/abs/2304.02009v1)
  - [ ]  [Visual Localization using Imperfect 3D Models from the Internet](http://ar5iv.org/abs/2304.05947v1)
  - [ ]  [SFD2: Semantic-Guided Feature Detection and Description](https://ar5iv.org/abs/2304.14845)<br>:star:[code](https://github.com/feixue94/sfd2)
  - [ ]  [SegLoc: Learning Segmentation-Based Representations for Privacy-Preserving Visual Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Pietrantoni_SegLoc_Learning_Segmentation-Based_Representations_for_Privacy-Preserving_Visual_Localization_CVPR_2023_paper.pdf)
  - [ ]  [Long-term Visual Localization with Mobile Sensors](https://ar5iv.org/abs/2304.07691)
  - [ ]  [Paired-Point Lifting for Enhanced Privacy-Preserving Visual Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Paired-Point_Lifting_for_Enhanced_Privacy-Preserving_Visual_Localization_CVPR_2023_paper.pdf)
- [ ]  VPR(Visual Place Recognition)
  - [ ]  [StructVPR: Distill Structural Knowledge With Weighting Samples for Visual Place Recognition](https://ar5iv.org/abs/2212.00937)
  - [ ]  [Data-efficient Large Scale Place Recognition with Graded Similarity Supervision](https://ar5iv.org/abs/2303.11739)<br>:star:[code](https://github.com/marialeyvallina/generalized_contrastive_loss)
- [ ]  视觉里程计
  - [ ]  [PVO: Panoptic Visual Odometry](https://ar5iv.org/abs/2207.01610)<br>:star:[code](https://github.com/zju3dv/PVO)
  - [ ]  [Modality-Invariant Visual Odometry for Embodied Vision](https://ar5iv.org/abs/2305.00348) 

<a name="28"/>

## 28.Style Transfer(风格迁移)
- [ ]  [CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer](http://ar5iv.org/abs/2303.17867v1)
- [ ]  [StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer](http://ar5iv.org/abs/2304.02744v1)<br>:star:[code](https://stylegan-salon.github.io/)
- [ ]  [Modernizing Old Photos Using Multiple References via Photorealistic Style Transfer](http://ar5iv.org/abs/2304.04461v1)<br>:star:[code](https://kaist-viclab.github.io/old-photo-modernization)
- [ ]  [Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer](http://ar5iv.org/abs/2304.11818v1)
- [ ]  [Neural Preset for Color Style Transfer](https://ar5iv.org/abs/2303.13511)<br>:house:[project](https://zhkkke.github.io/NeuralPreset)
- [ ]  [Learning Dynamic Style Kernels for Artistic Style Transfer](https://ar5iv.org/abs/2304.00414)
- [ ]  [Inversion-Based Style Transfer With Diffusion Models](https://ar5iv.org/abs/2211.13203)<br>:star:[code](https://github.com/zyxElsa/InST)
- [ ]  [QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity](https://ar5iv.org/abs/2212.10431)<br>:star:[code](https://github.com/siyuhuang/QuantArt)
- [ ]  文本驱动的室内风格化
  - [ ]  [Text2Scene: Text-Driven Indoor Scene Stylization With Part-Aware Details](https://openaccess.thecvf.com/content/CVPR2023/papers/Hwang_Text2Scene_Text-Driven_Indoor_Scene_Stylization_With_Part-Aware_Details_CVPR_2023_paper.pdf)

<a name="27"/>

## 27.Pose Estimation(物体姿势估计)
- [ ]  物体姿势估计
  - [ ]  [Object Pose Estimation with Statistical Guarantees: Conformal Keypoint Detection and Geometric Uncertainty Propagation](http://ar5iv.org/abs/2303.12246v1)
  - [ ]  [SMOC-Net: Leveraging Camera Pose for Self-Supervised Monocular Object Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_SMOC-Net_Leveraging_Camera_Pose_for_Self-Supervised_Monocular_Object_Pose_Estimation_CVPR_2023_paper.pdf)
  - [ ]  [HS-Pose: Hybrid Scope Feature Extraction for Category-level Object Pose Estimation](http://ar5iv.org/abs/2303.15743v1)
  - [ ]  [TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation](http://ar5iv.org/abs/2303.16730v1)<br>:house:[project](https://taeyeop.com/ttacope)
  - [ ]  [IMP: Iterative Matching and Pose Estimation with Adaptive Pooling](https://ar5iv.org/abs/2304.14837)<br>:star:[code](https://github.com/feixue94/imp-release)
- [ ]  6D
  - [ ]  [Rigidity-Aware Detection for 6D Object Pose Estimation](http://ar5iv.org/abs/2303.12396v1)
  - [ ]  [Neural Texture Learning for Self-Supervised 6D Object Pose Estimation](https://ar5iv.org/abs/2212.12902)
  - [ ]  [Shape-Constraint Recurrent Flow for 6D Object Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Hai_Shape-Constraint_Recurrent_Flow_for_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf)
  - [ ]  [Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions](https://ar5iv.org/abs/2205.14971)
  - [ ]  [HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling](https://openaccess.thecvf.com/content/CVPR2023/papers/Attal_HyperReel_High-Fidelity_6-DoF_Video_With_Ray-Conditioned_Sampling_CVPR_2023_paper.pdf)<br>:house:[project](https://hyperreel.github.io/)
- [ ]  4D
  - [ ]  [Transfer4D: A Framework for Frugal Motion Capture and Deformation Transfer](https://openaccess.thecvf.com/content/CVPR2023/papers/Maheshwari_Transfer4D_A_Framework_for_Frugal_Motion_Capture_and_Deformation_Transfer_CVPR_2023_paper.pdf)
- [ ]  动物姿态估计
  - [ ]  [ScarceNet: Animal Pose Estimation with Scarce Annotations](http://ar5iv.org/abs/2303.15023v1)
  - [ ]  [Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Matching_Is_Not_Enough_A_Two-Stage_Framework_for_Category-Agnostic_Pose_CVPR_2023_paper.pdf)<br>:star:[code](github.com/flyinglynx/CapeFormer)
  - [ ]  [BITE: Beyond Priors for Improved Three-D Dog Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruegg_BITE_Beyond_Priors_for_Improved_Three-D_Dog_Pose_Estimation_CVPR_2023_paper.pdf)<br>:house:[project](https://bite.is.tue.mpg.de/)

<a name="26"/>

## 26.GCN/GNN
- [ ]  GNN
  - [ ]  [Turning Strengths Into Weaknesses: A Certified Robustness Inspired Attack Framework Against Graph Neural Networks](https://ar5iv.org/abs/2303.06199)
  - [ ]  [From Node Interaction To Hop Interaction: New Effective and Scalable Graph Learning Paradigm](https://ar5iv.org/abs/2211.11761)<br>:star:[code](https://github.com/JC-202/HopGNN)

<a name="25"/>

## 25.Fine-Grained/Image Classification(细粒度/图像分类)
- [ ]  [Quantum-Inspired Spectral-Spatial Pyramid Network for Hyperspectral Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Quantum-Inspired_Spectral-Spatial_Pyramid_Network_for_Hyperspectral_Image_Classification_CVPR_2023_paper.pdf)
- [ ]  [Learning Partial Correlation Based Deep Visual Representation for Image Classification](http://ar5iv.org/abs/2304.11597)
- [ ]  [iCLIP: Bridging Image Classification and Contrastive Language-Image Pre-Training for Visual Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_iCLIP_Bridging_Image_Classification_and_Contrastive_Language-Image_Pre-Training_for_Visual_CVPR_2023_paper.pdf)
- [ ]  [I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification](https://ar5iv.org/abs/2212.02291)
- [ ]  [Soft Augmentation for Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Soft_Augmentation_for_Image_Classification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/youngleox/soft_augmentation)
- [ ]  [Explaining Image Classifiers With Multiscale Directional Image Representation](https://openaccess.thecvf.com/content/CVPR2023/papers/Kolek_Explaining_Image_Classifiers_With_Multiscale_Directional_Image_Representation_CVPR_2023_paper.pdf)
- [ ]  [Equiangular Basis Vectors](https://ar5iv.org/abs/2303.11637)<br>:star:[code](https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors)
- [ ]  [Prefix Conditioning Unifies Language and Label Supervision](https://ar5iv.org/abs/2206.01125)
- [ ]  [Improving Image Recognition by Retrieving from Web-Scale Image-Text Data](http://ar5iv.org/abs/2304.05173v1)
- [ ]  [Boosting Verified Training for Robust Image Classifications via Abstraction](https://ar5iv.org/abs/2303.11552)<br>:star:[code](https://github.com/zhangzhaodi233/ABSCERT.git)
- [ ]  [Semantic Prompt for Few-Shot Image Recognition](http://ar5iv.org/abs/2303.14123v1)
- [ ]  [Regularization of polynomial networks for image recognition](http://ar5iv.org/abs/2303.13896v1)<br>:star:[code](https://github.com/grigorisg9gr/regularized_polynomials)
- [ ]  [Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm](http://ar5iv.org/abs/2303.14382v1)<br>:star:[code](https://github.com/yichen928/ActiveFT)
- [ ]  [Dynamic Conceptional Contrastive Learning for Generalized Category Discovery](http://ar5iv.org/abs/2303.17393v1)<br>:star:[code](https://github.com/TPCD/DCCL)
- [ ]  [Learning Bottleneck Concepts in Image Classification](http://ar5iv.org/abs/2304.10131v1)<br>:house:[project](https://botcl.liangzhili.com/)<br>:star:[code](https://github.com/wbw520/BotCL)
- [ ]  [Learning Partial Correlation based Deep Visual Representation for Image Classification](http://ar5iv.org/abs/2304.11597v1)
- [ ]  [PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/M-Nauta/PIPNet)
- [ ]  [Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification](https://ar5iv.org/abs/2211.11158)
- [ ]  小样本图像分类
  - [ ]  [ProD: Prompting-To-Disentangle Domain Knowledge for Cross-Domain Few-Shot Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_ProD_Prompting-To-Disentangle_Domain_Knowledge_for_Cross-Domain_Few-Shot_Image_Classification_CVPR_2023_paper.pdf)
- [ ]  小样本分类
  - [ ]  [Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners](https://ar5iv.org/abs/2303.02151)<br>:star:[code](https://github.com/ZrrSkywalker/CaFo)
  - [ ]  [Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-shot Learning with Hyperspherical Embeddings](https://ar5iv.org/abs/2303.09352)<br>:star:[code](https://github.com/uitml/noHub)
  - [ ]  [Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Distilling_Self-Supervised_Vision_Transformers_for_Weakly-Supervised_Few-Shot_Classification__Segmentation_CVPR_2023_paper.pdf)
- [ ]  细粒度
  - [ ]  [Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems](https://ar5iv.org/abs/2303.01669)<br>:star:[code](https://github.com/GANPerf/LCR)
  - [ ]  [Fine-Grained Classification with Noisy Labels](https://ar5iv.org/abs/2303.02404)
  - [ ]  [An Erudite Fine-Grained Visual Classification Model](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_An_Erudite_Fine-Grained_Visual_Classification_Model_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/PRIS-CV/An-Erudite-FGVC-Model)
  - [ ]  [Weakly Supervised Posture Mining for Fine-Grained Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Weakly_Supervised_Posture_Mining_for_Fine-Grained_Classification_CVPR_2023_paper.pdf)
  - [ ]  [Learning Attribute and Class-Specific Representation Duet for Fine-Grained Fashion Analysis](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_Learning_Attribute_and_Class-Specific_Representation_Duet_for_Fine-Grained_Fashion_Analysis_CVPR_2023_paper.pdf)
- [ ]  视觉识别
  - [ ]  [Adapting Shortcut With Normalizing Flow: An Efficient Tuning Framework for Visual Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adapting_Shortcut_With_Normalizing_Flow_An_Efficient_Tuning_Framework_for_CVPR_2023_paper.pdf)
- [ ]  长尾分类
  - [ ]  [Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification](http://ar5iv.org/abs/2303.12307v1)
- [ ]  长尾视觉识别
  - [ ]  [SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail](http://ar5iv.org/abs/2304.00101v1)
  - [ ]  [Balanced Product of Calibrated Experts for Long-Tailed Recognition](https://ar5iv.org/abs/2206.05260)<br>:star:[code](https://github.com/emasa/BalPoE-CalibratedLT)
  - [ ]  [FCC: Feature Clusters Compression for Long-Tailed Visual Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_FCC_Feature_Clusters_Compression_for_Long-Tailed_Visual_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/lijian16/FCC)
  - [ ]  [Long-tailed Visual Recognition via Gaussian Clouded Logit Adjustment](http://ar5iv.org/abs/2305.11733v1)<br>:star:[code](https://github.com/Keke921/GCLLoss)
  - [ ]  [Global and Local Mixture Consistency Cumulative Learning for Long-tailed Visual Recognitions](http://ar5iv.org/abs/2305.08661v1)<br>:star:[code](https://github.com/ynu-yangpeng/GLMC)
  - [ ]  [Long-Tailed Visual Recognition via Self-Heterogeneous Integration with Knowledge Excavation](http://ar5iv.org/abs/2304.01279v1)
  - [ ]  [Class-Conditional Sharpness-Aware Minimization for Deep Long-Tailed Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Class-Conditional_Sharpness-Aware_Minimization_for_Deep_Long-Tailed_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zzpustc/CC-SAM)
  - [ ]  [No One Left Behind: Improving the Worst Categories in Long-Tailed Learning](https://ar5iv.org/abs/2303.03630)
- [ ]  多标签分类
  - [ ]  [Bridging the Gap between Model Explanations in Partially Annotated Multi-label Classification](http://ar5iv.org/abs/2304.01804v1)<br>:star:[code](https://github.com/youngwk/BridgeGapExplanationPAMC)
- [ ]  多标签识别
  - [ ]  [Exploring Structured Semantic Prior for Multi Label Recognition With Incomplete Labels](https://ar5iv.org/abs/2303.13223)<br>:star:[code](https://github.com/jameslahm/SCPNet)    
  - [ ]  [Texts as Images in Prompt Tuning for Multi-Label Image Recognition](https://ar5iv.org/abs/2211.12739)<br>:star:[code](https://github.com/guozix/TaI-DPT)
- [ ]  多视觉分类
  - [ ]  [Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification](https://ar5iv.org/abs/2304.05165)
- [ ]  Superclass Learning(超类学习)
  - [ ]  [Superclass Learning with Representation Enhancement](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Superclass_Learning_With_Representation_Enhancement_CVPR_2023_paper.pdf)
- [ ]  材料分类
  - [ ]  [Thermal Spread Functions (TSF): Physics-Guided Material Classification](https://ar5iv.org/abs/2304.00696)

<a name="24"/>

## 24.Super-Resolution(超分辨率)
- [ ]  [Cascaded Local Implicit Transformer for Arbitrary-Scale Super-Resolution](http://ar5iv.org/abs/2303.16513)
- [ ]  [Deep Arbitrary-Scale Image Super-Resolution via Scale-Equivariance Pursuit](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Arbitrary-Scale_Image_Super-Resolution_via_Scale-Equivariance_Pursuit_CVPR_2023_paper.pdf)
- [ ]  [N-Gram in Swin Transformers for Efficient Lightweight Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_N-Gram_in_Swin_Transformers_for_Efficient_Lightweight_Image_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/rami0205/NGramSwin)
- [ ]  [Perception-Oriented Single Image Super-Resolution Using Optimal Objective Estimation](https://ar5iv.org/abs/2211.13676)<br>:star:[code](https://github.com/seungho-snu/SROOE)
- [ ]  [Toward Stable, Interpretable, and Lightweight Hyperspectral Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Toward_Stable_Interpretable_and_Lightweight_Hyperspectral_Super-Resolution_CVPR_2023_paper.pdf)
- [ ]  [CiaoSR: Continuous Implicit Attention-in-Attention Network for Arbitrary-Scale Image Super-Resolution](http://ar5iv.org/abs/2212.04362)
- [ ]  [Zero-Shot Dual-Lens Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Zero-Shot_Dual-Lens_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/XrKang/ZeDuSR)
- [ ]  [Non-Line-of-Sight Imaging With Signal Superresolution Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Non-Line-of-Sight_Imaging_With_Signal_Superresolution_Network_CVPR_2023_paper.pdf)
- [ ]  [Kernel Aware Resampler](https://openaccess.thecvf.com/content/CVPR2023/papers/Bernasconi_Kernel_Aware_Resampler_CVPR_2023_paper.pdf)
- [ ]  [RobustNeRF: Ignoring Distractors With Robust Losses](https://openaccess.thecvf.com/content/CVPR2023/papers/Gou_Rethinking_Image_Super_Resolution_From_Long-Tailed_Distribution_Learning_Perspective_CVPR_2023_paper.pdf)
- [ ]  光场超分辨率
  - [ ]  [CutMIB: Boosting Light Field Super-Resolution via Multi-View Image Blending](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_CutMIB_Boosting_Light_Field_Super-Resolution_via_Multi-View_Image_Blending_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zeyuxiao1997/CutMIB)
- [ ]  ISR
  - [ ]  [OPE-SR: Orthogonal Position Encoding for Designing a Parameter-free Upsampling Module in Arbitrary-scale Image Super-Resolution](https://ar5iv.org/pdf/2303.01091.pdf)
  - [ ]  [Activating More Pixels in Image Super-Resolution Transformer](https://ar5iv.org/abs/2205.04437)<br>:star:[code](https://github.com/XPixelGroup/HAT)
  - [ ]  [Equivalent Transformation and Dual Stream Network Construction for Mobile Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Chao_Equivalent_Transformation_and_Dual_Stream_Network_Construction_for_Mobile_Image_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ECNUSR/ETDS)
  - [ ]  [Learning Generative Structure Prior for Blind Text Image Super-Resolution](http://ar5iv.org/abs/2303.14726)
  - [ ]  [Human Guided Ground-Truth Generation for Realistic Image Super-Resolution](https://ar5iv.org/abs/2303.13069)<br>:star:[code](https://github.com/ChrisDud0257/HGGT)
  - [ ]  [OSRT: Omnidirectional Image Super-Resolution With Distortion-Aware Transformer](https://ar5iv.org/abs/2302.03453)<br>:star:[code](https://github.com/Fanghua-Yu/OSRT)
  - [ ]  [CABM: Content-Aware Bit Mapping for Single Image Super-Resolution Network with Large Input](http://ar5iv.org/abs/2304.06454v1)
  - [ ]  [Memory-Friendly Scalable Super-Resolution via Rewinding Lottery Ticket Hypothesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Memory-Friendly_Scalable_Super-Resolution_via_Rewinding_Lottery_Ticket_Hypothesis_CVPR_2023_paper.pdf)
  - [ ]  [B-Spline Texture Coefficients Estimator for Screen Content Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Pak_B-Spline_Texture_Coefficients_Estimator_for_Screen_Content_Image_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ByeongHyunPak/btc)
  - [ ]  [Rethinking Image Super Resolution From Long-Tailed Distribution Learning Perspective](https://openaccess.thecvf.com/content/CVPR2023/papers/Gou_Rethinking_Image_Super_Resolution_From_Long-Tailed_Distribution_Learning_Perspective_CVPR_2023_paper.pdf)
  - [ ]  [Correspondence Transformers With Asymmetric Feature Learning and Matching Flow Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Correspondence_Transformers_With_Asymmetric_Feature_Learning_and_Matching_Flow_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/YXSUNMADMAX/ACTR)
  - [ ]  [Toward Accurate Post-Training Quantization for Image Super Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Toward_Accurate_Post-Training_Quantization_for_Image_Super_Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/huawei-noah/Efficient-Computing/tree/master/Quantization/PTQ4SR)
  - [ ]  [Image Super-Resolution Using T-Tetromino Pixels](https://openaccess.thecvf.com/content/CVPR2023/papers/Grosche_Image_Super-Resolution_Using_T-Tetromino_Pixels_CVPR_2023_paper.pdf)
  - [ ]  [Spectral Bayesian Uncertainty for Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Spectral_Bayesian_Uncertainty_for_Image_Super-Resolution_CVPR_2023_paper.pdf)
  - [ ]  [Super-Resolution Neural Operator](https://ar5iv.org/pdf/2303.02584.pdf)<br>:star:[code](https://github.com/2y7c3/Super-Resolution-Neural-Operator)
  - [ ]  [Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution](https://ar5iv.org/abs/2303.05156)
  - [ ]  [Better "CMOS" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution](http://ar5iv.org/abs/2304.03542v1)
  - [ ]  [Human Guided Ground-truth Generation for Realistic Image Super-resolution](http://ar5iv.org/abs/2303.13069v1)<br>:star:[code](https://github.com/ChrisDud0257/HGGT)
  - [ ]  [Implicit Diffusion Models for Continuous Super-Resolution](http://ar5iv.org/abs/2303.16491v1)
  - [ ]  [Better "CMOS" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution](https://ar5iv.org/abs/2304.03542)
  - [ ]  [Guided Depth Super-Resolution by Deep Anisotropic Diffusion](https://ar5iv.org/abs/2211.11592)<br>:star:[code](https://github.com/prs-eth/Diffusion-Super-Resolution)
  - [ ]  [Omni Aggregation Networks for Lightweight Image Super-Resolution](http://ar5iv.org/abs/2304.10244v1)<br>:star:[code](https://github.com/Francis0625/Omni-SR)
- [ ]  VSR
  - [ ]  [Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting](https://ar5iv.org/abs/2303.08331)<br>:star:[code](https://github.com/coulsonlee/STDO-CVPR2023.git)
  - [ ]  [Compression-Aware Video Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Compression-Aware_Video_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/aprBlue/CAVSR)
  - [ ]  [Structured Sparsity Learning for Efficient Video Super-Resolution](https://ar5iv.org/abs/2206.07687)<br>:star:[code](https://github.com/Zj-BinXia/SSL)
  - [ ]  [Consistent Direct Time-of-Flight Video Depth Super-Resolution](https://ar5iv.org/abs/2211.08658)<br>:star:[code](https://github.com/facebookresearch/DVSR/)
  - [ ]  [Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution](http://ar5iv.org/abs/2303.13767)
- [ ]  文本图像超分辨率
  - [ ]  [Learning Generative Structure Prior for Blind Text Image Super-resolution](http://ar5iv.org/abs/2303.14726v1)<br>:star:[code](https://github.com/csxmli2016/MARCONet)
- [ ]  Image Resampling(图像重采样)
  - [ ]  [Learning Steerable Function for Efficient Image Resampling](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Learning_Steerable_Function_for_Efficient_Image_Resampling_CVPR_2023_paper.pdf)


<a name="23"/>

## 23.Image Retrieval(图像检索)
- [ ]  [Towards a Smaller Student: Capacity Dynamic Distillation for Efficient Image Retrieval](https://ar5iv.org/abs/2303.09230)
- [ ]  [Asymmetric Feature Fusion for Image Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Asymmetric_Feature_Fusion_for_Image_Retrieval_CVPR_2023_paper.pdf)
- [ ]  [Improving Image Recognition by Retrieving From Web-Scale Image-Text Data](https://ar5iv.org/abs/2304.05173)
- [ ]  [Boundary-aware Backward-Compatible Representation via Adversarial Learning in Image Retrieval](http://ar5iv.org/abs/2305.02610v1)<br>:star:[code](https://github.com/Ashespt/AdvBCT)
- [ ]  [Revisiting Self-Similarity: Structural Embedding for Image Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Revisiting_Self-Similarity_Structural_Embedding_for_Image_Retrieval_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sungonce/SENet)
- [ ]  [Train/Test-Time Adaptation With Retrieval](https://ar5iv.org/abs/2303.14333)
- [ ]  [Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval](https://ar5iv.org/abs/2302.03084)<br>:star:[code](https://github.com/google-research/composed_image_retrieval)
- [ ]  基于草图的图像检索
  - [ ]  [Data-Free Sketch-Based Image Retrieval](https://ar5iv.org/abs/2303.07775)<br>:star:[code](https://github.com/abhrac/data-free-sbir)
  - [ ]  [CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not](http://ar5iv.org/abs/2303.13440v1)
  - [ ]  [Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR](http://ar5iv.org/abs/2303.13779v1)<br>:star:[code](https://aneeshan95.github.io/Sketch_PVT/)
  - [ ]  [Zero-Shot Everything Sketch-Based Image Retrieval, and in Explainable Style](http://ar5iv.org/abs/2303.14348v1)<br>:star:[code](https://github.com/buptLinfy/ZSE-SBIR)
- [ ]  视频-文本检索
  - [ ]  [Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning](http://ar5iv.org/abs/2303.14369v1)<br>:star:[code](https://jpthu17.github.io/HBI/)
  - [ ]  [Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Hao_Dual_Alignment_Unsupervised_Domain_Adaptation_for_Video-Text_Retrieval_CVPR_2023_paper.pdf)
- [ ]  视频-文本
  - [ ]  [SViTT: Temporal Learning of Sparse Video-Text Transformers](http://ar5iv.org/abs/2304.08809v1)<br>:house:[project](http://svcl.ucsd.edu/projects/svitt)视频文本检索和问答
- [ ]  多模态检索
  - [ ]  [ImageBind: One Embedding Space To Bind Them All](http://ar5iv.org/abs/2305.05665v1)<br>:house:[project](https://imagebind.metademolab.com/)<br>:star:[code](https://github.com/facebookresearch/ImageBind)
  - [ ]  [Pix2map: Cross-Modal Retrieval for Inferring Street Maps From Images](http://ar5iv.org/abs/2301.04224)
- [ ]  跨模态检索
  - [ ]  [VoP: Text-Video Co-Operative Prompt Tuning for Cross-Modal Retrieval](https://ar5iv.org/abs/2211.12764)<br>:star:[code](https://github.com/bighuang624/VoP)
  - [ ]  [Improving Cross-Modal Retrieval With Set of Diverse Embeddings](http://ar5iv.org/abs/2211.16761)
  - [ ]  [RONO: Robust Discriminative Learning With Noisy Labels for 2D-3D Cross-Modal Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_RONO_Robust_Discriminative_Learning_With_Noisy_Labels_for_2D-3D_Cross-Modal_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/penghu-cs/RONO)
- [ ]  文本-图像匹配
  - [ ]  [Learning Semantic Relationship among Instances for Image-Text Matching](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/CrossmodalGroup/HREM)
  - [ ]  [Fine-Grained Image-Text Matching by Cross-Modal Hard Aligning Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Fine-Grained_Image-Text_Matching_by_Cross-Modal_Hard_Aligning_Network_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ppanzx/CHAN)
- [ ]  图像文本检索
  - [ ]  [Multilateral Semantic Relations Modeling for Image Text Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multilateral_Semantic_Relations_Modeling_for_Image_Text_Retrieval_CVPR_2023_paper.pdf)
  - [ ]  [ViLEM: Visual-Language Error Modeling for Image-Text Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ViLEM_Visual-Language_Error_Modeling_for_Image-Text_Retrieval_CVPR_2023_paper.pdf)
- [ ]  文本-视频检索
  - [ ]  [Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?](https://ar5iv.org/abs/2301.00184)<br>:star:[code](https://github.com/whwu95/Cap4Video)
- [ ]  视频语言检索
  - [ ]  [CLIPPING: Distilling CLIP-Based Models With a Student Base for Video-Language Retrieval](https://openaccess.thecvf.com/content/CVPR2023/papers/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.pdf)
  - [ ]  [Towards Fast Adaptation of Pretrained Contrastive Models for Multi-Channel Video-Language Retrieval](http://ar5iv.org/abs/2206.02082)

<a name="22"/>

## 22.Image Synthesis/Generation(图像合成)
- [ ]  [LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation](https://ar5iv.org/abs/2303.17189)<br>:star:[code](https://github.com/ZGCTroy/LayoutDiffusion)
- [ ]  [Zero-shot Generative Model Adaptation via Image-specific Prompt Learning](http://ar5iv.org/abs/2304.03119v1)<br>:star:[code](https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation)
- [ ]  [TopNet: Transformer-based Object Placement Network for Image Compositing](https://ar5iv.org/abs/2304.03372)
- [ ]  基于草图生成
  - [ ]  [Picture that Sketch: Photorealistic Image Generation from Abstract Sketches](https://ar5iv.org/abs/2303.11162)<br>:house:[project](https://subhadeepkoley.github.io/PictureThatSketch)
- [ ]  图像-视频合成
  - [ ]  [Conditional Image-to-Video Generation with Latent Flow Diffusion Models](http://ar5iv.org/abs/2303.13744v1)<br>:star:[code](https://github.com/nihaomiao/CVPR23_LFDM)
- [ ]  海报生成
  - [ ]  [Unsupervised Domain Adaption with Pixel-level Discriminator for Image-aware Layout Generation](http://ar5iv.org/abs/2303.14377v1)
- [ ]  文本-图像合成
  - [ ]  [Variational Distribution Learning for Unsupervised Text-to-Image Generation](http://ar5iv.org/abs/2303.16105v1)
  - [ ]  [ReCo: Region-Controlled Text-to-Image Generation](https://ar5iv.org/abs/2211.15518)
  - [ ]  [Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models To Learn Any Unseen Style](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf)
  - [ ]  [Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation](http://ar5iv.org/abs/2304.01816v1)
  - [ ]  [DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.pdf)
  - [ ]  [Multi-Concept Customization of Text-to-Image Diffusion](https://ar5iv.org/abs/2212.04488)<br>:house:[project](https://www.cs.cmu.edu/~custom-diffusion/dataset.html)
  - [ ]  [Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models](https://ar5iv.org/abs/2212.14704)<br>:house:[project](https://bluestyle97.github.io/dream3d/)
  - [ ]  [Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models](https://ar5iv.org/abs/2212.08698)<br>:star:[code](https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement)
  - [ ]  [GLIGEN: Open-Set Grounded Text-to-Image Generation](https://ar5iv.org/abs/2301.07093)
  - [ ]  [RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation With Natural Prompts](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WUSTL-CSPL/RIATIG)
  - [ ]  [GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis](https://ar5iv.org/abs/2301.12959)<br>:star:[code](https://github.com/tobran/GALIP)
  - [ ]  [Shifted Diffusion for Text-to-image Generation](https://ar5iv.org/abs/2211.15388)<br>:star:[code](https://github.com/drboog/Shifted_Diffusion)
  - [ ]  [Conditional Text Image Generation With Diffusion Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Conditional_Text_Image_Generation_With_Diffusion_Models_CVPR_2023_paper.pdf)
  - [ ]  [Scaling Up GANs for Text-to-Image Synthesis](https://ar5iv.org/abs/2303.05511)<br>:house:[project](https://mingukkang.github.io/GigaGAN/)
- [ ]  prompting
  - [ ]  [Diversity-Aware Meta Visual Prompting](https://ar5iv.org/abs/2303.08138)<br>:star:[code](https://github.com/shikiw/DAM-VP)
- [ ]  图像生成
  - [ ]  [LayoutDM: Discrete Diffusion Model for Controllable Layout Generation](https://ar5iv.org/abs/2303.08137)<br>:house:[project](https://cyberagentailab.github.io/layout-dm/)
  - [ ]  [Private Image Generation With Dual-Purpose Auxiliary Classifier](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Private_Image_Generation_With_Dual-Purpose_Auxiliary_Classifier_CVPR_2023_paper.pdf)
  - [ ]  [Unsupervised Domain Adaption With Pixel-Level Discriminator for Image-Aware Layout Generation](http://ar5iv.org/abs/2303.14377)
  - [ ]  [SpaText: Spatio-Textual Representation for Controllable Image Generation](https://ar5iv.org/abs/2211.14305)<br>:house:[project](https://omriavrahami.com/spatext)
  - [ ]  [MaskSketch: Unpaired Structure-Guided Masked Image Generation](https://ar5iv.org/abs/2302.05496)
  - [ ]  [Where Is My Spot? Few-Shot Image Generation via Latent Subspace Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Where_Is_My_Spot_Few-Shot_Image_Generation_via_Latent_Subspace_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/chansey0529/LSO)
  - [ ]  [Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation](https://ar5iv.org/abs/2305.13607)<br>:star:[code](https://github.com/CrossmodalGroup/MaskedVectorQuantization)
  - [ ]  [Controllable Mesh Generation Through Sparse Latent Point Diffusion Models](https://ar5iv.org/abs/2303.07938)<br>:house:[project](https://slide-3d.github.io/) 
  - [ ]  [NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs](https://ar5iv.org/abs/2304.05866)<br>:house:[project](https://rangwani-harsh.github.io/NoisyTwins/)
  - [ ]  [Exploring Incompatible Knowledge Transfer in Few-shot Image Generation](http://ar5iv.org/abs/2304.07574v1)
  - [ ]  [Wavelet Diffusion Models Are Fast and Scalable Image Generators](https://ar5iv.org/abs/2211.16152)<br>:star:[code](https://github.com/VinAIResearch/WaveDiff.git)
  - [ ]  [Picture That Sketch: Photorealistic Image Generation From Abstract Sketches](https://ar5iv.org/abs/2303.11162)<br>:house:[project](https://subhadeepkoley.github.io/PictureThatSketch)
  - [ ]  [DiffCollage: Parallel Generation of Large Content with Diffusion Models](http://ar5iv.org/abs/2303.17076v1)<br>:house:[project](https://research.nvidia.com/labs/dir/diffcollage) 
  - [ ]  [Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization](http://ar5iv.org/abs/2305.11718v1)<br>:star:[code](https://github.com/CrossmodalGroup/DynamicVectorQuantization)
  - [ ]  [LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation](http://ar5iv.org/abs/2303.17189v1)<br>:star:[code](https://github.com/ZGCTroy/LayoutDiffusion)
  - [ ]  [Domain Expansion of Image Generators](https://ar5iv.org/abs/2301.05225)<br>:house:[project](https://yotamnitzan.github.io/domain-expansion/)
- [ ]  视频生成
  - [ ]  [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](http://ar5iv.org/abs/2304.08818v1)<br>:house:[project](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)  
  - [ ]  文本驱动的视频合成
    - [ ]  [Tell Me What Happened: Unifying Text-Guided Video Completion via Multimodal Masked Video Generation](https://ar5iv.org/abs/2211.12824)
- [ ]  Image Synthesis(图像合成)
  - [ ]  [Learning 3D-aware Image Synthesis with Unknown Pose Distribution](https://ar5iv.org/abs/2301.07702)<br>:house:[project](https://vivianszf.github.io/pof3d/)
  - [ ]  [Few-Shot Semantic Image Synthesis With Class Affinity Transfer](https://openaccess.thecvf.com/content/CVPR2023/papers/Careil_Few-Shot_Semantic_Image_Synthesis_With_Class_Affinity_Transfer_CVPR_2023_paper.pdf)
  - [ ]  [Fake It Till You Make It: Learning Transferable Representations From Synthetic ImageNet Clones](https://openaccess.thecvf.com/content/CVPR2023/papers/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.pdf)
  - [ ]  [3D-Aware Conditional Image Synthesis](http://ar5iv.org/abs/2302.08509)
  - [ ]  [SceneComposer: Any-Level Semantic Image Synthesis](https://ar5iv.org/abs/2211.11742)<br>:house:[project](https://zengxianyu.github.io/scenec/)
  - [ ]  [RWSC-Fusion: Region-Wise Style-Controlled Fusion Network for the Prohibited X-Ray Security Image Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Duan_RWSC-Fusion_Region-Wise_Style-Controlled_Fusion_Network_for_the_Prohibited_X-Ray_Security_CVPR_2023_paper.pdf)
  - [ ]  [Exploring Intra-Class Variation Factors With Learnable Cluster Prompts for Semi-Supervised Image Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Exploring_Intra-Class_Variation_Factors_With_Learnable_Cluster_Prompts_for_Semi-Supervised_CVPR_2023_paper.pdf)
  - [ ]  [Quantitative Manipulation of Custom Attributes on 3D-Aware Image Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Do_Quantitative_Manipulation_of_Custom_Attributes_on_3D-Aware_Image_Synthesis_CVPR_2023_paper.pdf)
  - [ ]  [Inferring and Leveraging Parts From Object Shape for Improving Semantic Image Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Inferring_and_Leveraging_Parts_From_Object_Shape_for_Improving_Semantic_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/csyxwei/iPOSE)
  - [ ]  [MAGE: MAsked Generative Encoder To Unify Representation Learning and Image Synthesis](https://ar5iv.org/abs/2211.09117)<br>:star:[code](https://github.com/LTH14/mage)
  - [ ]  [Person Image Synthesis via Denoising Diffusion Model](https://ar5iv.org/abs/2211.12500)
  - [ ]  [Freestyle Layout-to-Image Synthesis](http://ar5iv.org/abs/2303.14412v1)<br>:star:[code](https://github.com/essunny310/FreestyleNet)
  - [ ]  [Few-shot Semantic Image Synthesis with Class Affinity Transfer](http://ar5iv.org/abs/2304.02321v1)图像合成
  - [ ]  [Regularized Vector Quantization for Tokenized Image Synthesis](https://ar5iv.org/abs/2303.06424)
  - [ ]  [High-Fidelity Guided Image Synthesis with Latent Diffusion Models](https://ar5iv.org/abs/2211.17084)<br>:house:[project](https://1jsingh.github.io/gradop)
  - [ ]  [PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing](https://openaccess.thecvf.com/content/CVPR2023/papers/Sheng_PixHt-Lab_Pixel_Height_Based_Light_Effect_Generation_for_Image_Compositing_CVPR_2023_paper.pdf)<br>:house:[project](https://shengcn.github.io/PixHtLab/)
- [ ]  文本-运动生成
  - [ ]  [Being Comes From Not-Being: Open-Vocabulary Text-to-Motion Generation With Wordless Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Being_Comes_From_Not-Being_Open-Vocabulary_Text-to-Motion_Generation_With_Wordless_Training_CVPR_2023_paper.pdf)<br>:house:[project](https://github.com/junfanlin/)
- [ ]  纹理合成
  - [ ]  [Neural Texture Synthesis With Guided Correspondence](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Neural_Texture_Synthesis_With_Guided_Correspondence_CVPR_2023_paper.pdf)

<a name="21"/>

## 21.UAV/Remote Sensing/Satellite Image(无人机/遥感/卫星图像)
- [ ]  [TopDiG: Class-Agnostic Topological Directional Graph Extraction From Remote Sensing Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_TopDiG_Class-Agnostic_Topological_Directional_Graph_Extraction_From_Remote_Sensing_Images_CVPR_2023_paper.pdf)
- [ ]  [Change-Aware Sampling and Contrastive Learning for Satellite Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Mall_Change-Aware_Sampling_and_Contrastive_Learning_for_Satellite_Images_CVPR_2023_paper.pdf)
- [ ]  [MethaneMapper: Spectral Absorption aware Hyperspectral Transformer for Methane Detection](http://ar5iv.org/abs/2304.02767v1)
- [ ]  [ViTs for SITS: Vision Transformers for Satellite Image Time Series](https://ar5iv.org/abs/2301.04944)
- [ ]  [Adaptive Sparse Convolutional Networks With Global Context Enhancement for Faster Object Detection on Drone Images](https://ar5iv.org/abs/2303.14488)<br>:star:[code](https://github.com/Cuogeihong/CEASC)
- [ ]  图像检测
  - [ ]  [Adaptive Sparse Convolutional Networks with Global Context Enhancement for Faster Object Detection on Drone Images](http://ar5iv.org/abs/2303.14488v1)<br>:star:[code](https://github.com/Cuogeihong/CEASC)
- [ ]  跟踪
  - [ ]  [Resource-Efficient RGBD Aerial Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Resource-Efficient_RGBD_Aerial_Tracking_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/yjybuaa/RGBDAerialTracking)
- [ ]  雷达定位
  - [ ]  [SGLoc: Scene Geometry Encoding for Outdoor LiDAR Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SGLoc_Scene_Geometry_Encoding_for_Outdoor_LiDAR_Localization_CVPR_2023_paper.pdf)
- [ ]  无人机目标检测
  - [ ]  [Generalized UAV Object Detection via Frequency Domain Disentanglement](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Generalized_UAV_Object_Detection_via_Frequency_Domain_Disentanglement_CVPR_2023_paper.pdf)

<a name="20"/>

## 20.Autonomous vehicles(自动驾驶)
- [ ]  自动驾驶
  - [ ]  [UniSim: A Neural Closed-Loop Sensor Simulator](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.pdf)<br>:house:[project](https://waabi.ai/research/unisim/)
  - [ ]  [Planning-Oriented Autonomous Driving](http://ar5iv.org/abs/2212.10156)
  - [ ]  [Temporal Consistent 3D LiDAR Representation Learning for Semantic Perception in Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Nunes_Temporal_Consistent_3D_LiDAR_Representation_Learning_for_Semantic_Perception_in_CVPR_2023_paper.pdf)
  - [ ]  [TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_TBP-Former_Learning_Temporal_Birds-Eye-View_Pyramid_for_Joint_Perception_and_Prediction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/MediaBrain-SJTU/TBP-Former)
  - [ ]  [Weakly Supervised Class-Agnostic Motion Prediction for Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Weakly_Supervised_Class-Agnostic_Motion_Prediction_for_Autonomous_Driving_CVPR_2023_paper.pdf)
  - [ ]  [Learning and Aggregating Lane Graphs for Urban Automated Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Buchner_Learning_and_Aggregating_Lane_Graphs_for_Urban_Automated_Driving_CVPR_2023_paper.pdf)<br>:star:[code](http://urbanlanegraph.cs.uni-freiburg.de/)
  - [ ]  [RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving](https://ar5iv.org/abs/2301.10222)<br>:star:[code](https://github.com/valeoai/rangevit)
  - [ ]  [Azimuth Super-Resolution for FMCW Radar in Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Azimuth_Super-Resolution_for_FMCW_Radar_in_Autonomous_Driving_CVPR_2023_paper.pdf)
  - [ ]  [Unsupervised 3D Point Cloud Representation Learning by Triangle Constrained Contrast for Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_Unsupervised_3D_Point_Cloud_Representation_Learning_by_Triangle_Constrained_Contrast_CVPR_2023_paper.pdf)<br>:house:[project](https://bopang1996.github.io/)
  - [ ]  [Localized Semantic Feature Mixers for Efficient Pedestrian Detection in Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Khan_Localized_Semantic_Feature_Mixers_for_Efficient_Pedestrian_Detection_in_Autonomous_CVPR_2023_paper.pdf)
  - [ ]  [DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization](https://ar5iv.org/abs/2212.06331)
  - [ ]  [Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving](https://ar5iv.org/abs/2303.01788)
  - [ ]  [ReasonNet: End-to-End Driving with Temporal and Global Reasoning](http://ar5iv.org/abs/2305.10507v1)
  - [ ]  [LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation](http://ar5iv.org/abs/2304.11379v1)<br>:star:[code](https://github.com/songw-zju/LiDAR2Map)
  - [ ]  [Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction](https://ar5iv.org/abs/2302.07817)<br>:star:[code](https://github.com/wzzheng/TPVFormer)
  - [ ]  [Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.pdf)
- [ ]  [MSeg3D: Multi-Modal 3D Semantic Segmentation for Autonomous Driving](https://ar5iv.org/abs/2303.08600)<br>:star:[code](https://github.com/jialeli1/lidarseg3d)
  - [ ]  [Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving](http://ar5iv.org/abs/2305.06242v1)
  - [ ]  [GINA-3D: Learning to Generate Implicit Neural Assets in the Wild](http://ar5iv.org/abs/2304.02163v1)自动驾驶
  - [ ]  [Neural Map Prior for Autonomous Driving](http://ar5iv.org/abs/2304.08481v1)
- [ ]  轨迹预测
  - [ ]  [IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction](https://ar5iv.org/pdf/2303.00575.pdf)
  - [ ]  [ViP3D: End-to-End Visual Trajectory Prediction via 3D Agent Queries](https://ar5iv.org/abs/2208.01582)
  - [ ]  [Query-Centric Trajectory Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Query-Centric_Trajectory_Prediction_CVPR_2023_paper.pdf)
  - [ ]  [Leapfrog Diffusion Model for Stochastic Trajectory Prediction](https://ar5iv.org/abs/2303.10895)<br>:star:[code](https://github.com/MediaBrain-SJTU/LED)
  - [ ]  [Uncovering the Missing Pattern: Unified Framework Towards Trajectory Imputation and Prediction](http://ar5iv.org/abs/2303.16005v1)<br>:star:[code](https://github.com/colorfulfuture/GC-VRNN)
  - [ ]  [FEND: A Future Enhanced Distribution-Aware Contrastive Learning Framework for Long-tail Trajectory Prediction](http://ar5iv.org/abs/2303.16574v1)
  - [ ]  [Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction](http://ar5iv.org/abs/2304.04298v1)
  - [ ]  [Stimulus Verification Is a Universal and Effective Sampler in Multi-Modal Human Trajectory Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Stimulus_Verification_Is_a_Universal_and_Effective_Sampler_in_Multi-Modal_CVPR_2023_paper.pdf)
- [ ]  Place Recognition
  - [ ]  [Data-efficient Large Scale Place Recognition with Graded Similarity Supervision](https://ar5iv.org/abs/2303.11739)<br>:star:[code](https://github.com/marialeyvallina/generalized_contrastive_loss)
  - [ ]  [R2Former: Unified Retrieval and Reranking Transformer for Place Recognition](https://ar5iv.org/abs/2304.03410)<br>:star:[code](https://github.com/Jeff-Zilence/R2Former)
- [ ]  车道线检测
  - [ ]  [BEV-LaneDet: An Efficient 3D Lane Detection Based on Virtual Camera via Key-Points](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BEV-LaneDet_An_Efficient_3D_Lane_Detection_Based_on_Virtual_Camera_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/gigo-team/bev_lane_det)  
  - [ ]  [Anchor3DLane: Learning To Regress 3D Anchors for Monocular 3D Lane Detection](https://ar5iv.org/abs/2301.02371)<br>:star:[code](https://github.com/tusen-ai/Anchor3DLane)
- [ ]  鸟瞰识别
  - [ ]  [BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.pdf)
  - [ ]  [SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Gosala_SkyEye_Self-Supervised_Birds-Eye-View_Semantic_Mapping_Using_Monocular_Frontal_View_Images_CVPR_2023_paper.pdf)<br>:house:[project](http://skyeye.cs.uni-freiburg.de/)

<a name="19"/>

## 19.Neural Architecture Search(神经架构搜索)
- [ ]  [PA&DA: Jointly Sampling PAth and DAta for Consistent NAS](https://ar5iv.org/pdf/2302.14772.pdf)<br>:star:[code](https://github.com/ShunLu91/PA-DA)
- [ ]  [Differentiable Architecture Search With Random Features](https://ar5iv.org/abs/2208.08835)
- [ ]  [Adversarially Robust Neural Architecture Search for Graph Neural Networks](https://ar5iv.org/abs/2304.04168)
- [ ]  [MDL-NAS: A Joint Multi-Domain Learning Framework for Vision Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MDL-NAS_A_Joint_Multi-Domain_Learning_Framework_for_Vision_Transformer_CVPR_2023_paper.pdf)
- [ ]  [HOTNAS: Hierarchical Optimal Transport for Neural Architecture Search](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_HOTNAS_Hierarchical_Optimal_Transport_for_Neural_Architecture_Search_CVPR_2023_paper.pdf)
- [ ]  [EMT-NAS:Transferring Architectural Knowledge Between Tasks From Different Datasets](https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_EMT-NASTransferring_Architectural_Knowledge_Between_Tasks_From_Different_Datasets_CVPR_2023_paper.pdf)

<a name="18"/>

## 18.Person Re-Identification(人员重识别)
- [ ]  [Towards Modality-Agnostic Person Re-Identification With Descriptive Query](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Towards_Modality-Agnostic_Person_Re-Identification_With_Descriptive_Query_CVPR_2023_paper.pdf)
- [ ]  [Event-Guided Person Re-Identification via Sparse-Dense Complementary Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Event-Guided_Person_Re-Identification_via_Sparse-Dense_Complementary_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Chengzhi-Cao/SDCL)
- [ ]  [Patch-Wise High-Frequency Augmentation for Transformer-Based Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PHA_Patch-Wise_High-Frequency_Augmentation_for_Transformer-Based_Person_Re-Identification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhangguiwei610/PHA)
- [ ]  [TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification](https://ar5iv.org/abs/2303.06819)<br>:star:[code](https://github.com/Kali-Hac/TranSG)
- [ ]  人员检索
  - [ ]  [Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval](http://ar5iv.org/abs/2303.12501v1)<br>:star:[code](https://github.com/anosorae/IRRA)
  - [ ]  [Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Physically_Realizable_Natural-Looking_Clothing_Textures_Evade_Person_Detectors_via_3D_CVPR_2023_paper.pdf)
- [ ]  换衣重识别
  - [ ]  [Good Is Bad: Causality Inspired Cloth-Debiasing for Cloth-Changing Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Good_Is_Bad_Causality_Inspired_Cloth-Debiasing_for_Cloth-Changing_Person_Re-Identification_CVPR_2023_paper.pdf)  
  - [ ]  [Clothing-Change Feature Augmentation for Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2023/papers/Han_Clothing-Change_Feature_Augmentation_for_Person_Re-Identification_CVPR_2023_paper.pdf)
  - [ ]  [An In-Depth Exploration of Person Re-Identification and Gait Recognition in Cloth-Changing Conditions](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_An_In-Depth_Exploration_of_Person_Re-Identification_and_Gait_Recognition_in_CVPR_2023_paper.pdf)
- [ ]  可见光-红外人员重识别(VIReID)
  - [ ]  [Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification](http://ar5iv.org/abs/2303.14481v1)<br>:star:[code](https://github.com/ZYK100/LLCM)
  - [ ]  [Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification](http://ar5iv.org/abs/2304.04205v1)
  - [ ]  [PartMix: Regularization Strategy to Learn Part Discovery for Visible-Infrared Person Re-identification](http://ar5iv.org/abs/2304.01537v1)可见光-红外人员重识别(VI-ReID)
  - [ ]  [Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Unsupervised_Visible-Infrared_Person_Re-Identification_via_Progressive_Graph_Matching_and_Alternate_CVPR_2023_paper.pdf)
- [ ]  G-ReID
  - [ ]  [Similarity Metric Learning for RGB-Infrared Group Re-Identification](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Similarity_Metric_Learning_for_RGB-Infrared_Group_Re-Identification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WhollyOat/CM-Group)
- [ ]  行人检测
  - [ ]  [VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision](http://ar5iv.org/abs/2304.03135v1)<br>:star:[code](https://github.com/lmy98129/VLPD)
  - [ ]  [Optimal Proposal Learning for Deployable End-to-End Pedestrian Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Optimal_Proposal_Learning_for_Deployable_End-to-End_Pedestrian_Detection_CVPR_2023_paper.pdf)
- [ ]  人群计数
  - [ ]  [CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model](http://ar5iv.org/abs/2304.04231v1)<br>:star:[code](https://github.com/dk-liang/CrowdCLIP)
  - [ ]  [Boosting Detection in Crowd Analysis via Underutilized Output Features](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Boosting_Detection_in_Crowd_Analysis_via_Underutilized_Output_Features_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/wskingdom/Crowd-Hat)
  - [ ]  [Optimal Transport Minimization: Crowd Localization on Density Maps for Semi-Supervised Counting](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Optimal_Transport_Minimization_Crowd_Localization_on_Density_Maps_for_Semi-Supervised_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Elin24/OT-M)
- [ ]  步态识别
  - [ ]  [Dynamic Aggregated Network for Gait Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Dynamic_Aggregated_Network_for_Gait_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/XKMar/FastGait)
  - [ ]  [LidarGait: Benchmarking 3D Gait Recognition With Point Clouds](https://ar5iv.org/abs/2211.10598)<br>:house:[project](https://lidargait.github.io/)
  - [ ]  [GaitGCI: Generative Counterfactual Intervention for Gait Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Dou_GaitGCI_Generative_Counterfactual_Intervention_for_Gait_Recognition_CVPR_2023_paper.pdf)
  - [ ]  [OpenGait: Revisiting Gait Recognition Towards Better Practicality](http://ar5iv.org/abs/2211.06597)
  - [ ]  [Multi-Modal Gait Recognition via Effective Spatial-Temporal Feature Fusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Multi-Modal_Gait_Recognition_via_Effective_Spatial-Temporal_Feature_Fusion_CVPR_2023_paper.pdf)

<a name="17"/>

## 17.Medical Image(医学影像)
- [ ]  [Geometric Visual Similarity Learning in 3D Medical Image Self-Supervised Pre-Training](http://ar5iv.org/abs/2303.00874)
- [ ]  [Interventional Bag Multi-Instance Learning on Whole-Slide Pathological Images](https://ar5iv.org/abs/2303.06873)<br>:star:[code](https://github.com/HHHedo/IBMIL)
- [ ]  [Causally-Aware Intraoperative Imputation for Overall Survival Time Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Causally-Aware_Intraoperative_Imputation_for_Overall_Survival_Time_Prediction_CVPR_2023_paper.pdf)
- [ ]  [Flexible-Cm GAN: Towards Precise 3D Dose Prediction in Radiotherapy](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Flexible-Cm_GAN_Towards_Precise_3D_Dose_Prediction_in_Radiotherapy_CVPR_2023_paper.pdf)
- [ ]  [Towards Trustable Skin Cancer Diagnosis via Rewriting Model’s Decision](https://ar5iv.org/pdf/2303.00885.pdf)
- [ ]  [Hierarchical discriminative learning improves visual representations of biomedical microscopy](https://ar5iv.org/abs/2303.01605)<br>:house:[project](https://hidisc.mlins.org/)
- [ ]  [Topology-Guided Multi-Class Cell Context Generation for Digital Pathology](http://ar5iv.org/abs/2304.02255v1)
- [ ]  [Image Quality-aware Diagnosis via Meta-knowledge Co-embedding](http://ar5iv.org/abs/2303.15038v1)
- [ ]  [METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens](http://ar5iv.org/abs/2304.02211v1)医学诊断
- [ ]  3D医学
  - [ ]  [Geometric Visual Similarity Learning in 3D Medical Image Self-supervised Pre-training](https://ar5iv.org/pdf/2303.00874.pdf)<br>:star:[code](https://github.com/YutingHe-list/GVSL)
- [ ]  图像配准
  - [ ]  [Indescribable Multi-modal Spatial Evaluator](https://ar5iv.org/pdf/2303.00369.pdf)<br>:star:[code](https://github.com/Kid-Liet/IMSE)
- [ ]  图像分类
  - [ ]  [ask-specific Fine-tuning via Variational Information Bottleneck for Weakly-supervised Pathology Whole Slide Image Classification](https://ar5iv.org/abs/2303.08446)<br>:star:[code](https://github.com/invoker-LL/WSI-finetuning)
  - [ ]  [RankMix: Data Augmentation for Weakly Supervised Learning of Classifying Whole Slide Images With Diverse Sizes and Imbalanced Categories](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_RankMix_Data_Augmentation_for_Weakly_Supervised_Learning_of_Classifying_Whole_CVPR_2023_paper.pdf)
  - [ ]  [Grounding Counterfactual Explanation of Image Classifiers to Textual Concept Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Grounding_Counterfactual_Explanation_of_Image_Classifiers_to_Textual_Concept_Space_CVPR_2023_paper.pdf)
  - [ ]  [PEFAT: Boosting Semi-Supervised Medical Image Classification via Pseudo-Loss Estimation and Feature Adversarial Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_PEFAT_Boosting_Semi-Supervised_Medical_Image_Classification_via_Pseudo-Loss_Estimation_and_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/maxwell0027/PEFAT)
  - [ ]  [Task-Specific Fine-Tuning via Variational Information Bottleneck for Weakly-Supervised Pathology Whole Slide Image Classification](https://ar5iv.org/abs/2303.08446)<br>:star:[code](https://github.com/invoker-LL/WSI-finetuning)
  - [ ]  [A Loopback Network for Explainable Microvascular Invasion Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_A_Loopback_Network_for_Explainable_Microvascular_Invasion_Classification_CVPR_2023_paper.pdf)
- [ ]  报告生成
  - [ ]  [Dynamic Graph Enhanced Contrastive Learning for Chest X-ray Report Generation](https://ar5iv.org/abs/2303.10323)<br>:star:[code](https://github.com/mlii0117/DCL)
  - [ ]  [Interactive and Explainable Region-Guided Radiology Report Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Tanida_Interactive_and_Explainable_Region-Guided_Radiology_Report_Generation_CVPR_2023_paper.pdf)
  - [ ]  [METransformer: Radiology Report Generation by Transformer With Multiple Learnable Expert Tokens](https://ar5iv.org/abs/2304.02211)
    - [ ]  [KiUT: Knowledge-Injected U-Transformer for Radiology Report Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.pdf)
- [ ]  医学影像分割
  - [ ]  [Orthogonal Annotation Benefits Barely-supervised Medical Image Segmentation](http://ar5iv.org/abs/2303.13090v1)
  - [ ]  [SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation](http://ar5iv.org/abs/2305.11012v1)
  - [ ]  [Pseudo-Label Guided Contrastive Learning for Semi-Supervised Medical Image Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Basak_Pseudo-Label_Guided_Contrastive_Learning_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/hritam-98/PatchCL-MedSeg)
  - [ ]  [Fair Federated Medical Image Segmentation via Client Contribution Estimation](http://ar5iv.org/abs/2303.16520v1)
  - [ ]  [Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation](https://ar5iv.org/abs/2305.00673)<br>:star:[code](https://github.com/DeepMed-Lab-ECNU/BCP)
  - [ ]  [Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization](http://ar5iv.org/abs/2304.00212v1)
  - [ ]  [Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model](http://ar5iv.org/abs/2304.03572v1)
  - [ ]  [MCF: Mutual Correction Framework for Semi-Supervised Medical Image Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MCF_Mutual_Correction_Framework_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WYC-321/MCF)
  - [ ]  [Rethinking Few-Shot Medical Segmentation: A Vector Quantization View](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Few-Shot_Medical_Segmentation_A_Vector_Quantization_View_CVPR_2023_paper.pdf)
  - [ ]  [Devil Is in the Queries: Advancing Mask Transformers for Real-World Medical Image Segmentation and Out-of-Distribution Localization](https://ar5iv.org/abs/2304.00212)
  - [ ]  [MagicNet: Semi-Supervised Multi-Organ Segmentation via Magic-Cube Partition and Recovery](https://ar5iv.org/abs/2212.14310)<br>:star:[code](https://github.com/DeepMed-Lab-ECNU/MagicNet)
  - [ ]  [Ambiguous Medical Image Segmentation Using Diffusion Models](https://ar5iv.org/abs/2304.04745)
  - [ ]  [Directional Connectivity-Based Segmentation of Medical Images](http://ar5iv.org/abs/2304.00145)
- [ ]  医学影像分析
  - [ ]  [Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data](http://ar5iv.org/abs/2303.14080v1)
  - [ ]  [Directional Connectivity-based Segmentation of Medical Images](http://ar5iv.org/abs/2304.00145v1)<br>:star:[code](https://github.com/Zyun-Y/DconnNet)
- [ ]  肿瘤分割
  - [ ]  [Label-Free Liver Tumor Segmentation](http://ar5iv.org/abs/2303.14869v1)
- [ ]  医学影像报告生成
  - [ ]  [Interactive and Explainable Region-guided Radiology Report Generation](http://ar5iv.org/abs/2304.08295v1)<br>:star:[code](https://github.com/ttanida/rgrg)自动生成放射学报告 
- [ ]  切片分析
  - [ ]  [Histopathology Whole Slide Image Analysis With Heterogeneous Graph Representation Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/HKU-MedAI/WSI-HGNN)
- [ ]  细胞检测、跟踪与计数
  - [ ]  [DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell Detection and Counting](https://ar5iv.org/abs/2304.00741)
  - [ ]  [Overlapped Cell on Tissue Dataset for Histopathology](https://ar5iv.org/abs/2303.13110)
  - [ ]  [Unsupervised Contour Tracking of Live Cells by Mechanical and Cycle Consistency Losses](https://ar5iv.org/abs/2303.08364)<br>:star:[code](https://github.com/JunbongJang/contour-tracking/)
  - [ ]  [Masked Autoencoder Guided Segmentation at Pixel Resolution for Accurate, Self-Supervised Subcellular Structure Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_MAESTER_Masked_Autoencoder_Guided_Segmentation_at_Pixel_Resolution_for_Accurate_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/bowang-lab/MAESTER)
- [ ]  单目内窥镜跟踪
  - [ ]  [Constrained Evolutionary Diffusion Filter for Monocular Endoscope Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Constrained_Evolutionary_Diffusion_Filter_for_Monocular_Endoscope_Tracking_CVPR_2023_paper.pdf)
- [ ]  皮肤癌诊断
  - [ ]  [Towards Trustable Skin Cancer Diagnosis via Rewriting Model's Decision](https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Towards_Trustable_Skin_Cancer_Diagnosis_via_Rewriting_Models_Decision_CVPR_2023_paper.pdf)
- [ ]  MRI 重建
  - [ ]  [Learning Federated Visual Prompt in Null Space for MRI Reconstruction](https://ar5iv.org/abs/2303.16181)
- [ ]  生物医学
  - [ ]  [Hierarchical Discriminative Learning Improves Visual Representations of Biomedical Microscopy](http://ar5iv.org/abs/2303.01605)

<a name="16"/>

## 16.Semi/self-supervised learning(半/自监督)
- [ ]  无监督学习
  - [ ]  [Non-Contrastive Unsupervised Learning of Physiological Signals from Video](https://ar5iv.org/abs/2303.07944)<br>:star:[code](https://github.com/CVRL/SiNC-rPPG)
  - [ ]  [Neural Rate Estimator and Unsupervised Learning for Efficient Distributed Image Analytics in Split-DNN Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Ahuja_Neural_Rate_Estimator_and_Unsupervised_Learning_for_Efficient_Distributed_Image_CVPR_2023_paper.pdf)
  - [ ]  [Non-Contrastive Unsupervised Learning of Physiological Signals From Video](https://ar5iv.org/abs/2303.07944)
- [ ]  自监督
  - [ ]  [Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning](https://ar5iv.org/abs/2303.11101)
  - [ ]  [StepFormer: Self-Supervised Step Discovery and Localization in Instructional Videos](https://ar5iv.org/abs/2304.13265)
  - [ ]  [DLBD: A Self-Supervised Direct-Learned Binary Descriptor](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_DLBD_A_Self-Supervised_Direct-Learned_Binary_Descriptor_CVPR_2023_paper.pdf)
  - [ ]  [Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields](http://ar5iv.org/abs/2212.02493)
  - [ ]  [Self-Supervised Learning From Images With a Joint-Embedding Predictive Architecture](https://ar5iv.org/abs/2301.08243)
  - [ ]  [Defending Against Patch-Based Backdoor Attacks on Self-Supervised Learning](https://ar5iv.org/abs/2304.01482)<br>:star:[code](https://github.com/UCDvision/PatchSearch)
  - [ ]  [DrapeNet: Garment Generation and Self-Supervised Draping](https://ar5iv.org/abs/2211.11277)<br>:star:[code](https://github.com/liren2515/DrapeNet)
  - [ ]  [Neural Congealing: Aligning Images to a Joint Semantic Atlas](https://ar5iv.org/abs/2302.03956)<br>:house:[project](https://neural-congealing.github.io/)
  - [ ]  [Self-Supervised AutoFlow](https://ar5iv.org/abs/2212.01762)
  - [ ]  [Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Towards_Realistic_Long-Tailed_Semi-Supervised_Learning_Consistency_Is_All_You_Need_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Gank0078/ACR)
  - [ ]  [Siamese Image Modeling for Self-Supervised Vision Representation Learning](https://ar5iv.org/abs/2206.01204)<br>:star:[code](https://github.com/fundamentalvision/Siamese-Image-Modeling)
  - [ ]  [SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow](https://ar5iv.org/abs/2211.14020)<br>:star:[code](https://github.com/itailang/SCOOP)
  - [ ]  [Three Guidelines You Should Know for Universally Slimmable Self-Supervised Learning](https://ar5iv.org/abs/2303.06870)<br>:star:[code](https://github.com/megvii-research/US3L-CVPR2023)
  - [ ]  [Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence](https://ar5iv.org/abs/2206.06424)
  - [ ]  [Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss](https://ar5iv.org/abs/2301.05709)
  - [ ]  [Evolved Part Masking for Self-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Evolved_Part_Masking_for_Self-Supervised_Learning_CVPR_2023_paper.pdf)
  - [ ]  [Towards Professional Level Crowd Annotation of Expert Domain Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Towards_Professional_Level_Crowd_Annotation_of_Expert_Domain_Data_CVPR_2023_paper.pdf)
  - [ ]  [ALSO: Automotive Lidar Self-Supervision by Occupancy Estimation](https://ar5iv.org/abs/2212.05867)<br>:star:[code](https://github.com/valeoai/ALSO)
  - [ ]  [Correlational Image Modeling for Self-Supervised Visual Pre-Training](http://ar5iv.org/abs/2303.12670v1)
  - [ ]  [Beyond Appearance: a Semantic Controllable Self-Supervised Learning Framework for Human-Centric Visual Tasks](http://ar5iv.org/abs/2303.17602v1)<br>:star:[code](https://github.com/tinyvision/SOLIDER)<br>:thumbsup:[CVPR 2023 深挖无标签数据价值！自监督学习框架SOLIDER：用于以人为中心的视觉](https://mp.weixin.qq.com/s/XYewbXlp38MWSDJigrInmw)
  - [ ]  [Mixed Autoencoder for Self-supervised Visual Representation Learning](http://ar5iv.org/abs/2303.17152v1)
  - [ ]  [Siamese DETR](http://ar5iv.org/abs/2303.18144v1)<br>:star:[code](https://github.com/Zx55/SiameseDETR)
  - [ ]  [Token Boosting for Robust Self-Supervised Visual Transformer Pre-training](https://eprints.lancs.ac.uk/id/eprint/189827/1/CVPR2023_Lingeng_Tianjiao_TBM.pdf)
  - [ ]  [Self-Supervised Representation Learning for CAD](https://openaccess.thecvf.com/content/CVPR2023/papers/Jones_Self-Supervised_Representation_Learning_for_CAD_CVPR_2023_paper.pdf)
- [ ]  半监督
  - [ ]  [Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data](https://ar5iv.org/abs/2303.11066)<br>:star:[code](https://github.com/megvii-research/FullMatch)
  - [ ]  [HyperMatch: Noise-Tolerant Semi-Supervised Learning via Relaxed Contrastive Constraint](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_HyperMatch_Noise-Tolerant_Semi-Supervised_Learning_via_Relaxed_Contrastive_Constraint_CVPR_2023_paper.pdf)
  - [ ]  [Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Out-of-Distributed_Semantic_Pruning_for_Robust_Semi-Supervised_Learning_CVPR_2023_paper.pdf)
  - [ ]  [DualRel: Semi-Supervised Mitochondria Segmentation From a Prototype Perspective](https://openaccess.thecvf.com/content/CVPR2023/papers/Mai_DualRel_Semi-Supervised_Mitochondria_Segmentation_From_a_Prototype_Perspective_CVPR_2023_paper.pdf)
  - [ ]  [CHMATCH:Contrastive Hierarchical Matching and Robust Adaptive Threshold Boosted Semi-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_CHMATCH_Contrastive_Hierarchical_Matching_and_Robust_Adaptive_Threshold_Boosted_Semi-Supervised_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sailist/CHMatch)
  - [ ]  [ProtoCon: Pseudo-label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-supervised Learning](http://ar5iv.org/abs/2303.13556v1)
  - [ ]  [Class Balanced Adaptive Pseudo Labeling for Federated Semi-Supervised Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Class_Balanced_Adaptive_Pseudo_Labeling_for_Federated_Semi-Supervised_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/minglllli/)
  - [ ]  [MarginMatch:Improving Semi-Supervised Learning with Pseudo-Margins](https://openaccess.thecvf.com/content/CVPR2023/papers/Sosea_MarginMatch_Improving_Semi-Supervised_Learning_with_Pseudo-Margins_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tsosea%202/MarginMatch)
  - [ ]  [Semi-Supervised Learning Made Simple With Self-Supervised Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Fini_Semi-Supervised_Learning_Made_Simple_With_Self-Supervised_Clustering_CVPR_2023_paper.pdf)
- [ ]  弱监督
  - [ ]  [Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Shaharabany_Similarity_Maps_for_Self-Training_Weakly-Supervised_Phrase_Grounding_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/talshaharabany/Similarity-Maps-for-Self-Training-Weakly-Supervised-Phrase-Grounding)

<a name="15"/>

## 15.Vision Transformers
- [ ]  [Transformer-Based Learned Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Gartner_Transformer-Based_Learned_Optimization_CVPR_2023_paper.pdf)
- [ ]  [Teaching Matters: Investigating the Role of Supervision in Vision Transformers](http://ar5iv.org/abs/2212.03862)
- [ ]  [Beyond Attentive Tokens: Incorporating Token Importance and Diversity for Efficient Vision Transformers](http://ar5iv.org/abs/2211.11315)
- [ ]  [PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Grainger_PaCa-ViT_Learning_Patch-to-Cluster_Attention_in_Vision_Transformers_CVPR_2023_paper.pdf)
- [ ]  [NLOST: Non-Line-of-Sight Imaging with Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_NLOST_Non-Line-of-Sight_Imaging_With_Transformer_CVPR_2023_paper.pdf)
- [ ]  [SVGformer: Representation Learning for Continuous Vector Graphics Using Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_SVGformer_Representation_Learning_for_Continuous_Vector_Graphics_Using_Transformers_CVPR_2023_paper.pdf)
- [ ]  [Adversarial Normalization: I Can visualize Everything (ICE)](https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Adversarial_Normalization_I_Can_Visualize_Everything_ICE_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Hanyang-HCC-Lab/ICE)
- [ ]  [Hint-Aug: Drawing Hints From Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/GATECH-EIC/Hint-Aug)
- [ ]  [PanoSwin: A Pano-Style Swin Transformer for Panorama Understanding](https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_PanoSwin_A_Pano-Style_Swin_Transformer_for_Panorama_Understanding_CVPR_2023_paper.pdf)
- [ ]  [D2Former: Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-Based Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/He_D2Former_Jointly_Learning_Hierarchical_Detectors_and_Contextual_Descriptors_via_Agent-Based_CVPR_2023_paper.pdf)
- [ ]  [NAR-Former: Neural Architecture Representation Learning Towards Holistic Attributes Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_NAR-Former_Neural_Architecture_Representation_Learning_Towards_Holistic_Attributes_Prediction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/yuny220/NAR-Former)
- [ ]  [DropKey for Vision Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DropKey_for_Vision_Transformer_CVPR_2023_paper.pdf)
- [ ]  [Integrally Pre-Trained Transformer Pyramid Networks](https://ar5iv.org/abs/2211.12735)<br>:star:[code](https://github.com/sunsmarterjie/iTPN)
- [ ]  [DSVT: Dynamic Sparse Voxel Transformer With Rotated Sets](http://ar5iv.org/abs/2301.06051)
- [ ]  [Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Slide-Transformer_Hierarchical_Vision_Transformer_With_Local_Self-Attention_CVPR_2023_paper.pdf)
- [ ]  [Trade-Off Between Robustness and Accuracy of Vision Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Trade-Off_Between_Robustness_and_Accuracy_of_Vision_Transformers_CVPR_2023_paper.pdf)
- [ ]  [A Light Touch Approach to Teaching Transformers Multi-view Geometry](https://ar5iv.org/abs/2211.15107)
- [ ]  [Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers](https://ar5iv.org/abs/2205.12551)<br>:star:[code](https://github.com/yhlleo/MJP)
- [ ]  [RGB no more: Minimally-decoded JPEG Vision Transformers](https://ar5iv.org/abs/2211.16421)
- [ ]  [Making Vision Transformers Efficient from A Token Sparsification View](https://ar5iv.org/abs/2303.08685)<br>:star:[code](http://github.com/changsn/STViT-R)
- [ ]  [Blur Interpolation Transformer for Real-World Motion from Blur](https://ar5iv.org/abs/2211.11423)<br>:star:[code](https://github.com/zzh-tech/BiT)
- [ ]  [Neighborhood Attention Transformer](https://ar5iv.org/abs/2204.07143)<br>:star:[code](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)
- [ ]  [MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers](https://ar5iv.org/abs/2205.13137)<br>:star:[code](https://github.com/Sense-X/MixMIM)
- [ ]  [Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers](https://ar5iv.org/abs/2303.11251)<br>:house:[project](https://sites.google.com/view/mebt-cvpr2023)
- [ ]  [Improving Robustness of Vision Transformers by Reducing Sensitivity To Patch Corruptions](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Improving_Robustness_of_Vision_Transformers_by_Reducing_Sensitivity_To_Patch_CVPR_2023_paper.pdf)
- [ ]  [Latency Matters: Real-Time Action Forecasting Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Girase_Latency_Matters_Real-Time_Action_Forecasting_Transformer_CVPR_2023_paper.pdf)<br>:star:[code](https://karttikeya.github.io/publication/RAFTformer/)
- [ ]  [OmniMAE: Single Model Masked Pretraining on Images and Videos](https://ar5iv.org/abs/2206.08356)<br>:star:[code](https://github.com/facebookresearch/omnivore)
- [ ]  [MAGVIT: Masked Generative Video Transformer](https://ar5iv.org/abs/2212.05199)<br>:house:[project](https://magvit.cs.cmu.edu/)
- [ ]  [Learning Imbalanced Data with Vision Transformers](https://ar5iv.org/abs/2212.02015)<br>:star:[code](https://github.com/XuZhengzhuo/LiVT)
- [ ]  [Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves](https://ar5iv.org/pdf/2303.01112.pdf)<br>:house:[project](https://masora1030.github.io/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves/)
- [ ]  [AMIGO: Sparse Multi-Modal Graph Transformer with Shared-Context Processing for Representation Learning of Giga-pixel Images](https://ar5iv.org/pdf/2303.00865.pdf)<br>:star:[code](https://github.com/raminnakhli/AMIGO)
- [ ]  [Generic-to-Specific Distillation of Masked Autoencoders](https://ar5iv.org/pdf/2302.14771.pdf)<br>:star:[code](https://github.com/pengzhiliang/G2SD)
- [ ]  [BiFormer: Vision Transformer with Bi-Level Routing Attention](https://ar5iv.org/abs/2303.08810)<br>:star:[code](https://github.com/rayleizhu/BiFormer)
- [ ]  [Making Vision Transformers Efficient from A Token Sparsification View](https://ar5iv.org/abs/2303.08685)
- [ ]  [Dual-path Adaptation from Image to Video Transformers](https://ar5iv.org/abs/2303.09857)<br>:star:[code](https://github.com/park-jungin/DualPath)
- [ ]  [Spherical Transformer for LiDAR-based 3D Recognition](http://ar5iv.org/abs/2303.12766v1)<br>:star:[code](https://github.com/dvlab-research/SphereFormer.git)
- [ ]  [MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models](http://ar5iv.org/abs/2303.13009v1)<br>:star:[code](https://github.com/mlvlab/MELTR)
- [ ]  [Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers](http://ar5iv.org/abs/2303.13755v1)
- [ ]  [Learning Expressive Prompting With Residuals for Vision Transformers](http://ar5iv.org/abs/2303.15591v1)
- [ ]  [SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer](http://ar5iv.org/abs/2303.17605v1)<br>:house:[project](https://sparsevit.mit.edu)
- [ ]  [Visual Dependency Transformers: Dependency Tree Emerges from Reversed Attention](http://ar5iv.org/abs/2304.03282v1)Transformer
- [ ]  [Token Boosting for Robust Self-Supervised Visual Transformer Pre-training](http://ar5iv.org/abs/2304.04175v1)Transformer
- [ ]  [Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention](http://ar5iv.org/abs/2304.04237v1)<br>:star:[code](https://github.com/LeapLabTHU/Slide-Transformer)
- [ ]  [RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer](http://ar5iv.org/abs/2304.05659v1)<br>:star:[code](https://techmonsterwang.github.io/RIFormer/)
- [ ]  [DropKey](https://ar5iv.org/abs/2208.02646)<br>:thumbsup:[CVPR 2023｜两行代码高效缓解视觉Transformer过拟合，美图&国科大联合提出正则化方法DropKey](https://mp.weixin.qq.com/s/rmr40kryYPILwEinzZOjUA)
- [ ]  [Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers](http://ar5iv.org/abs/2304.10716v1)<br>:star:[code](https://github.com/megvii-research/TPS-CVPR2023)
- [ ]  [EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention](http://ar5iv.org/abs/2305.07027v1)<br>:star:[code](https://github.com/microsoft/Cream/tree/main/EfficientViT)
- [ ]  [TrojViT: Trojan Insertion in Vision Transformers](https://ar5iv.org/abs/2208.13049)
- [ ]  [Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference](https://openaccess.thecvf.com/content/CVPR2023/papers/You_Castling-ViT_Compressing_Self-Attention_via_Switching_Towards_Linear-Angular_Attention_at_Vision_CVPR_2023_paper.pdf)<br>:house:[project](https://www.haoranyou.com/castling-vit)
- [ ]  [ResFormer: Scaling ViTs with Multi-Resolution Training](https://ar5iv.org/abs/2212.00776)<br>:star:[code](https://github.com/ruitian12/resformer)
- [ ]  [Vision Transformer With Super Token Sampling](https://ar5iv.org/abs/2211.11167)<br>:star:[code](https://github.com/hhb072/SViT)
- [ ]  [Vision Transformers Are Good Mask Auto-Labelers](https://ar5iv.org/abs/2301.03992)

<a name="14"/>

## 14.Video
- [ ]  [PointAvatar: Deformable Point-Based Head Avatars From Videos](http://ar5iv.org/abs/2212.08377)
- [ ]  [Video Probabilistic Diffusion Models in Projected Latent Space](http://ar5iv.org/abs/2302.07685)
- [ ]  [Masked Motion Encoding for Self-Supervised Video Representation Learning](https://ar5iv.org/abs/2210.06096)<br>:star:[code](https://github.com/XinyuSun/MME)
- [ ]  [Modular Memorability: Tiered Representations for Video Memorability Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Dumont_Modular_Memorability_Tiered_Representations_for_Video_Memorability_Prediction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tekal-ai/modular-memorability)
- [ ]  [Language-Guided Music Recommendation for Video via Prompt Analogies](https://openaccess.thecvf.com/content/CVPR2023/papers/McKee_Language-Guided_Music_Recommendation_for_Video_via_Prompt_Analogies_CVPR_2023_paper.pdf)
- [ ]  [Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning](https://ar5iv.org/abs/2212.03229)
- [ ]  [1000 FPS HDR Video With a Spike-RGB Hybrid Camera](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_1000_FPS_HDR_Video_With_a_Spike-RGB_Hybrid_Camera_CVPR_2023_paper.pdf)<br>:house:[project](https://changyakun.github.io/1000FPS-HDR)
- [ ]  [Egocentric Video Task Translatio](https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Egocentric_Video_Task_Translation_CVPR_2023_paper.pdf)<br>:house:[project](https://vision.cs.utexas.edu/projects/egot2/)
- [ ]  [Relational Space-Time Query in Long-Form Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Relational_Space-Time_Query_in_Long-Form_Videos_CVPR_2023_paper.pdf)
- [ ]  [Spatial-Then-Temporal Self-Supervised Learning for Video Correspondence](https://ar5iv.org/abs/2209.07778)<br>:star:[code](https://github.com/qianduoduolr/Spa-then-Temp)
- [ ]  [Few-Shot Referring Relationships in Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_Few-Shot_Referring_Relationships_in_Videos_CVPR_2023_paper.pdf)<br>:house:[project](https://vl2g.github.io/projects/refRelations/)
- [ ]  [Aligning Step-by-Step Instructional Diagrams to Video Demonstrations](https://ar5iv.org/abs/2303.13800)<br>:house:[project](https://academic.davidz.cn/en/publication/zhang-cvpr-2023/)
- [ ]  [3D Video Loops From Asynchronous Input](https://ar5iv.org/abs/2303.05312)<br>:house:[project](https://limacv.github.io/VideoLoop3D_web/)
- [ ]  [VideoMAE V2: Scaling Video Masked Autoencoders With Dual Masking](https://ar5iv.org/abs/2303.16727)<br>:star:[code](https://github.com/OpenGVLab/VideoMAEv2)
- [ ]  [Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos](http://ar5iv.org/abs/2303.12370v1)<br>:star:[code](https://github.com/svip-lab/WeakSVR)
- [ ]  [StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos](http://ar5iv.org/abs/2304.13265v1)
- [ ]  [VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking](http://ar5iv.org/abs/2303.16727v1)
- [ ]  [Implicit View-Time Interpolation of Stereo Videos using Multi-Plane Disparities and Non-Uniform Coordinates](http://ar5iv.org/abs/2303.17181v1)<br>:house:[project](https://people.engr.tamu.edu/nimak/Papers/CVPR23StereoVideo/index.html)<br>:tv:[video](https://www.youtube.com/watch?v=XJa_bf8OCrc)
- [ ]  [How You Feelin'? Learning Emotions and Mental States in Movie Scenes](https://ar5iv.org/abs/2304.05634)<br>:house:[project](https://katha-ai.github.io/projects/emotx/)
- [ ]  视频时刻检索
  - [ ]  [Towards Generalisable Video Moment Retrieval:Visual-Dynamic Injection to Image-Text Pre-Training](https://ar5iv.org/pdf/2303.00040.pdf)
  - [ ]  [Query-Dependent Video Representation for Moment Retrieval and Highlight Detection](http://ar5iv.org/abs/2303.13874v1)<br>:star:[code](https://github.com/wjun0830/QD-DETR)
  - [ ]  [Hierarchical Video-Moment Retrieval and Step-Captioning](http://ar5iv.org/abs/2303.16406v1)<br>:star:[code](https://hirest-cvpr2023.github.io)
- [ ]  视频高亮检测
  - [ ]  [Collaborative Noisy Label Cleaner: Learning Scene-aware Trailers for Multi-modal Highlight Detection in Movies](http://ar5iv.org/abs/2303.14768v1)<br>:star:[code](https://github.com/TencentYoutuResearch/HighlightDetection-CLC)
- [ ]  视频帧插值
  - [ ]  [Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation](https://ar5iv.org/pdf/2303.00440.pdf)<br>:star:[code](https://github.com/MCG-NJU/EMA-VFI)
  - [ ]  [AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation](http://ar5iv.org/abs/2304.09790v1)<br>:star:[code](https://github.com/MCG-NKU/AMT)
  - [ ]  [Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation](https://ar5iv.org/abs/2203.10291)
  - [ ]  [Exploring Discontinuity for Video Frame Interpolation](https://ar5iv.org/abs/2202.07291)
  - [ ]  [Event-Based Video Frame Interpolation With Cross-Modal Asymmetric Bidirectional Motion Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Event-Based_Video_Frame_Interpolation_With_Cross-Modal_Asymmetric_Bidirectional_Motion_Fields_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/intelpro/CBMNet)
  - [ ]  [A Unified Pyramid Recurrent Network for Video Frame Interpolation](https://ar5iv.org/abs/2211.03456)<br>:star:[code](https://github.com/srcn-ivl/UPR-Net)
  - [ ]  [Joint Video Multi-Frame Interpolation and Deblurring under Unknown Exposure Time](http://ar5iv.org/abs/2303.15043v1)<br>:star:[code](https://github.com/shangwei5/VIDUE)
  - [ ]  [BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation](http://ar5iv.org/abs/2304.02225v1)<br>:star:[code](https://github.com/JunHeum/BiFormer)视频帧插值
  - [ ]  [Frame Interpolation Transformer and Uncertainty Guidance](https://openaccess.thecvf.com/content/CVPR2023/papers/Plack_Frame_Interpolation_Transformer_and_Uncertainty_Guidance_CVPR_2023_paper.pdf)
  - [ ]  [Range-Nullspace Video Frame Interpolation With Focalized Motion Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Range-Nullspace_Video_Frame_Interpolation_With_Focalized_Motion_Estimation_CVPR_2023_paper.pdf)
- [ ]  视频合成
  - [ ]  [Decomposed Diffusion Models for High-Quality Video Generation](https://ar5iv.org/abs/2303.08320)
  - [ ]  [MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/researchmm/MM-Diffusion)
  - [ ]  [Align Your Latents: High-Resolution Video Synthesis With Latent Diffusion Models](https://ar5iv.org/abs/2304.08818)<br>:house:[project](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)
- [ ]  视频预测
  - [ ]  [MOSO: Decomposing MOtion, Scene and Object for Video Prediction](https://ar5iv.org/abs/2303.03684)<br>:star:[code](https://github.com/anonymous202203/MOSO)
  - [ ]  [A Dynamic Multi-Scale Voxel Flow Network for Video Prediction](https://ar5iv.org/abs/2303.09875)<br>:star:[code](https://huxiaotaostasy.github.io/DMVFN/)
- [ ]  视频理解
  - [ ]  [Selective Structured State-Spaces for Long-Form Video Understanding](http://ar5iv.org/abs/2303.14526v1)
  - [ ]  [How you feelin'? Learning Emotions and Mental States in Movie Scenes](http://ar5iv.org/abs/2304.05634v1)<br>:star:[code](https://katha-ai.github.io/projects/emotx/)
  - [ ]  [System-status-aware Adaptive Network for Online Streaming Video Understanding](http://ar5iv.org/abs/2303.15742v1)
  - [ ]  [LAVENDER: Unifying Video-Language Understanding As Masked Language Modeling](https://ar5iv.org/abs/2206.07160)<br>:star:[code](https://github.com/microsoft/LAVENDER)
  - [ ]  [System-Status-Aware Adaptive Network for Online Streaming Video Understanding](https://ar5iv.org/abs/2303.15742)
  - [ ]  [Therbligs in Action: Video Understanding Through Motion Primitives](https://openaccess.thecvf.com/content/CVPR2023/papers/Dessalene_Therbligs_in_Action_Video_Understanding_Through_Motion_Primitives_CVPR_2023_paper.pdf)
  - [ ]  [Streaming Video Model](http://ar5iv.org/abs/2303.17228v1)<br>:star:[code](https://github.com/yuzhms/Streaming-Video-Model)
  - [ ]  [Procedure-Aware Pretraining for Instructional Video Understanding](http://ar5iv.org/abs/2303.18230v1)<br>:star:[code](https://github.com/salesforce/paprika)
  - [ ]  [Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations](http://ar5iv.org/abs/2303.17839v1)<br>:star:[code](https://github.com/facebookresearch/ProcedureVRL)
  - [ ]  [Revisiting Temporal Modeling for CLIP-based Image-to-Video Knowledge Transferring](https://ar5iv.org/pdf/2301.11116.pdf)<br>:star:[code](https://github.com/farewellthree/STAN)
- [ ]  视频分类
  - [ ]  [Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting](http://ar5iv.org/abs/2304.03307v1)<br>:star:[code](https://github.com/TalalWasim/Vita-CLIP)
  - [ ]  [Class Prototypes Based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Class_Prototypes_Based_Contrastive_Learning_for_Classifying_Multi-Label_and_Fine-Grained_CVPR_2023_paper.pdf)<br>:house:[project](https://nusci.csl.sri.com/project/APPROVE)
- [ ]  视频描述
  - [ ]  [Fine-grained Audible Video Description](http://ar5iv.org/abs/2303.15616v1)
- [ ]  视频摘要
  - [ ]  [Align and Attend: Multimodal Summarization with Dual Contrastive Losses](https://ar5iv.org/abs/2303.07284)<br>:star:[code](https://boheumd.github.io/A2Summ/)
- [ ]  视频识别
  - [ ]  [Frame Flexible Network](http://ar5iv.org/abs/2303.14817v1)<br>:star:[code](https://github.com/BeSpontaneous/FFN)<br>:thumbsup:[CVPR-2023 | FFN: 针对视频识别的通用Once-For-All框架](https://mp.weixin.qq.com/s/-h0P7_mcxlqwTt5uWgeiMg)
  - [ ]  [Use Your Head: Improving Long-Tail Video Recognition](http://ar5iv.org/abs/2304.01143v1)
- [ ]  Video Deflickering(去闪烁)
  - [ ]  [Blind Video Deflickering by Neural Filtering with a Flawed Atlas](https://ar5iv.org/abs/2303.08120)<br>:star:[code](http://github.com/ChenyangLEI/All-In-One-Deflicker)
- [ ]  时间句子定位(TSG)
  - [ ]  [You Can Ground Earlier than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos](https://ar5iv.org/abs/2303.07863)
  - [ ]  [Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided Self-Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Weakly_Supervised_Temporal_Sentence_Grounding_With_Uncertainty-Guided_Self-Training_CVPR_2023_paper.pdf)
- [ ]  VAD
  - [ ]  [Hierarchical Semantic Contrast for Scene-aware Video Anomaly Detection](http://ar5iv.org/abs/2303.13051v1)
  - [ ]  [Video Event Restoration Based on Keyframes for Video Anomaly Detection](http://ar5iv.org/abs/2304.05112v1)
  - [ ]  [Generating Anomalies for Video Anomaly Detection With Prompt-Based Feature Mapping](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.pdf) 
  - [ ]  [Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection](http://ar5iv.org/abs/2303.12369)
  - [ ]  [Exploiting Completeness and Uncertainty of Pseudo Labels for Weakly Supervised Video Anomaly Detection](https://ar5iv.org/abs/2212.04090)
  - [ ]  [Look Around for Anomalies: Weakly-Supervised Anomaly Detection via Context-Motion Relational Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Look_Around_for_Anomalies_Weakly-Supervised_Anomaly_Detection_via_Context-Motion_Relational_CVPR_2023_paper.pdf)
- [ ]  视频异常定位
  - [ ]  [EVAL: Explainable Video Anomaly Localization](https://ar5iv.org/abs/2212.07900)
- [ ]  视频镜像检测
  - [ ]  [Learning To Detect Mirrors From Videos via Dual Correspondences](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Learning_To_Detect_Mirrors_From_Videos_via_Dual_Correspondences_CVPR_2023_paper.pdf)<br>:house:[project](https://jiaying.link/cvpr2023-vmd/)
  - [ ]  视频表示学习
    - [ ]  [Weakly Supervised Video Representation Learning With Unaligned Text for Sequential Videos](https://ar5iv.org/abs/2303.12370)<br>:star:[code](https://github.com/svip-lab/WeakSVR)
    - [ ]  [Learning Procedure-Aware Video Representation From Instructional Videos and Their Narrations](https://ar5iv.org/abs/2303.17839)<br>:star:[code](https://github.com/facebookresearch/ProcedureVRL)
    - [ ]  [Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised Video Representation Learning](https://ar5iv.org/abs/2212.04500)<br>:star:[code](https://github.com/ruiwang2021/mvd)
    - [ ]  [Modeling Video As Stochastic Processes for Fine-Grained Video Representation Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Modeling_Video_As_Stochastic_Processes_for_Fine-Grained_Video_Representation_Learning_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/hengRUC/VSP)
- [ ]  Video Paragraph Grounding
  - [ ]  [Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.pdf)
- [ ]  Video Grounding
  - [ ]  [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://ar5iv.org/abs/2303.04995)
  - [ ]  [WINNER: Weakly-Supervised hIerarchical decompositioN and aligNment for Spatio-tEmporal Video gRounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.pdf)
  - [ ]  [Iterative Proposal Refinement for Weakly-Supervised Video Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.pdf)
  - [ ]  [Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.pdf)
  - [ ]  [ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ProTeGe_Untrimmed_Pretraining_for_Video_Temporal_Grounding_by_Video_Temporal_CVPR_2023_paper.pdf)
- [ ]  视频阴影检测
  - [ ]  [A Transformer Video Shadow Detection Framework](https://ar5iv.org/abs/2211.06885)<br>:house:[project](https://lihaoliu-cambridge.github.io/scotch_and_soda/)
- [ ]  视频关键点检测
  - [ ]  [Recurrence Without Recurrence: Stable Video Landmark Detection With Deep Equilibrium Models](https://ar5iv.org/abs/2304.00600)
- [ ]  视频情感检测
  - [ ]  [Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf)
- [ ]  场景检测
  - [ ]  [Efficient Movie Scene Detection Using State-Space Transformers](http://ar5iv.org/abs/2212.14427)

<a name="13"/>

## 13.GAN
- [ ]  [AdaptiveMix: Improving GAN Training via Feature Space Shrinkage](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_AdaptiveMix_Improving_GAN_Training_via_Feature_Space_Shrinkage_CVPR_2023_paper.pdf)
- [ ]  [Masked Auto-Encoders Meet Generative Adversarial Networks and Beyond](https://openaccess.thecvf.com/content/CVPR2023/papers/Fei_Masked_Auto-Encoders_Meet_Generative_Adversarial_Networks_and_Beyond_CVPR_2023_paper.pdf)
- [ ]  [Spider GAN: Leveraging Friendly Neighbors To Accelerate GAN Training](http://ar5iv.org/abs/2305.07613)
- [ ]  [Learning on Gradients: Generalized Artifacts Representation for GAN-Generated Images Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Learning_on_Gradients_Generalized_Artifacts_Representation_for_GAN-Generated_Images_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/chuangchuangtan/LGrad)
- [ ]  [MoStGAN-V: Video Generation With Temporal Motion Styles](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_MoStGAN-V_Video_Generation_With_Temporal_Motion_Styles_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xiaoqian-shen/MoStGAN-V)
- [ ]  [Sequential Training of GANs Against GAN-Classifiers Reveals Correlated "Knowledge Gaps" Present Among Independently Trained GAN Instances](https://ar5iv.org/abs/2303.15533)
- [ ]  [Re-GAN: Data-Efficient GANs Training via Architectural Reconfiguration](https://openaccess.thecvf.com/content/CVPR2023/papers/Saxena_Re-GAN_Data-Efficient_GANs_Training_via_Architectural_Reconfiguration_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/IntellicentAI-Lab/Re-GAN)
- [ ]  [HumanGen: Generating Human Radiance Fields With Explicit Priors](https://ar5iv.org/abs/2212.05321)
- [ ]  [Bi-Directional Feature Fusion Generative Adversarial Network for Ultra-High Resolution Pathological Image Virtual Re-Staining](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Bi-Directional_Feature_Fusion_Generative_Adversarial_Network_for_Ultra-High_Resolution_Pathological_CVPR_2023_paper.pdf)
- [ ]  [GlassesGAN: Eyewear Personalization Using Synthetic Appearance Discovery and Targeted Subspace Modeling](https://ar5iv.org/abs/2210.14145)
- [ ]  [Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint](https://ar5iv.org/abs/2211.11448)<br>:house:[project](https://kumapowerliu.github.io/CLCAE)
- [ ]  [3DAvatarGAN: Bridging Domains for Personalized Editable Avatars](https://ar5iv.org/abs/2301.02700)<br>:house:[project](https://rameenabdal.github.io/3DAvatarGAN/)
- [ ]  [GLeaD: Improving GANs With a Generator-Leading Task](https://ar5iv.org/abs/2212.03752)<br>:house:[project](https://ezioby.github.io/glead/)
- [ ]  [Transforming the Residuals for Real Image Editing With StyleGAN](https://ar5iv.org/abs/2212.14359)<br>:star:[code](https://github.com/hamzapehlivan/StyleRes)
- [ ]  [Improving GAN Training via Feature Space Shrinkage](https://ar5iv.org/abs/2303.01559)<br>:star:[code](https://github.com/WentianZhang-ML/AdaptiveMix)
- [ ]  [Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training](http://ar5iv.org/abs/2305.07613v1)
- [ ]  [NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs](http://ar5iv.org/abs/2304.05866v1)<br>:star:[code](https://rangwani-harsh.github.io/NoisyTwins/)
- [ ]  [Graph Transformer GANs for Graph-Constrained House Generation](https://ar5iv.org/abs/2303.08225)
- [ ]  [Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences between Pretrained Generative Models](https://ar5iv.org/abs/2303.10774)<br>:star:[code](https://github.com/mattolson93/cross_gan_auditing)
- [ ]  [Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis](http://ar5iv.org/abs/2303.14157v1)<br>:star:[code](https://github.com/VinAIResearch/CREPS)
- [ ]  [VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs](http://ar5iv.org/abs/2303.15893v1)<br>:star:[code](http://afruehstueck.github.io/vive3D)
- [ ]  [Discriminator-Cooperated Feature Map Distillation for GAN Compression](https://ar5iv.org/abs/2212.14169)<br>:star:[code](https://github.com/poopit/DCD-official)
- [ ]  [Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field](http://ar5iv.org/abs/2304.03526v1)<br>:star:[code](https://len-li.github.io/lift3d-web)
- [ ]  图像-文本合成
  - [ ]  [Scaling up GANs for Text-to-Image Synthesis](https://ar5iv.org/abs/2303.05511)<br>:house:[project](https://mingukkang.github.io/GigaGAN/)
- [ ]  扩散模型
  - [ ]  [How to Backdoor Diffusion Models?](https://ar5iv.org/abs/2212.05400)<br>:star:[code](https://github.com/IBM/BadDiffusion)
  - [ ]  [Diffusion Probabilistic Model Made Slim](http://ar5iv.org/abs/2211.17106)
  - [ ]  [VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models](http://ar5iv.org/abs/2211.11319)
  - [ ]  [Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models](http://ar5iv.org/abs/2212.03860)
  - [ ]  [Seeing Beyond the Brain: Conditional Diffusion Model With Sparse Masked Modeling for Vision Decoding](http://ar5iv.org/abs/2211.06956)
  - [ ]  [Self-Guided Diffusion Models](http://ar5iv.org/abs/2210.06462)
  - [ ]  [ObjectStitch: Object Compositing With Diffusion Model](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_ObjectStitch_Object_Compositing_With_Diffusion_Model_CVPR_2023_paper.pdf)
  - [ ]  [Solving 3D Inverse Problems Using Pre-Trained 2D Diffusion Models](https://ar5iv.org/abs/2211.10655)
  - [ ]  [Parallel Diffusion Models of Operator and Image for Blind Inverse Problems](https://ar5iv.org/abs/2211.10656)
  - [ ]  [RGBD2: Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models](https://ar5iv.org/abs/2212.05993)<br>:house:[project](https://jblei.site/proj/rgbd-diffusion)
  - [ ]  [Dimensionality-Varying Diffusion Process](https://ar5iv.org/abs/2211.16032)
  - [ ]  [TrojDiff: Trojan Attacks on Diffusion Models With Diverse Targets](https://ar5iv.org/abs/2303.05762)<br>:star:[code](https://github.com/chenweixin107/TrojDiff)
  - [ ]  [Towards Practical Plug-and-Play Diffusion Models](https://ar5iv.org/abs/2212.05973)<br>:star:[code](https://github.com/riiid/PPAP)
  - [ ]  [All Are Worth Words: A ViT Backbone for Diffusion Models](https://ar5iv.org/abs/2209.12152)
  - [ ]  [Unite and Conquer: Plug & Play Multi-Modal Synthesis using Diffusion Models](https://ar5iv.org/abs/2212.00793)<br>:house:[project](https://nithin-gk.github.io/projectpages/Multidiff/index.html)
  - [ ]  [Binary Latent Diffusion](https://ar5iv.org/abs/2304.04820)
  - [ ]  [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://ar5iv.org/abs/2211.05105)
  - [ ]  [Lookahead Diffusion Probabilistic Models for Refining Mean Estimation](https://ar5iv.org/abs/2304.11312)
  - [ ]  [EDICT: Exact Diffusion Inversion via Coupled Transformations](https://ar5iv.org/abs/2211.12446)<br>:star:[code](https://github.com/salesforce/EDICT)
  - [ ]  [ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf)
- [ ]  GAN 逆映射
  - [ ]  [3D GAN Inversion With Facial Symmetry Prior](https://ar5iv.org/abs/2211.16927)<br>:house:[project](https://feiiyin.github.io/SPI/)
  - [ ]  [NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-Shot Real Image Animation](https://ar5iv.org/abs/2211.17235)
  - [ ]  [Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion](https://ar5iv.org/abs/2212.07409)<br>:house:[project](https://nirvanalan.github.io/projects/E3DGE/index.html)
  - [ ]  [High-Fidelity 3D GAN Inversion by Pseudo-Multi-View Optimization](https://ar5iv.org/abs/2211.15662)<br>:house:[project](https://ken-ouyang.github.io/HFGI3D/index.html)

<a name="12"/>

## 12.Image-to-Image Translation(图像到图像翻译)
- [ ]  [3D-Aware Multi-Class Image-to-Image Translation With NeRFs](http://ar5iv.org/abs/2303.15012)
- [ ]  [Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation](http://ar5iv.org/abs/2211.12572)
- [ ]  [DSI2I: Dense Style for Unpaired Image-to-Image Translation](https://ar5iv.org/abs/2212.13253)
- [ ]  [Fix the Noise: Disentangling Source Feature for Controllable Domain Translation](https://ar5iv.org/abs/2303.11545)<br>:star:[code](https://github.com/LeeDongYeun/FixNoise)
- [ ]  [3D-Aware Multi-Class Image-to-Image Translation with NeRFs](http://ar5iv.org/abs/2303.15012v1)  
- [ ]  [LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data](https://ar5iv.org/abs/2208.14889)<br>:house:[project](https://ku-cvlab.github.io/LANIT/)
- [ ]  [Unpaired Image-to-Image Translation With Shortest Path Regularization](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Unpaired_Image-to-Image_Translation_With_Shortest_Path_Regularization_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Mid-Push/santa)
- [ ]  [BBDM: Image-to-Image Translation With Brownian Bridge Diffusion Models](https://ar5iv.org/abs/2205.07680)
- [ ]  图像翻译    
  - [ ]  [Masked and Adaptive Transformer for Exemplar Based Image Translation](http://ar5iv.org/abs/2303.17123v1)<br>:star:[code](https://github.com/AiArt-HDU/MATEBIT)
- [ ]  视频翻译
  - [ ]  [On the Difficulty of Unpaired Infrared-to-Visible Video Translation: Fine-Grained Content-Rich Patches Transfer](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_On_the_Difficulty_of_Unpaired_Infrared-to-Visible_Video_Translation_Fine-Grained_Content-Rich_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/BIT-DA/I2V-Processing) 
  
<a name="11"/>

## 11.Face(人脸)
- [ ]  [Rethinking Feature-Based Knowledge Distillation for Face Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Rethinking_Feature-Based_Knowledge_Distillation_for_Face_Recognition_CVPR_2023_paper.pdf)
- [ ]  [Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning](http://ar5iv.org/abs/2302.11102)
- [ ]  [Learning a 3D Morphable Face Reflectance Model From Low-Cost Data](http://ar5iv.org/abs/2303.11686)
- [ ]  [CLIP2Protect: Protecting Facial Privacy Using Text-Guided Makeup via Adversarial Latent Search](https://openaccess.thecvf.com/content/CVPR2023/papers/Shamshad_CLIP2Protect_Protecting_Facial_Privacy_Using_Text-Guided_Makeup_via_Adversarial_Latent_CVPR_2023_paper.pdf)
- [ ]  [Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis From Monocular Image](https://ar5iv.org/abs/2211.13901)<br>:house:[project](https://yudeng.github.io/GRAMInverter/)
- [ ]  [Learning Neural Proto-Face Field for Disentangled 3D Face Modeling in the Wild](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf)
- [ ]  [Evading Forensic Classifiers With Attribute-Conditioned Adversarial Faces](https://openaccess.thecvf.com/content/CVPR2023/papers/Shamshad_Evading_Forensic_Classifiers_With_Attribute-Conditioned_Adversarial_Faces_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/koushiksrivats/face_attribute_attack)
- [ ]  [Improving Fairness in Facial Albedo Estimation via Visual-Textual Cues](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Improving_Fairness_in_Facial_Albedo_Estimation_via_Visual-Textual_Cues_CVPR_2023_paper.pdf)
- [ ]  [Attribute-Preserving Face Dataset Anonymization via Latent Code Optimization](https://ar5iv.org/abs/2303.11296)<br>:star:[code](https://github.com/chi0tzp/FALCO)
- [ ]  [Pose-Disentangled Contrastive Learning for Self-Supervised Facial Representation](https://ar5iv.org/abs/2211.13490)<br>:star:[code](https://github.com/DreamMr/PCL)
- [ ]  [Privacy-Preserving Adversarial Facial Features](https://ar5iv.org/abs/2305.05391)
- [ ]  [BioNet: A Biologically-Inspired Network for Face Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_BioNet_A_Biologically-Inspired_Network_for_Face_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/pengyuLPY/BioNet.git)
- [ ]  [High-Res Facial Appearance Capture From Polarized Smartphone Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Azinovic_High-Res_Facial_Appearance_Capture_From_Polarized_Smartphone_Images_CVPR_2023_paper.pdf)
- [ ]  [MARLIN: Masked Autoencoder for Facial Video Representation LearnINg](https://ar5iv.org/abs/2211.06627)<br>:star:[code](https://github.com/ControlNet/MARLIN)
- [ ]  [Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face Recognition](http://ar5iv.org/abs/2303.12512v1)<br>:house:[project](https://carlyx.github.io/DPE/)
- [ ]  [Disentanglement of Pose and Expression for General Video Portrait Editing](https://ar5iv.org/abs/2301.06281)
- [ ]  [BlendFields: Few-Shot Example-Driven Facial Modeling](http://ar5iv.org/abs/2305.07514v1)<br>:star:[code](https://blendfields.github.io/)
- [ ]  [Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition](http://ar5iv.org/abs/2303.15818v1)<br>:thumbsup:[CVPR 2023 | 人脸识别路漫漫：清华、北大等提出AT3D人脸识别系统攻击方法](https://zhuanlan.zhihu.com/p/618189634)
- [ ]  [Collaborative Diffusion for Multi-Modal Face Generation and Editing](http://ar5iv.org/abs/2304.10530v1)<br>:star:[code](https://ziqihuangg.github.io/projects/collaborative-diffusion.html)<br>:star:[code](https://github.com/ziqihuangg/Collaborative-Diffusion)<br>:thumbsup:[CVPR 2023 | Collaborative Diffusion 怎样让不同的扩散模型合作？](https://mp.weixin.qq.com/s/yhxcKlj6m-M6Hw7X31-JYQ)
- [ ]  [Recognizability Embedding Enhancement for Very Low-Resolution Face Recognition and Quality Estimation](http://ar5iv.org/abs/2304.10066v1)
- [ ]  [DiffusionRig: Learning Personalized Priors for Facial Appearance Editing](http://ar5iv.org/abs/2304.06711v1)<br>:star:[code](https://diffusionrig.github.io)
- [ ]  [Probabilistic Knowledge Distillation of Face Ensembles](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Probabilistic_Knowledge_Distillation_of_Face_Ensembles_CVPR_2023_paper.pdf)
- [ ]  [DCFace: Synthetic Face Generation with Dual Condition Diffusion Model](http://ar5iv.org/abs/2304.07060v1)<br>:star:[code](https://github.com/mk-minchul/dcface)
- [ ]  [Discrete Point-Wise Attack Is Not Enough: Generalized Manifold Adversarial Attack for Face Recognition](https://ar5iv.org/abs/2301.06083)
- [ ]  3D 人脸
  - [ ]  [Graphics Capsule: Learning Hierarchical 3D Face Representations from 2D Images](https://ar5iv.org/abs/2303.10896)
  - [ ]  [Physical-World Optical Adversarial Attacks on 3D Face Recognition](https://ar5iv.org/abs/2205.13412)
  - [ ]  [Learning a 3D Morphable Face Reflectance Model from Low-cost Data](https://ar5iv.org/abs/2303.11686)<br>:house:[project](https://yxuhan.github.io/ReflectanceMM/index.html)
  - [ ]  [NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images](http://ar5iv.org/abs/2303.14092v1)<br>:star:[code](https://github.com/aejion/NeuFace)
  - [ ]  [FaceLit: Neural 3D Relightable Faces](http://ar5iv.org/abs/2303.15437v1)
  - [ ]  [Learning Neural Proto-face Field for Disentangled 3D Face Modeling In the Wild](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf)
  - [ ]  [High-Fidelity 3D Face Generation From Natural Language Descriptions](https://ar5iv.org/abs/2305.03302)<br>:star:[code](https://github.com/zhuhao-nju/describe3d)
  - [ ]  [CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior](https://ar5iv.org/abs/2301.02379)<br>:star:[code](https://github.com/Doubiiu/CodeTalker)
  - [ ]  [PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360deg](https://openaccess.thecvf.com/content/CVPR2023/papers/An_PanoHead_Geometry-Aware_3D_Full-Head_Synthesis_in_360deg_CVPR_2023_paper.pdf)
- [ ]  人脸重建
  - [ ]  [A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images](https://ar5iv.org/pdf/2302.14434.pdf)<br>:house:[project](https://younglbw.github.io/HRN-homepage/)
  - [ ]  [Graphics Capsule: Learning Hierarchical 3D Face Representations From 2D Images](https://ar5iv.org/abs/2303.10896)
  - [ ]  [FFHQ-UV: Normalized Facial UV-Texture Dataset for 3D Face Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_FFHQ-UV_Normalized_Facial_UV-Texture_Dataset_for_3D_Face_Reconstruction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/csbhr/FFHQ-UV)
  - [ ]  [Robust Model-Based Face Reconstruction Through Weakly-Supervised Outlier Segmentation](https://ar5iv.org/abs/2106.09614)
  - [ ]  [AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction](https://ar5iv.org/abs/2304.13115)<br>:house:[project](https://aggelinacha.github.io/AVFace/)
- [ ]  人脸恢复
  - [ ]  [DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration](https://ar5iv.org/abs/2303.06885)
- [ ]  人脸对齐
  - [ ]  [- [ ]  [DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face Alignment](http://ar5iv.org/abs/2305.11522v1)<br>:star:[code](https://github.com/lhyfst/DSFNet)]
- [ ]  人脸匿名化
  - [ ]  [Attribute-preserving Face Dataset Anonymization via Latent Code Optimization](https://ar5iv.org/abs/2303.11296)<br>:star:[code](https://github.com/chi0tzp/FALCO)
- [ ]  人脸超分辨率
  - [ ]  [Spatial-Frequency Mutual Learning for Face Super-Resolution](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Spatial-Frequency_Mutual_Learning_for_Face_Super-Resolution_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/wcy-cs/SFMNet)
- [ ]  裸眼年龄识别
  - [ ]  [DAA: A Delta Age AdaIN operation for age estimation via binary code transformer](https://ar5iv.org/abs/2303.07929)
- [ ]  情绪识别
  - [ ]  [Context De-confounded Emotion Recognition](https://ar5iv.org/abs/2303.11921)
  - [ ]  [Decoupled Multimodal Distilling for Emotion Recognition](http://ar5iv.org/abs/2303.13802v1)<br>:star:[code](https://github.com/mdswyz/DMD)
  - [ ]  [Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Multivariate_Multi-Frequency_and_Multimodal_Rethinking_Graph_Neural_Networks_for_Emotion_CVPR_2023_paper.pdf)
  - [ ]  [Learning Emotion Representations from Verbal and Nonverbal Communication](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Emotion_Representations_From_Verbal_and_Nonverbal_Communication_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Xeaver/EmotionCLIP)
- [ ]  人像照明
  - [ ]  [LightPainter: Interactive Portrait Relighting with Freehand Scribble](http://ar5iv.org/abs/2303.12950v1)
- [ ]  人脸活体检测
  - [ ]  [Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment](http://ar5iv.org/abs/2303.13662v1)
  - [ ]  [Instance-Aware Domain Generalization for Face Anti-Spoofing](http://ar5iv.org/abs/2304.05640v1)<br>:star:[code](https://github.com/qianyuzqy/IADG)
- [ ]  说话头
  - [ ]  [OTAvatar: One-shot Talking Face Avatar with Controllable Tri-plane Rendering](http://ar5iv.org/abs/2303.14662v1)<br>:star:[code](https://github.com/theEricMa/OTAvatar)
  - [ ]  [High-Fidelity Generalized Emotional Talking Face Generation With Multi-Modal Emotion Space Learning](http://ar5iv.org/abs/2305.02572)
  - [ ]  [Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis](https://ar5iv.org/abs/2211.14506)
  - [ ]  [LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LipFormer_High-Fidelity_and_Generalizable_Talking_Face_Generation_With_a_Pre-Learned_CVPR_2023_paper.pdf)
  - [ ]  [SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation](https://ar5iv.org/abs/2211.12194)<br>:star:[code](https://github.com/Winfredy/SadTalker)
  - [ ]  [Implicit Neural Head Synthesis via Controllable Local Deformation Fields](http://ar5iv.org/abs/2304.11113v1)
  - [ ]  [Identity-Preserving Talking Face Generation with Landmark and Appearance Priors](http://ar5iv.org/abs/2305.08293v1)<br>:star:[code](https://github.com/Weizhi-Zhong/IP_LAP)
  - [ ]  [Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert](http://ar5iv.org/abs/2303.17480v1)
  - [ ]  [High-Fidelity and Freely Controllable Talking Head Video Generation](http://ar5iv.org/abs/2304.10168v1)<br>:house:[project](https://yuegao.me/PECHead)
  - [ ]  [High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning](http://ar5iv.org/abs/2305.02572)
  - [ ]  [GANHead: Towards Generative Animatable Neural Head Avatars](http://ar5iv.org/abs/2304.03950v1)<br>:star:[code](https://wsj-sjtu.github.io/GANHead/)
  - [ ]  [One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field](http://ar5iv.org/abs/2304.05097v1)<br>:house:[project](https://www.waytron.net/hidenerf/)
  - [ ]  [MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation](https://ar5iv.org/abs/2212.08062)<br>:house:[project](https://meta-portrait.github.io/)
- [ ]  人脸分割
  - [ ]  [Parameter Efficient Local Implicit Image Function Network for Face Segmentation](http://ar5iv.org/abs/2303.15122v1)
- [ ]  眨眼检测
  - [ ]  [Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed Video](http://ar5iv.org/abs/2303.16053v1)
- [ ]  三维头像生成
  - [ ]  [Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos](http://ar5iv.org/abs/2304.01436v1)<br>:star:[code](https://augmentedperception.github.io/monoavatar/)
  - [ ]  [Instant Volumetric Head Avatars](https://ar5iv.org/abs/2211.12499)<br>:house:[project](https://zielon.github.io/insta/)
  - [ ]  [Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars](https://ar5iv.org/abs/2211.11208)<br>:house:[project](https://mrtornado24.github.io/Next3D/)
  - [ ]  [OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis](http://ar5iv.org/abs/2303.15539)
  - [ ]  [PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360◦](https://ar5iv.org/abs/2303.13071)
- [ ]  人脸表情识别
  - [ ]  [Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Rethinking_the_Learning_Paradigm_for_Dynamic_Facial_Expression_Recognition_CVPR_2023_paper.pdf)
- [ ]  微表情识别
  - [ ]  [Micron-BERT: BERT-based Facial Micro-Expression Recognition](http://ar5iv.org/abs/2304.03195v1)<br>:star:[code](https://github.com/uark-cviu/Micron-BERT)微表情识别
  - [ ]  [Feature Representation Learning With Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition](https://ar5iv.org/abs/2304.04420)
  - [ ]  [SelfME: Self-Supervised Motion Learning for Micro-Expression Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_SelfME_Self-Supervised_Motion_Learning_for_Micro-Expression_Recognition_CVPR_2023_paper.pdf)
- [ ]  人脸合成
  - [ ]  [High-Fidelity 3D Face Generation from Natural Language Descriptions](https://ar5iv.org/abs/2305.03302)<br>:house:[project](https://github.com/zhuhao-nju/describe3d)
  - [ ]  [StyleGene: Crossover and Mutation of Region-Level Facial Genes for Kinship Face Synthesis](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_StyleGene_Crossover_and_Mutation_of_Region-Level_Facial_Genes_for_Kinship_CVPR_2023_paper.pdf)
- [ ]  假脸检测
  - [ ]  [AUNet: Learning Relations Between Action Units for Face Forgery Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_AUNet_Learning_Relations_Between_Action_Units_for_Face_Forgery_Detection_CVPR_2023_paper.pdf)
  - [ ]  [AltFreezing for More General Video Face Forgery Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_AltFreezing_for_More_General_Video_Face_Forgery_Detection_CVPR_2023_paper.pdf)
- [ ]  Facial Action Unit Detection
  - [ ]  [Biomechanics-guided Facial Action Unit Detection through Force Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Biomechanics-Guided_Facial_Action_Unit_Detection_Through_Force_Modeling_CVPR_2023_paper.pdf)
- [ ]  人脸视频编辑
  - [ ]  [Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding](https://ar5iv.org/abs/2212.02802)<br>:house:[project](https://diff-video-ae.github.io/)
- [ ]  人脸质量评估
  - [ ]  [Face Image Quality Assessment by Learning Sample Relative Classifiability](https://openaccess.thecvf.com/content/CVPR2023/papers/Boutros_CR-FIQA_Face_Image_Quality_Assessment_by_Learning_Sample_Relative_Classifiability_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/fdbtrs/CR-FIQA)
- [ ]  人脸交换
  - [ ]  [3D-Aware Face Swapping](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_3D-Aware_Face_Swapping_CVPR_2023_paper.pdf)<br>:star:[code](https://lyx0208.github.io/3dSwap)
  - [ ]  [Implicit Identity Driven Deepfake Face Swapping Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Implicit_Identity_Driven_Deepfake_Face_Swapping_Detection_CVPR_2023_paper.pdf)
  - [ ]  [StyleIPSB: Identity-Preserving Semantic Basis of StyleGAN for High Fidelity Face Swapping](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_StyleIPSB_Identity-Preserving_Semantic_Basis_of_StyleGAN_for_High_Fidelity_Face_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/a686432/StyleIPSB)
  - [ ]  [Fine-Grained Face Swapping via Regional GAN Inversion](https://ar5iv.org/abs/2211.14068)<br>:house:[project](http://e4s2022.github.io/)
  - [ ]  [DiffSwap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_DiffSwap_High-Fidelity_and_Controllable_Face_Swapping_via_3D-Aware_Masked_Diffusion_CVPR_2023_paper.pdf)
- [ ]  人脸聚类
  - [ ]  [Local Connectivity-Based Density Estimation for Face Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Local_Connectivity-Based_Density_Estimation_for_Face_Clustering_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/illian01/LCE-PCENet)
- [ ]  人脸修饰
  - [ ]  [Blemish-Aware and Progressive Face Retouching With Limited Paired Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Blemish-Aware_and_Progressive_Face_Retouching_With_Limited_Paired_Data_CVPR_2023_paper.pdf)
- [ ]  三维数字头像
  - [ ]  [A Generative Model for Sculpting 3D Digital Avatars Using Diffusion](https://ar5iv.org/abs/2212.06135)<br>:house:[project](https://3d-avatar-diffusion.microsoft.com/)
  - [ ]  [High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors](https://ar5iv.org/abs/2211.15064)
- [ ]  音频驱动的人脸重演
  - [ ]  [Parametric Implicit Face Representation for Audio-Driven Facial Reenactment](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Parametric_Implicit_Face_Representation_for_Audio-Driven_Facial_Reenactment_CVPR_2023_paper.pdf)
- [ ]  隐私保护
  - [ ]  [DartBlur: Privacy Preservation With Detection Artifact Suppression](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_DartBlur_Privacy_Preservation_With_Detection_Artifact_Suppression_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/JaNg2333/DartBlur)
- [ ]  人脸关键点检测
  - [ ]  [3D-Aware Facial Landmark Detection via Multi-View Consistent Training on Synthetic Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_3D-Aware_Facial_Landmark_Detection_via_Multi-View_Consistent_Training_on_Synthetic_CVPR_2023_paper.pdf)
  - [ ]  [STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_STAR_Loss_Reducing_Semantic_Ambiguity_in_Facial_Landmark_Detection_CVPR_2023_paper.pdf)
- [ ]  头部捕获
  - [ ]  [Instant Multi-View Head Capture Through Learnable Registration](https://openaccess.thecvf.com/content/CVPR2023/papers/Bolkart_Instant_Multi-View_Head_Capture_Through_Learnable_Registration_CVPR_2023_paper.pdf)<br>:house:[project](https://tempeh.is.tue.mpg.de/)
- [ ]  年龄估计
  - [ ]  [DAA: A Delta Age AdaIN Operation for Age Estimation via Binary Code Transformer](http://ar5iv.org/abs/2303.07929)

<a name="10"/>

## 10.3D(三维重建\三维视觉)
- [ ]  [Structured 3D Features for Reconstructing Controllable Avatars](https://ar5iv.org/abs/2212.06820)<br>:house:[project](https://enriccorona.github.io/s3f/)
- [ ]  [In-Hand 3D Object Scanning from an RGB Sequence](https://ar5iv.org/abs/2211.16193)
- [ ]  [Learning Geometric-Aware Properties in 2D Representation Using Lightweight CAD Models, or Zero Real 3D Pairs](https://openaccess.thecvf.com/content/CVPR2023/papers/Arsomngern_Learning_Geometric-Aware_Properties_in_2D_Representation_Using_Lightweight_CAD_Models_CVPR_2023_paper.pdf)<br>:star:[code]( https://GeoAware2dRepUsingCAD.github.io/)
- [ ]  [3D Concept Learning and Reasoning from Multi-View Images](https://ar5iv.org/abs/2303.11327)<br>:house:[project](https://vis-www.cs.umass.edu/3d-clr/)
- [ ]  [LP-DIF: Learning Local Pattern-Specific Deep Implicit Function for 3D Objects and Scenes](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LP-DIF_Learning_Local_Pattern-Specific_Deep_Implicit_Function_for_3D_Objects_CVPR_2023_paper.pdf)<br>:house:[project](https://github.com/gtyxyz/lpdif)
- [ ]  [DynamicStereo: Consistent Dynamic Depth From Stereo Videos](https://ar5iv.org/abs/2305.02296)<br>:house:[project](https://dynamic-stereo.github.io/)
- [ ]  [ARO-Net: Learning Implicit Fields from Anchored Radial Observations](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ARO-Net_Learning_Implicit_Fields_From_Anchored_Radial_Observations_CVPR_2023_paper.pdf)
- [ ]  [G-MSM:Unsupervised Multi-Shape Matching With Graph-Based Affinity Priors](https://openaccess.thecvf.com/content/CVPR2023/papers/Eisenberger_G-MSM_Unsupervised_Multi-Shape_Matching_With_Graph-Based_Affinity_Priors_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/marvin-eisenberger/gmsm-matching)
- [ ]  [Magic3D: High-Resolution Text-to-3D Content Creation](https://ar5iv.org/abs/2211.10440)<br>:house:[project](https://research.nvidia.com/labs/dir/magic3d/)
- [ ]  [PointListNet: Deep Learning on 3D Point Lists](https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_PointListNet_Deep_Learning_on_3D_Point_Lists_CVPR_2023_paper.pdf)
- [ ]  [Omnimatte3D: Associating Objects and Their Effects in Unconstrained Monocular Video](https://openaccess.thecvf.com/content/CVPR2023/papers/Suhail_Omnimatte3D_Associating_Objects_and_Their_Effects_in_Unconstrained_Monocular_Video_CVPR_2023_paper.pdf)
- [ ]  [HexPlane: A Fast Representation for Dynamic Scenes](https://ar5iv.org/abs/2301.09632)<br>:house:[project](https://caoang327.github.io/HexPlane)
- [ ]  [Energy-Efficient Adaptive 3D Sensing](https://openaccess.thecvf.com/content/CVPR2023/papers/Tilmon_Energy-Efficient_Adaptive_3D_Sensing_CVPR_2023_paper.pdf)<br>:house:[project](https://btilmon.github.io/e3d.html)
- [ ]  [Objaverse: A Universe of Annotated 3D Objects](https://ar5iv.org/abs/2212.08051)<br>:house:[project](http://objaverse.allenai.org/)
- [ ]  [Level-S2fM: Structure from Motion on Neural Level Set of Implicit Surfaces](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Level-S2fM_Structure_From_Motion_on_Neural_Level_Set_of_Implicit_CVPR_2023_paper.pdf)<br>:house:[project](https://henry123-boy.github.io/level-s2fm/)
- [ ]  [3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions](https://ar5iv.org/abs/2212.11263)<br>:star:[code](https://github.com/threedle/3DHighlighter)
- [ ]  [OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation](https://ar5iv.org/abs/2301.07525)<br>:house:[project](https://omniobject3d.github.io/)<br>:thumbsup:[CVPR 2023 Award Candidate | 真实高精三维物体数据集OmniObject3D](https://mp.weixin.qq.com/s/aJQCc-Iu50bYP2cE1BrY-w)
- [ ]  [Neural Scene Chronology](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Neural_Scene_Chronology_CVPR_2023_paper.pdf)<br>:house:[project](https://zju3dv.github.io/NeuSC/)
- [ ]  [3D Neural Field Generation Using Triplane Diffusion](https://ar5iv.org/abs/2211.16677)<br>:house:[project](https://jryanshue.com/nfd)
- [ ]  [Learning Adaptive Dense Event Stereo From the Image Domain](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Learning_Adaptive_Dense_Event_Stereo_From_the_Image_Domain_CVPR_2023_paper.pdf)
- [ ]  [GANmouflage: 3D Object Nondetection With Texture Fields](https://ar5iv.org/abs/2201.07202)<br>:house:[project](https://rrrrrguo.github.io/ganmouflage/)
- [ ]  [Learning Accurate 3D Shape Based on Stereo Polarimetric Imaging](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Learning_Accurate_3D_Shape_Based_on_Stereo_Polarimetric_Imaging_CVPR_2023_paper.pdf)
- [ ]  [Sphere-Guided Training of Neural Implicit Surfaces](https://ar5iv.org/abs/2209.15511)<br>:house:[project](https://andreeadogaru.github.io/SphereGuided/)
- [ ]  [PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D Supervision](https://ar5iv.org/abs/2303.09554)<br>:house:[project](https://ktertikas.github.io/part_nerf)
- [ ]  [Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning](http://ar5iv.org/abs/2303.14191v1)<br>:star:[code](https://github.com/Pointcept/Pointcept)
- [ ]  [Self-Supervised Learning for Multimodal Non-Rigid 3D Shape Matching](https://ar5iv.org/abs/2303.10971)<br>:star:[code](https://github.com/dongliangcao/Self-Supervised-Multimodal-Shape-Matching)
- [ ]  [SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field](http://ar5iv.org/abs/2303.13277v1)<br>:star:[code](https://zju3dv.github.io/sine/)
- [ ]  [3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process](https://ar5iv.org/abs/2303.10406)<br>:star:[code](https://github.com/colorful-liyu/3DQD)
- [ ]  [DynamicStereo: Consistent Dynamic Depth from Stereo Videos](https://ar5iv.org/abs/2305.02296)<br>:house:[project](https://dynamic-stereo.github.io/)
- [ ]  [3D Concept Learning and Reasoning from Multi-View Images](https://ar5iv.org/abs/2303.11327)<br>:house:[project](https://vis-www.cs.umass.edu/3d-clr/)
- [ ]  [PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\circ}$](http://ar5iv.org/abs/2303.13071v1)<br>:star:[code](https://sizhean.github.io/panohead)
- [ ]  [Persistent Nature: A Generative Model of Unbounded 3D Worlds](https://ar5iv.org/abs/2303.13515)<br>:house:[project](https://chail.github.io/persistent-nature/)
- [ ]  [TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision](http://ar5iv.org/abs/2303.13273v1)
- [ ]  [Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization](http://ar5iv.org/abs/2303.13232v1)
- [ ]  [Robust Outlier Rejection for 3D Registration With Variational Bayes](http://ar5iv.org/abs/2304.01514)<br>:star:[code](https://github.com/Jiang-HB/VBReg)
- [ ]  [On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks](http://ar5iv.org/abs/2303.14840v1)<br>:star:[code](https://github.com/Junggy/HAMMER-dataset)
- [ ]  [SUDS: Scalable Urban Dynamic Scenes](http://ar5iv.org/abs/2303.14536v1)<br>:house:[project](https://haithemturki.com/suds/)
- [ ]  [Understanding and Improving Features Learned in Deep Functional Maps](http://ar5iv.org/abs/2303.16527v1)<br>:star:[code](https://github.com/pvnieo/clover)
- [ ]  [TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using Differentiable Rendering](http://ar5iv.org/abs/2303.15060v1)<br>:star:[code](https://jh-choi.github.io/TMO/)
- [ ]  [Generalizable Local Feature Pre-training for Deformable Shape Analysis](http://ar5iv.org/abs/2303.15104v1)<br>:star:[code](https://github.com/pvnieo/vader)
- [ ]  [CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects](http://ar5iv.org/abs/2303.15782v1)<br>:house:[project](http://carto.cs.uni-freiburg.de)
- [ ]  [CCuantuMM: Cycle-Consistent Quantum-Hybrid Matching of Multiple Shapes](http://ar5iv.org/abs/2303.16202v1)<br>:house:[project](https://4dqv.mpi-inf.mpg.de/CCuantuMM/)
- [ ]  [HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images](http://ar5iv.org/abs/2303.16509v1)<br>:star:[code](https://holodiffusion.github.io/)
- [ ]  [Multi-View Azimuth Stereo via Tangent Space Consistency](http://ar5iv.org/abs/2303.16447v1)<br>:star:[code](https://xucao-42.github.io/mvas_homepage/)
- [ ]  [3D Line Mapping Revisited](http://ar5iv.org/abs/2303.17504v1)<br>:star:[code](https://github.com/cvg/limap)
- [ ]  [NeRF-Supervised Deep Stereo](http://ar5iv.org/abs/2303.17603v1)<br>:star:[code](https://nerfstereo.github.io/)<br>:star:[code](https://github.com/fabiotosi92/NeRF-Supervised-Deep-Stereo)
- [ ]  [Robust Outlier Rejection for 3D Registration with Variational Bayes](http://ar5iv.org/abs/2304.01514v1)三维
- [ ]  [Incremental 3D Semantic Scene Graph Prediction from RGB Sequences](http://ar5iv.org/abs/2305.02743v1)
- [ ]  Stereo Matching
  - [ ]  [Iterative Geometry Encoding Volume for Stereo Matching](https://ar5iv.org/abs/2303.06615)<br>:star:[code](https://github.com/gangweiX/IGEV)
  - [ ]  [Masked representation learning for domain generalized stereo matching](https://openaccess.thecvf.com/content/CVPR2023/papers/Rao_Masked_Representation_Learning_for_Domain_Generalized_Stereo_Matching_CVPR_2023_paper.pdf)
  - [ ]  [Learning the Distribution of Errors in Stereo Matching for Joint Disparity and Uncertainty Estimation](http://ar5iv.org/abs/2304.00152v1)
  - [ ]  [Domain Generalized Stereo Matching via Hierarchical Visual Transformation](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Domain_Generalized_Stereo_Matching_via_Hierarchical_Visual_Transformation_CVPR_2023_paper.pdf)
  - [ ]  [Unsupervised Deep Asymmetric Stereo Matching With Spatially-Adaptive Self-Similarity](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Unsupervised_Deep_Asymmetric_Stereo_Matching_With_Spatially-Adaptive_Self-Similarity_CVPR_2023_paper.pdf)
  - [ ]  [High-frequency Stereo Matching Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_High-Frequency_Stereo_Matching_Network_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/David-Zhao-1997/High-frequency-Stereo-Matching-Network)
- [ ]  三维视觉
  - [ ]  [Context-Aware Alignment and Mutual Masking for 3D-Language Pre-Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/leolyj/3D-VLP)
- [ ]  三维重建
  - [ ]  [Neural Lens Modeling](http://ar5iv.org/abs/2304.04848v1)<br>:star:[code](https://neural-lens.github.io)
  - [ ]  [Self-Supervised Super-Plane for Neural 3D Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Self-Supervised_Super-Plane_for_Neural_3D_Reconstruction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/botaoye/S3PRecon)
  - [ ]  [Multi-Sensor Large-Scale Dataset for Multi-View 3D Reconstruction](http://ar5iv.org/abs/2203.06111)
  - [ ]  [ALTO: Alternating Latent Topologies for Implicit 3D Reconstruction](http://ar5iv.org/abs/2212.04096)
  - [ ]  [Towards Unbiased Volume Rendering of Neural Implicit Surfaces With Geometry Priors](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Unbiased_Volume_Rendering_of_Neural_Implicit_Surfaces_With_Geometry_CVPR_2023_paper.pdf)
  - [ ]  [Multiview Compressive Coding for 3D Reconstruction](https://ar5iv.org/abs/2301.08247)<br>:house:[project](https://mcc3d.github.io/)
  - [ ]  [Multi-View Reconstruction Using Signed Ray Distance Functions (SRDF)](http://ar5iv.org/abs/2209.00082)
  - [ ]  [PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Melas-Kyriazi_PC2_Projection-Conditioned_Point_Cloud_Diffusion_for_Single-Image_3D_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion)
  - [ ]  [RealFusion: 360deg Reconstruction of Any Object From a Single Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Melas-Kyriazi_RealFusion_360deg_Reconstruction_of_Any_Object_From_a_Single_Image_CVPR_2023_paper.pdf)<br>:house:[project](https://lukemelas.github.io/realfusion)
  - [ ]  [Deep Polarization Reconstruction With PDAVIS Events](https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Deep_Polarization_Reconstruction_With_PDAVIS_Events_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/SensorsINI/e2p)
  - [ ]  [RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Anciukevicius/RenderDiffusion)
  - [ ]  [Distilling Neural Fields for Real-Time Articulated Shape Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Distilling_Neural_Fields_for_Real-Time_Articulated_Shape_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://jefftan969.github.io/dasr)
  - [ ]  [High-Fidelity Clothed Avatar Reconstruction from a Single Image](https://ar5iv.org/abs/2304.03903)
  - [ ]  [Efficient Second-Order Plane Adjustment](https://ar5iv.org/abs/2211.11542)
  - [ ]  [SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation](https://ar5iv.org/abs/2212.04493)<br>:house:[project](https://yccyenchicheng.github.io/SDFusion/)
  - [ ]  [Reconstructing Animatable Categories From Videos](https://ar5iv.org/abs/2305.06351)<br>:house:[project](https://gengshan-y.github.io/rac-www/)
  - [ ]  [OReX: Object Reconstruction From Planar Cross-Sections Using Neural Fields](https://ar5iv.org/abs/2211.12886)
  - [ ]  [Learning Articulated Shape with Keypoint Pseudo-labels from Web Images](http://ar5iv.org/abs/2304.14396v1)<br>:star:[code](https://statho.github.io/projects/animals3d/index.html)
  - [ ]  [SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction](https://ar5iv.org/abs/2212.00792)<br>:house:[project](https://sparsefusion.github.io/)
  - [ ]  [3D Shape Reconstruction of Semi-Transparent Worms](https://ar5iv.org/abs/2304.14841)
  - [ ]  [Power Bundle Adjustment for Large-Scale 3D Reconstruction](https://ar5iv.org/abs/2204.12834)
  - [ ]  [PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces](http://ar5iv.org/abs/2305.05594v1)<br>:star:[code](https://github.com/yiqun-wang/PET-NeuS)
  - [ ]  [AutoRecon: Automated 3D Object Discovery and Reconstruction](http://ar5iv.org/abs/2305.08810v1)<br>:star:[code](https://zju3dv.github.io/autorecon)
  - [ ]  [3D Registration with Maximal Cliques](http://ar5iv.org/abs/2305.10854v1)
  - [ ]  [3D shape reconstruction of semi-transparent worms](https://ar5iv.org/abs/2304.14841)
  - [ ]  [VisFusion: Visibility-aware Online 3D Scene Reconstruction from Videos](http://ar5iv.org/abs/2304.10687v1)<br>:star:[code](https://github.com/huiyu-gao/VisFusion)
  - [ ]  [NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering](http://ar5iv.org/abs/2304.10080v1)<br>:house:[project](http://geometrylearning.com/neudf/)
  - [ ]  [ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-based Consistency](http://ar5iv.org/abs/2304.06247v1)<br>:house:[project](https://zixuanh.com/projects/shapeclipper.html)
  - [ ]  [BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects](http://ar5iv.org/abs/2303.14158v1)<br>:star:[code](https://bundlesdf.github.io)
  - [ ]  [PAniC-3D: Stylized Single-view 3D Reconstruction from Portraits of Anime Characters](http://ar5iv.org/abs/2303.14587v1)<br>:star:[code](https://github.com/ShuhongChen/panic3d-anime-reconstruction)
  - [ ]  [Unsupervised 3D Shape Reconstruction by Part Retrieval and Assembly](https://ar5iv.org/abs/2303.01999)
  - [ ]  [MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices](https://ar5iv.org/abs/2303.01932)<br>:house:[project](http://code.active.vision/MobileBrick/)
  - [ ]  [Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container](http://ar5iv.org/abs/2303.13805v1)<br>:star:[code](https://github.com/hirotong/ReNeuS)
  - [ ]  [SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates](http://ar5iv.org/abs/2303.13582v1)<br>:star:[code](https://scade-spacecarving-nerfs.github.io)
  - [ ]  [MACARONS: Mapping And Coverage Anticipation with RGB Online Self-Supervision](https://ar5iv.org/abs/2303.03315)<br>:house:[project](https://imagine.enpc.fr/~guedona/MACARONS/)
  - [ ]  [Scalable, Detailed and Mask-Free Universal Photometric Stereo](http://ar5iv.org/abs/2303.15724v1)<br>:star:[code](https://github.com/satoshi-ikehata/SDM-UniPS-CVPR2023)
  - [ ]  [Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction](https://ar5iv.org/abs/2303.05937)
  - [ ]  [NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images](https://ar5iv.org/abs/2303.07653)<br>:house:[project](https://yunfan1202.github.io/NEF/)
  - [ ]  [Behind the Scenes: Density Fields for Single View Reconstruction](https://ar5iv.org/abs/2301.07668)<br>:house:[project](https://fwmb.github.io/bts/)
  - [ ]  [VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction](https://ar5iv.org/abs/2212.08067)
  - [ ]  Surface Reconstruction(曲面重建)
    - [ ]  [NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction](https://ar5iv.org/abs/2303.02375)
    - [ ]  [Octree Guided Unoriented Surface Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Koneputugodage_Octree_Guided_Unoriented_Surface_Reconstruction_CVPR_2023_paper.pdf)
    - [ ]  [Neuralangelo: High-Fidelity Neural Surface Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Neuralangelo_High-Fidelity_Neural_Surface_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://research.nvidia.com/labs/dir/neuralangelo)
    - [ ]  [Neural Kernel Surface Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.pdf)
    - [ ]  [Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections](https://ar5iv.org/abs/2304.08706)<br>:star:[code](https://github.com/JiaxiongQ/NeuS-HSR)
- [ ]  深度估计
  - [ ]  [Fully Self-Supervised Depth Estimation from Defocus Clue](https://ar5iv.org/abs/2303.10752)<br>:star:[code](https://github.com/Ehzoahis/DEReD)
  - [ ]  [Gated Stereo: Joint Depth Estimation From Gated and Wide-Baseline Active Stereo Cues](https://openaccess.thecvf.com/content/CVPR2023/papers/Walz_Gated_Stereo_Joint_Depth_Estimation_From_Gated_and_Wide-Baseline_Active_CVPR_2023_paper.pdf)<br>:house:[project](https://light.princeton.edu/gatedstereo/)
  - [ ]  [OmniVidar: Omnidirectional Depth Estimation From Multi-Fisheye Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_OmniVidar_Omnidirectional_Depth_Estimation_From_Multi-Fisheye_Images_CVPR_2023_paper.pdf)
  - [ ]  [Learning To Fuse Monocular and Multi-View Cues for Multi-Frame Depth E](https://ar5iv.org/abs/2304.08993)<br>:star:[code](https://github.com/ruili3/dynamic-multiframe-depth)
  - [ ]  [SfM-TTR: Using Structure From Motion for Test-Time Refinement of Single-View Depth Networks](https://openaccess.thecvf.com/content/CVPR2023/papers/Izquierdo_SfM-TTR_Using_Structure_From_Motion_for_Test-Time_Refinement_of_Single-View_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/serizba/SfM-TTR)
  - [ ]  [Shakes on a Plane: Unsupervised Depth Estimation From Unstabilized Photography](https://ar5iv.org/abs/2212.12324)<br>:house:[project](https://light.princeton.edu/publication/soap)
  - [ ]  [Depth Estimation From Camera Image and mmWave Radar Point Cloud](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_Depth_Estimation_From_Camera_Image_and_mmWave_Radar_Point_Cloud_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/nesl/radar-camera-fusion-depth)
  - [ ]  [Deep Depth Estimation From Thermal Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Deep_Depth_Estimation_From_Thermal_Image_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/UkcheolShin/MS2-MultiSpectralStereoDataset)
  - [ ]  [LightedDepth: Video Depth Estimation in Light of Limited Inference View Angles](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_LightedDepth_Video_Depth_Estimation_in_Light_of_Limited_Inference_View_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ShngJZ/LightedDepth)
  - [ ]  [Trap Attention: Monocular Depth Estimation With Manual Traps](https://openaccess.thecvf.com/content/CVPR2023/papers/Ning_Trap_Attention_Monocular_Depth_Estimation_With_Manual_Traps_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ICSResearch/TrapAttention)
  - [ ]  [PlaneDepth: Self-supervised Depth Estimation via Orthogonal Planes](https://ar5iv.org/abs/2210.01612)<br>:star:[code](https://github.com/svip-lab/PlaneDepth)
  - [ ]  [Depth Estimation From Indoor Panoramas With Neural Scene Representation](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Depth_Estimation_From_Indoor_Panoramas_With_Neural_Scene_Representation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/WJ-Chang-42/IndoorPanoDepth)
  - [ ]  [Polarimetric iToF:Measuring High-Fidelity Depth Through Scattering Media](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeon_Polarimetric_iToF_Measuring_High-Fidelity_Depth_Through_Scattering_Media_CVPR_2023_paper.pdf)
  - [ ]  [SCADE: NeRFs from Space Carving With Ambiguity-Aware Depth Estimates](https://ar5iv.org/abs/2303.13582)<br>:star:[code](https://scade-spacecarving-nerfs.github.io/)
  - [ ]  [iDisc: Internal Discretization for Monocular Depth Estimation](http://ar5iv.org/abs/2304.06334v1)<br>:house:[project](http://vis.xyz/pub/idisc)
  - [ ]  [HRDFuse: Monocular 360°Depth Estimation by Collaboratively Learning Holistic-with-Regional Depth Distributions](https://ar5iv.org/abs/2303.11616)<br>:house:[project](https://haoai-1997.github.io/HRDFuse/)
  - [ ]  [Learning to Fuse Monocular and Multi-view Cues for Multi-frame Depth Estimation in Dynamic Scenes](http://ar5iv.org/abs/2304.08993v1)<br>:star:[code](https://github.com/ruili3/dynamic-multiframe-depth)
  - [ ]  [Temporally Consistent Online Depth Estimation Using Point-Based Fusion](http://ar5iv.org/abs/2304.07435v1)<br>:house:[project](https://research.facebook.com/publications/temporally-consistent-online-depth-estimation-using-point-based-fusion/)
  - [ ]  [DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium](http://ar5iv.org/abs/2304.03560v1)<br>:star:[code](https://antabangun.github.io/projects/DualRefine/)<br>:star:[code](https://github.com/antabangun/DualRefine)
  - [ ]  [Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation](https://ar5iv.org/abs/2211.13202)<br>:star:[code](https://github.com/noahzn/Lite-Mono)<br>:thumbsup:[CVPR2023 | 轻量高效的自监督深度估计框架Lite-Mono](https://zhuanlan.zhihu.com/p/616672642)
- [ ]  深度补全
  - [ ]  [CompletionFormer: Depth Completion with Convolutions and Vision Transformers](http://ar5iv.org/abs/2304.13030v1)<br>:star:[code](https://github.com/youmi-zym/CompletionFormer)<br>:star:[code](https://youmi-zym.github.io/projects/CompletionFormer/)
  - [ ]  [BEV@DC: Bird’s-Eye View Assisted Training for Depth Completion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_BEVDC_Birds-Eye_View_Assisted_Training_for_Depth_Completion_CVPR_2023_paper.pdf)
- [ ]  室内场景重建
  - [ ]  [I2-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs](https://ar5iv.org/abs/2303.07634)<br>:house:[project](https://jingsenzhu.github.io/i2-sdf)
  - [ ]  [SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Reconstruction of Indoor Scenes](http://ar5iv.org/abs/2304.08971v1)
  - [ ]  [U2RLE: Uncertainty-Guided 2-Stage Room Layout Estimation](http://ar5iv.org/abs/2304.08580v1)
- [ ]  场景重建
  - [ ]  [Neural Fields meet Explicit Geometric Representation for Inverse Rendering of Urban Scenes](http://ar5iv.org/abs/2304.03266v1)<br>:star:[code](https://nv-tlabs.github.io/fegr/)
  - [ ]  [Fast Monocular Scene Reconstruction With Global-Sparse Local-Dense Grids](https://ar5iv.org/abs/2305.13220)
  - [ ]  [BUOL: A Bottom-Up Framework With Occupancy-Aware Lifting for Panoptic 3D Scene Reconstruction From a Single Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_BUOL_A_Bottom-Up_Framework_With_Occupancy-Aware_Lifting_for_Panoptic_3D_CVPR_2023_paper.pdf)
- [ ]  3D场景生成
  - [ ]  [Patch-Based 3D Natural Scene Generation From a Single Example](https://ar5iv.org/abs/2304.12670)<br>:house:[project](http://weiyuli.xyz/Sin3DGen/)
  - [ ]  [Diffusion-Based Generation, Optimization, and Planning in 3D Scenes](http://ar5iv.org/abs/2301.06015)
  - [ ]  [MIME: Human-Aware 3D Scene Generation](https://ar5iv.org/abs/2212.04360)<br>:house:[project](https://mime.is.tue.mpg.de/)
- [ ]  MVS
  - [ ]  [Multi-View Stereo Representation Revist: Region-Aware MVSNet](http://ar5iv.org/abs/2304.13614v1)
  - [ ]  [Adaptive Patch Deformation for Textureless-Resilient Multi-View Stereo](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adaptive_Patch_Deformation_for_Textureless-Resilient_Multi-View_Stereo_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/whoiszzj/APD-MVS)
  - [ ]  [RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo](https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_RIAV-MVS_Recurrent-Indexing_an_Asymmetric_Volume_for_Multi-View_Stereo_CVPR_2023_paper.pdf)
  - [ ]  [GeoMVSNet: Learning Multi-View Stereo with Geometry Perception](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_GeoMVSNet_Learning_Multi-View_Stereo_With_Geometry_Perception_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/doubleZ0108/GeoMVSNet)
- [ ]  三维形状分类
  - [ ]  [Robust 3D Shape Classification via Non-Local Graph Attention Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Robust_3D_Shape_Classification_via_Non-Local_Graph_Attention_Network_CVPR_2023_paper.pdf)
- [ ]  三维图像
  - [ ]  [Seeing a Rose in Five Thousand Ways](https://ar5iv.org/abs/2212.04965)
- [ ]  三维形状
  - [ ]  [Conjugate Product Graphs for Globally Optimal 2D-3D Shape Matching](https://openaccess.thecvf.com/content/CVPR2023/papers/Roetzer_Conjugate_Product_Graphs_for_Globally_Optimal_2D-3D_Shape_Matching_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/paul0noah/sm-2D3D)
  - [ ]  [Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures](https://ar5iv.org/abs/2211.07600)<br>:star:[code](https://github.com/eladrich/latent-nerf)
  - [ ]  [Diffusion-SDF: Text-To-Shape via Voxelized Diffusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Diffusion-SDF_Text-To-Shape_via_Voxelized_Diffusion_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ttlmh/Diffusion-SDF)
- [ ]  三维形状生成
  - [ ] [Diffusion-Based Signed Distance Fields for 3D Shape Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Shim_Diffusion-Based_Signed_Distance_Fields_for_3D_Shape_Generation_CVPR_2023_paper.pdf)
  - [ ]  [TAPS3D: Text-Guided 3D Textured Shape Generation From Pseudo Supervision](http://ar5iv.org/abs/2303.13273)
- [ ]  三维形状重建
  - [ ]  [Teleidoscopic Imaging System for Microscale 3D Shape Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Kawahara_Teleidoscopic_Imaging_System_for_Microscale_3D_Shape_Reconstruction_CVPR_2023_paper.pdf)
  - [ ]  [What You Can Reconstruct From a Shadow](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_What_You_Can_Reconstruct_From_a_Shadow_CVPR_2023_paper.pdf)
- [ ]  3D动画
  - [ ]  [RaBit: Parametric Modeling of 3D Biped Cartoon Characters With a Topological-Consistent Dataset](https://ar5iv.org/abs/2303.12564)<br>:house:[project](https://gaplab.cuhk.edu.cn/projects/RaBit/)
  - [ ]  [MagicPony: Learning Articulated 3D Animals in the Wild](http://ar5iv.org/abs/2211.12497)
- [ ]  室内布局
  - [ ]  [Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness](https://ar5iv.org/abs/2303.00971)<br>:star:[code](https://github.com/zhijieshen-bjtu/DOPNet)
- [ ]  视频重建
  - [ ]  [Learning Event Guided High Dynamic Range Video Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Learning_Event_Guided_High_Dynamic_Range_Video_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://yixinyang-00.github.io/HDRev/)

<a name="9"/>

## 9.Human Pose Estimation(人体姿态估计)
- [ ]  手势
  - [ ]  [A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image](http://ar5iv.org/abs/2304.03635v1)<br>:star:[code](https://github.com/ChanglongJiangGit/A2J-Transformer)3D交互手势姿势估计
  - [ ]  [Neural Voting Field for Camera-Space 3D Hand Pose Estimation](http://ar5iv.org/abs/2305.04328v1)
  - [ ]  [AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation](http://ar5iv.org/abs/2304.12301v1)<br>:star:[code](https://assemblyhands.github.io/)
  - [ ]  [Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos](https://ar5iv.org/abs/2209.09484)<br>:house:[project](https://fylwen.github.io/htt.html)
  - [ ]  [Cross-Domain 3D Hand Pose Estimation with Dual Modalities](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Cross-Domain_3D_Hand_Pose_Estimation_With_Dual_Modalities_CVPR_2023_paper.pdf)
  - [ ]  音频驱动的联合语音手势生成
    - [ ]  [Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation](https://ar5iv.org/abs/2303.09119)<br>:star:[code](https://github.com/Advocate99/DiffGesture)
    - [ ]  [QPGesture: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation](http://ar5iv.org/abs/2305.11094v1)<br>:star:[code](https://github.com/YoungSeng/QPGesture)
  - [ ]  手势合成
    - [ ]  [Co-Speech Gesture Synthesis by Reinforcement Learning With Contrastive Pre-Trained Rewards](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Co-Speech_Gesture_Synthesis_by_Reinforcement_Learning_With_Contrastive_Pre-Trained_Rewards_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/RLracer/RACER.git)
    - [ ]  3D手势合成
      - [ ]  [Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement](https://ar5iv.org/abs/2303.01765)
  - [ ]  手部重建
    - [ ]  [ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction](https://ar5iv.org/abs/2303.05938)<br>:star:[code](https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction)
    - [ ]  [High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition](https://openaccess.thecvf.com/content/CVPR2023/papers/Luan_High_Fidelity_3D_Hand_Shape_Reconstruction_via_Scalable_Graph_Frequency_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tyluann/FreqHand)
    - [ ]  [ACR: Attention Collaboration-Based Regressor for Arbitrary Two-Hand Reconstruction](https://ar5iv.org/abs/2303.05938)<br>:star:[code](https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction)
    - [ ]  [HARP: Personalized Hand Reconstruction From a Monocular RGB Video](https://ar5iv.org/abs/2212.09530)<br>:house:[project](https://korrawe.github.io/harp-project/)
    - [ ]  [Overcoming the Trade-off Between Accuracy and Plausibility in 3D Hand Shape Reconstruction](https://ar5iv.org/abs/2305.00646)
    - [ ]  [A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image](https://ar5iv.org/abs/2304.14299)
    - [ ]  [gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction](http://ar5iv.org/abs/2304.11970v1)<br>:star:[code](https://zerchen.github.io/projects/gsdf.html)
    - [ ]  [MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction](http://ar5iv.org/abs/2303.15718)
    - [ ]  [POEM: Reconstructing Hand in a Point Embedded Multi-view Stereo](http://ar5iv.org/abs/2304.04038v1)<br>:star:[code](https://github.com/lixiny/POEM)
    - [ ]  [Handy: Towards a high fidelity 3D hand shape and appearance model](https://openaccess.thecvf.com/content/CVPR2023/papers/Potamias_Handy_Towards_a_High_Fidelity_3D_Hand_Shape_and_Appearance_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/rolpotamias/handy)
  - [ ]  3D手部恢复
    - [ ]  [Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild](http://ar5iv.org/abs/2303.13652v1)<br>:star:[code](https://github.com/facebookresearch/InterWild)
    - [ ]  [Recovering 3D Hand Mesh Sequence from a Single Blurry Image: A New Dataset and Temporal Unfolding](http://ar5iv.org/abs/2303.15417v1)<br>:star:[code](https://github.com/JaehaKim97/BlurHand_RELEASE)
    - [ ]  [Semi-Supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination](https://ar5iv.org/abs/2303.06380)<br>:house:[project](https://www.yangangwang.com/)
    - [ ]  [H2ONet: Hand-Occlusion-and-Orientation-Aware Network for Real-Time 3D Hand Mesh Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_H2ONet_Hand-Occlusion-and-Orientation-Aware_Network_for_Real-Time_3D_Hand_Mesh_Reconstruction_CVPR_2023_paper.pdf)
  - [ ]  手物姿态估计
    - [ ]  [Harmonious Feature Learning for Interactive Hand-Object Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Harmonious_Feature_Learning_for_Interactive_Hand-Object_Pose_Estimation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/lzfff12/HFL-Net)
  - [ ]  3D手势预测
    - [ ]  [Diverse 3D Hand Gesture Prediction From Body Dynamics by Bilateral Hand Disentanglement](https://ar5iv.org/abs/2303.01765)
- [ ]  人体
  - [ ]  HPE  
    - [ ]  [DistilPose: Tokenized Pose Regression with Heatmap Distillation](https://ar5iv.org/abs/2303.02455)
    - [ ]  [Towards Stable Human Pose Estimation via Cross-View Fusion and Foot Stabilization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuo_Towards_Stable_Human_Pose_Estimation_via_Cross-View_Fusion_and_Foot_CVPR_2023_paper.pdf)
    - [ ]  [Human Pose As Compositional Tokens](https://ar5iv.org/abs/2303.11638)<br>:star:[code](https://github.com/Gengzigang/PCT)
    - [ ]  [Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency Pseudo Label Correction Module](https://ar5iv.org/abs/2303.04346)<br>:star:[code](https://github.com/hlz0606/SSPCM)
    - [ ]  [TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_TokenHPE_Learning_Orientation_Tokens_for_Efficient_Head_Pose_Estimation_via_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zc2023/TokenHPE)
    - [ ]  [A Characteristic Function-Based Method for Bottom-Up Human Pose Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_A_Characteristic_Function-Based_Method_for_Bottom-Up_Human_Pose_Estimation_CVPR_2023_paper.pdf)
    - [ ]  [Analyzing and Diagnosing Pose Estimation With Attributions](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Analyzing_and_Diagnosing_Pose_Estimation_With_Attributions_CVPR_2023_paper.pdf)<br>:house:[project](https://qy-h00.github.io/poseig/)
    - [ ]  [PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation](https://ar5iv.org/abs/2303.07337)<br>:star:[code](https://github.com/qihao067/PoseExaminer)
    - [ ]  [Human Pose as Compositional Tokens](https://ar5iv.org/abs/2303.11638)<br>:star:[code](https://github.com/Gengzigang/PCT)
    - [ ]  [Unified Pose Sequence Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Foo_Unified_Pose_Sequence_Modeling_CVPR_2023_paper.pdf)
    - [ ]  [Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video](https://ar5iv.org/abs/2303.08475)
    - [ ]  [Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation](https://ar5iv.org/abs/2303.11180)
    - [ ]  [HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation](http://ar5iv.org/abs/2305.06968v1)<br>:star:[code](https://github.com/akashsengupta1997/HuManiFlow)
    - [ ]  [Human Pose Estimation in Extremely Low-Light Conditions](http://ar5iv.org/abs/2303.15410v1)
  - [ ]  3D HPE
    - [ ]  [PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with Progressive Video Transformers](https://ar5iv.org/abs/2303.09187)
    - [ ]  [PLIKS: A Pseudo-Linear Inverse Kinematic Solver for 3D Human Body Estimation](https://ar5iv.org/abs/2211.11734)
    - [ ]  [NIKI: Neural Inverse Kinematics With Invertible Neural Networks for 3D Human Pose and Shape Estimation](https://ar5iv.org/abs/2305.08590)<br>:star:[code](https://github.com/Jeff-sjtu/NIKI)
    - [ ]  [DiffPose: Toward More Reliable 3D Pose Estimation](https://ar5iv.org/abs/2211.16940)<br>:house:[project](https://gongjia0208.github.io/Diffpose/)
    - [ ]  [Scene-Aware Egocentric 3D Human Pose Estimation](https://ar5iv.org/abs/2212.11684)
    - [ ]  [Self-Supervised 3D Keypoint Discovery From Multi-View Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_BKinD-3D_Self-Supervised_3D_Keypoint_Discovery_From_Multi-View_Videos_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/b-kind/3d)
    - [ ]  [Global-to-Local Modeling for Video-Based 3D Human Pose and Shape Estimation](https://ar5iv.org/abs/2303.14747)<br>:star:[code](https://github.com/sxl142/GLoT)
    - [ ]  [3D Human Pose Estimation With Spatio-Temporal Criss-Cross Attention](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf)
    - [ ]  [Ego-Body Pose Estimation via Ego-Head Pose Estimation](https://ar5iv.org/abs/2212.04636)<br>获奖论文候选
    - [ ]  [Listening Human Behavior: 3D Human Pose Estimation With Acoustic Signals](https://openaccess.thecvf.com/content/CVPR2023/papers/Shibata_Listening_Human_Behavior_3D_Human_Pose_Estimation_With_Acoustic_Signals_CVPR_2023_paper.pdf)
    - [ ]  [NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation](http://ar5iv.org/abs/2305.08590v1)<br>:star:[code](https://github.com/Jeff-sjtu/NIKI)
    - [ ]  [GFPose: Learning 3D Human Pose Prior With Gradient Fields](https://ar5iv.org/abs/2212.08641)<br>:house:[project](https://sites.google.com/view/gfpose/)
    - [ ]  [PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation](http://ar5iv.org/abs/2303.17472v1)<br>:star:[code](https://qitaozhao.github.io/PoseFormerV2)<br>:star:[code](https://github.com/QitaoZhao/PoseFormerV2)
    - [ ]  [3D Human Pose Estimation via Intuitive Physics](http://ar5iv.org/abs/2303.18246v1)<br>:house:[project](https://ipman.is.tue.mpg.de)
    - [ ]  3D 人体关键点估计
      - [ ]  [3D Human Keypoints Estimation From Point Clouds in the Wild Without Human Labels](https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_3D_Human_Keypoints_Estimation_From_Point_Clouds_in_the_Wild_CVPR_2023_paper.pdf)
  - [ ]  4D HPE
    - [ ]  [SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments](https://ar5iv.org/abs/2303.09095)<br>:star:[code](http://www.lidarhumanmotion.net/sloper4d/) 
  - [ ]  网格恢复
    - [ ]  [POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery](http://ar5iv.org/abs/2303.13357v1)<br>:star:[code](https://zczcwh.github.io/potter_page) 
    - [ ]  [Deformable Mesh Transformer for 3D Human Mesh Recovery](https://openaccess.thecvf.com/content/CVPR2023/papers/Yoshiyasu_Deformable_Mesh_Transformer_for_3D_Human_Mesh_Recovery_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/yusukey03012/DeFormer)
    - [ ]  [One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer](https://ar5iv.org/abs/2303.16160)<br>:house:[project](https://osx-ubody.github.io/)
    - [ ]  [Learning Human Mesh Recovery in 3D Scenes](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Learning_Human_Mesh_Recovery_in_3D_Scenes_CVPR_2023_paper.pdf)<br>:star:[code](https://zju3dv.github.io/sahmr/)
    - [ ]  [One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer](http://ar5iv.org/abs/2303.16160v1)<br>:star:[code](https://osx-ubody.github.io/)<br>:thumbsup:[CVPR2023 IDEA与清华提出首个一阶段3D全身人体网格重建算法OSX](https://mp.weixin.qq.com/s/vAhPl4PJqR6LWWq-K28GQA)
    - [ ]  [Learning Analytical Posterior Probability for Human Mesh Recovery](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_Learning_Analytical_Posterior_Probability_for_Human_Mesh_Recovery_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/NetEase-GameAI/ProPose)
    - [ ]  [Implicit 3D Human Mesh Recovery Using Consistency With Pose and Shape From Unseen-View](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Implicit_3D_Human_Mesh_Recovery_Using_Consistency_With_Pose_and_CVPR_2023_paper.pdf)
  - [ ]  三维人体网格估计
    - [ ]  [3D Human Mesh Estimation from Virtual Markers](https://ar5iv.org/abs/2303.11726)<br>:star:[code](https://github.com/ShirleyMaxx/VirtualMarker) 
  - [ ]  三维人体网格重建
    - [ ]  [Sampling is Matter: Point-guided 3D Human Mesh Reconstruction](http://ar5iv.org/abs/2304.09502v1)<br>:star:[code](https://github.com/DCVL-3D/PointHMR_release)
  - [ ]  3D人体重建
    - [ ]  [High-fidelity 3D Human Digitization from Single 2K Resolution Images](http://ar5iv.org/abs/2303.15108v1)<br>:star:[code](https://github.com/SangHunHan92/2K2K)
    - [ ]  [Crowd3D: Towards Hundreds of People Reconstruction From a Single Image](http://ar5iv.org/abs/2301.09376)
    - [ ]  [PersonNeRF: Personalized Reconstruction From Photo Collections](https://ar5iv.org/abs/2302.08504)<br>:house:[project](https://grail.cs.washington.edu/projects/personnerf/)
    - [ ]  [NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_NeMo_Learning_3D_Neural_Motion_Fields_From_Multiple_Video_Instances_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/nemo-neural-motion-field)
    - [ ]  [FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER](https://ar5iv.org/abs/2205.15448)<br>:house:[project](https://zczcwh.github.io/feater_page/)
    - [ ]  [CloSET: Modeling Clothed Humans on Continuous Surface With Explicit Template Decomposition](https://ar5iv.org/abs/2304.03167)<br>:star:[code](https://www.liuyebin.com/closet)
    - [ ]  [Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting](http://ar5iv.org/abs/2304.11900v1)
    - [ ]  [FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER](https://ar5iv.org/pdf/2205.15448.pdf)<br>:house:[project](https://zczcwh.github.io/feater_page/)
    - [ ]  [Learning Semantic-Aware Disentangled Representation for Flexible 3D Human Body Editing](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Learning_Semantic-Aware_Disentangled_Representation_for_Flexible_3D_Human_Body_Editing_CVPR_2023_paper.pdf)<br>:house:[project](http://cic.tju.edu.cn/faculty/likun/projects/SemanticHuman)
    - [ ]  [Complete 3D Human Reconstruction From a Single Incomplete Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Complete_3D_Human_Reconstruction_From_a_Single_Incomplete_Image_CVPR_2023_paper.pdf)
    - [ ]  [High-Fidelity 3D Human Digitization From Single 2K Resolution Images](https://ar5iv.org/abs/2303.15108)<br>:star:[code](https://github.com/SangHunHan92/2K2K)
    - [ ]  [BAAM: Monocular 3D Pose and Shape Reconstruction With Bi-Contextual Attention Module and Attention-Guided Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_BAAM_Monocular_3D_Pose_and_Shape_Reconstruction_With_Bi-Contextual_Attention_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/gywns6287/BAAM)
    - [ ]  [Humans As Light Bulbs: 3D Human Reconstruction From Thermal Reflection](http://ar5iv.org/abs/2305.01652)
    - [ ]  Clothed Human Reconstruction(穿衣人体重建)
      - [ ]  [DIFu: Depth-Guided Implicit Function for Clothed Human Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_DIFu_Depth-Guided_Implicit_Function_for_Clothed_Human_Reconstruction_CVPR_2023_paper.pdf)<br>:house:[project](https://eadcat.github.io/DIFu)
      - [ ]  [REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_REC-MV_REconstructing_3D_Dynamic_Cloth_From_Monocular_Videos_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/GAP-LAB-CUHK-SZ/REC-MV)
      - [ ]  [SeSDF: Self-evolved Signed Distance Field for Implicit 3D Clothed Human Reconstruction](https://ar5iv.org/abs/2304.00359)
  - [ ]  人体形状补全
    - [ ]  [Human Body Shape Completion With Implicit Shape and Flow Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Human_Body_Shape_Completion_With_Implicit_Shape_and_Flow_Learning_CVPR_2023_paper.pdf)  
- [ ]  多人姿态预测
  - [ ]  [Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting](https://ar5iv.org/abs/2303.05095)<br>:star:[code](https://github.com/xiaogangpeng/TBIFormer)
- [ ]  人体解析
  - [ ]  [Semantic Human Parsing via Scalable Semantic Transfer over Multiple Label Domains](http://ar5iv.org/abs/2304.04140v1)<br>:star:[code](https://github.com/yangjie-cv/SST)
- [ ]  姿势迁移
  - [ ]  [Zero-shot Pose Transfer for Unrigged Stylized 3D Characters](https://ar5iv.org/abs/2306.00200)<br>:house:[project](https://jiashunwang.github.io/ZPT/)
- [ ]  Avatar
  - [ ]  [X-Avatar: Expressive Human Avatars](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_X-Avatar_Expressive_Human_Avatars_CVPR_2023_paper.pdf)<br>:house:[project](https://ait.ethz.ch/X-Avatar)
  - [ ]  [Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild via Self-supervised Scene Decomposition](https://ar5iv.org/abs/2302.11566)<br>:house:[project](https://moygcc.github.io/vid2avatar/)


  
<a name="8"/>

## 8.Action Detection(人体动作检测与识别)
- [ ]  [Video Test-Time Adaptation for Action Recognition](http://ar5iv.org/abs/2211.15393)
- [ ]  [A Large-Scale Robustness Analysis of Video Action Recognition Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Schiappa_A_Large-Scale_Robustness_Analysis_of_Video_Action_Recognition_Models_CVPR_2023_paper.pdf)
- [ ]  [How Can Objects Help Action Recognitio](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf)
- [ ]  [MMG-Ego4D: Multimodal Generalization in Egocentric Action Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/MMG)
- [ ]  [Dual-Path Adaptation From Image to Video Transformers](https://ar5iv.org/abs/2303.09857)<br>:star:[code](https://github.com/park-jungin/DualPath)
- [ ]  [Hybrid Active Learning via Deep Clustering for Video Action Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Rana_Hybrid_Active_Learning_via_Deep_Clustering_for_Video_Action_Detection_CVPR_2023_paper.pdf)<br>:house:[project](https://tinyurl.com/hybridclaus)
- [ ]  [Prompt-Guided Zero-Shot Anomaly Action Recognition using Pretrained Deep Skeleton Features](http://ar5iv.org/abs/2303.15167v1)
- [ ]  [Learning Action Changes by Measuring Verb-Adverb Textual Relationships](http://ar5iv.org/abs/2303.15086v1)<br>:star:[code](https://github.com/dmoltisanti/air-cvpr23)
- [ ]  [STMixer: A One-Stage Sparse Action Detector](http://ar5iv.org/abs/2303.15879v1)
- [ ]  [AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation](http://ar5iv.org/abs/2304.01110v1)
- [ ]  [Search-Map-Search: A Frame Selection Paradigm for Action Recognition](http://ar5iv.org/abs/2304.10316v1)
- [ ]  [On the Benefits of 3D Pose and Tracking for Human Action Recognition](http://ar5iv.org/abs/2304.01199v1)<br>:star:[code](https://brjathu.github.io/LART)
- [ ]  [MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition](http://ar5iv.org/abs/2305.07214v1)<br>:star:[code](https://github.com/facebookresearch/MMG_Ego4D)
- [ ]  [SVFormer: Semi-Supervised Video Transformer for Action Recognition](https://ar5iv.org/abs/2211.13222)
- [ ]  基于骨架的动作识别
  - [ ]  [Learning Discriminative Representations for Skeleton Based Action Recognition](https://ar5iv.org/abs/2303.03729)
  - [ ]  [Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition](https://ar5iv.org/abs/2303.10904)<br>:house:[project](https://langlandslin.github.io/projects/ActCLR/)
  - [ ]  [3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition](http://ar5iv.org/abs/2303.14474v1)
  - [ ]  [HaLP: Hallucinating Latent Positives for Skeleton-based Self-Supervised Learning of Actions](http://ar5iv.org/abs/2304.00387v1)<br>:star:[code](https://github.com/anshulbshah/HaLP)
  - [ ]  [Neural Koopman Pooling: Control-Inspired Temporal Dynamics Encoding for Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Neural_Koopman_Pooling_Control-Inspired_Temporal_Dynamics_Encoding_for_Skeleton-Based_Action_CVPR_2023_paper.pdf)
- [ ]  基于关键点的动作识别
  - [ ]  [Unified Keypoint-based Action Recognition Framework via Structured Keypoint Pooling](http://ar5iv.org/abs/2303.15270v1)
- [ ]  时序动作识别
  - [ ]  [TriDet: Temporal Action Detection with Relative Boundary Modeling](https://ar5iv.org/abs/2303.07347)<br>:star:[code](https://github.com/sssste/TriDet)
  - [ ]  [Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Proposal-Based_Multiple_Instance_Learning_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf)<br>:star:[code](github.com/RenHuan1999/CVPR2023_P-MIL)
  - [ ]  [Post-Processing Temporal Action Detection](https://ar5iv.org/abs/2211.14924)<br>:star:[code](https://github.com/sauradip/GAP)
  - [ ]  [Decomposed Cross-modal Distillation for RGB-based Temporal Action Detection](http://ar5iv.org/abs/2303.17285v1)
  - [ ]  [PivoTAL: Prior-Driven Supervision for Weakly-Supervised Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Rizve_PivoTAL_Prior-Driven_Supervision_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf)
- [ ]  开集动作识别
  - [ ]  [Open Set Action Recognition via Multi-Label Evidential Learning](http://ar5iv.org/abs/2303.12698v1) 
  - [ ]  [Enlarging Instance-specific and Class-specific Information for Open-set Action Recognition](http://ar5iv.org/abs/2303.15467v1)<br>:star:[code](https://github.com/Jun-CEN/PSL)
- [ ]  基于MoCap的动作识别
  - [ ]  [STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition](http://ar5iv.org/abs/2303.18177v1)<br>:star:[code](https://github.com/zgzxy001/STMT)
- [ ]  小样本动作识别
  - [ ]  [MoLo: Motion-augmented Long-short Contrastive Learning for Few-shot Action Recognition](http://ar5iv.org/abs/2304.00946v1)<br>:star:[code](https://github.com/alibaba-mmai-research/MoLo)
  - [ ]  [Active Exploration of Multimodal Complementarity for Few-Shot Action Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wanyan_Active_Exploration_of_Multimodal_Complementarity_for_Few-Shot_Action_Recognition_CVPR_2023_paper.pdf)
- [ ]  半监督动作识别
  - [ ]  [TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition](http://ar5iv.org/abs/2303.16268v1)<br>:star:[code](https://github.com/DAVEISHAN/TimeBalance)
- [ ]  时序动作定位
  - [ ]  [Boosting Weakly-Supervised Temporal Action Localization with Text Information](https://ar5iv.org/abs/2305.00607)<br>:star:[code](https://github.com/lgzlIlIlI/Boosting-WTAL)
  - [ ]  [Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks](http://ar5iv.org/abs/2211.06023)
  - [ ]  [Two-Stream Networks for Weakly-Supervised Temporal Action Localization With Semantic-Aware Mechanisms](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Two-Stream_Networks_for_Weakly-Supervised_Temporal_Action_Localization_With_Semantic-Aware_Mechanisms_CVPR_2023_paper.pdf)
  - [ ]  [Cascade Evidential Learning for Open-World Weakly-Supervised Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Cascade_Evidential_Learning_for_Open-World_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf)
  - [ ]  [Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels](http://ar5iv.org/abs/2304.07978)
  - [ ]  [Distilling Vision-Language Pre-Training To Collaborate With Weakly-Supervised Temporal Action Localization](https://ar5iv.org/abs/2212.09335)
  - [ ]  [AdamsFormer for Spatial Action Localization in the Future](https://openaccess.thecvf.com/content/CVPR2023/papers/Chi_AdamsFormer_for_Spatial_Action_Localization_in_the_Future_CVPR_2023_paper.pdf)
  - [ ]  [Re2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Re2TAL_Rewiring_Pretrained_Video_Backbones_for_Reversible_Temporal_Action_Localization_CVPR_2023_paper.pdf)
- [ ]  群组动作质量评估
  - [ ]  [LOGO: A Long-Form Video Dataset for Group Action Quality Assessment](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/shiyi-zh0408/LOGO)
- [ ]  群体动作识别
  - [ ]  [An Actor-Centric Causality Graph for Asynchronous Temporal Inference in Group Activity](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_An_Actor-Centric_Causality_Graph_for_Asynchronous_Temporal_Inference_in_Group_CVPR_2023_paper.pdf)


<a name="7"/>

## 7.Point Cloud(点云)
- [ ]  [FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer](http://ar5iv.org/abs/2301.08739)
- [ ]  [Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent With Learned Distance Functions](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Grad-PU_Arbitrary-Scale_Point_Cloud_Upsampling_via_Gradient_Descent_With_Learned_CVPR_2023_paper.pdf)
- [ ]  [Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning](http://ar5iv.org/abs/2212.05330)
- [ ]  [Unsupervised Inference of Signed Distance Functions From Single Sparse Point Clouds Without Learning Priors](http://ar5iv.org/abs/2303.14505)
- [ ]  [PointVector: A Vector Representation in Point Cloud Analysis](http://ar5iv.org/abs/2205.10528)
- [ ]  [CLIP2: Contrastive Language-Image-Point Pretraining From Real-World Point Cloud Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_CLIP2_Contrastive_Language-Image-Point_Pretraining_From_Real-World_Point_Cloud_Data_CVPR_2023_paper.pdf)
- [ ]  [PointClustering: Unsupervised Point Cloud Pre-Training Using Transformation Invariance in Clustering](https://openaccess.thecvf.com/content/CVPR2023/papers/Long_PointClustering_Unsupervised_Point_Cloud_Pre-Training_Using_Transformation_Invariance_in_Clustering_CVPR_2023_paper.pdf)
- [ ]  [Adversarially Masking Synthetic To Mimic Real: Adaptive Noise Injection for Point Cloud Segmentation Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Adversarially_Masking_Synthetic_To_Mimic_Real_Adaptive_Noise_Injection_for_CVPR_2023_paper.pdf)
- [ ]  [Parts2Words: Learning Joint Embedding of Point Clouds and Texts by Bidirectional Matching Between Parts and Words](http://ar5iv.org/abs/2107.01872)
- [ ]  [Attention-Based Point Cloud Edge Sampling](https://ar5iv.org/abs/2302.14673)
- [ ]  [Meta Architecture for Point Cloud Analysis](https://ar5iv.org/abs/2211.14462)
- [ ]  [Building Rearticulable Models for Arbitrary 3D Objects From 4D Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Building_Rearticulable_Models_for_Arbitrary_3D_Objects_From_4D_Point_CVPR_2023_paper.pdf)<br>:house:[project](https://stevenlsw.github.io/reart/)
- [ ]  [Implicit Surface Contrastive Clustering for LiDAR Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Implicit_Surface_Contrastive_Clustering_for_LiDAR_Point_Clouds_CVPR_2023_paper.pdf)
- [ ]  [Poly-PC: A Polyhedral Network for Multiple Point Cloud Tasks at Once](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Poly-PC_A_Polyhedral_Network_for_Multiple_Point_Cloud_Tasks_at_CVPR_2023_paper.pdf)
- [ ]  [TriVol: Point Cloud Rendering via Triple Volumes](https://ar5iv.org/abs/2303.16485)
- [ ]  [PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations](https://ar5iv.org/abs/2303.16958)
- [ ]  [PointCMP: Contrastive Mask Prediction for Self-supervised Learning on Point Cloud Videos](https://ar5iv.org/abs/2305.04075)
- [ ]  [GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_GrowSP_Unsupervised_Semantic_Segmentation_of_3D_Point_Clouds_CVPR_2023_paper.pdf)
- [ ]  [Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting](https://ar5iv.org/abs/2302.13130)<br>:star:[code](https://github.com/tarashakhurana/4d-occ-forecasting)
- [ ]  [ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.pdf)
- [ ]  [SE-ORNet: Self-Ensembling Orientation-Aware Network fhttpsor Unsupervised Point Cloud Shape Correspondence](https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_SE-ORNet_Self-Ensembling_Orientation-Aware_Network_for_Unsupervised_Point_Cloud_Shape_Correspondence_CVPR_2023_paper.pdf)
- [ ]  [GeoMAE: Masked Geometric Target Prediction for Self-Supervised Point Cloud Pre-Training](https://ar5iv.org/abs/2305.08808)
- [ ]  [Neural Intrinsic Embedding for Non-rigid Point Cloud Matching](https://ar5iv.org/pdf/2303.01038.pdf)
- [ ]  [3D Spatial Multimodal Knowledge Accumulation for Scene Graph Prediction in Point Cloud](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_3D_Spatial_Multimodal_Knowledge_Accumulation_for_Scene_Graph_Prediction_in_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/HHrEtvP/SMKA)
- [ ]  [SHS-Net: Learning Signed Hyper Surfaces for Oriented Normal Estimation of Point Clouds](http://ar5iv.org/abs/2305.05873v1)
- [ ]  [GeoMAE: Masked Geometric Target Prediction for Self-supervised Point Cloud Pre-Training](http://ar5iv.org/abs/2305.08808v1)
- [ ]  [SCPNet: Semantic Scene Completion on Point Cloud](https://ar5iv.org/abs/2303.06884)
- [ ]  [NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds](http://ar5iv.org/abs/2305.03049v1)<br>:star:[code](https://immortalco.github.io/NeuralEditor)
- [ ]  [Rotation-Invariant Transformer for Point Cloud Matching](https://ar5iv.org/abs/2303.08231)
- [ ]  [Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and Continuous Isometry Invariants with no False Negatives and no False Positives](http://ar5iv.org/abs/2303.15385v1)<br>:house:[project](http://kurlin.org/projects/cloud-isometry-spaces/distance-based-invariants.pdf)
- [ ]  [PointCMP: Contrastive Mask Prediction for Self-supervised Learning on Point Cloud Videos](http://ar5iv.org/abs/2305.04075v1)
- [ ]  [VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud](http://ar5iv.org/abs/2303.14408v1)<br>:star:[code](https://github.com/wz7in/CVPR2023-VLSAT)
- [ ]  [Unsupervised Inference of Signed Distance Functions from Single Sparse Point Clouds without Learning Priors](http://ar5iv.org/abs/2303.14505v1)<br>:star:[code](https://github.com/chenchao15/NeuralTPS)
- [ ]  [Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent with Learned Distance Functions](http://ar5iv.org/abs/2304.11846v1)<br>:star:[code](https://github.com/yunhe20/Grad-PU)
- [ ]  [Binarizing Sparse Convolutional Networks for Efficient Point Cloud Analysis](http://ar5iv.org/abs/2303.15493v1)
- [ ]  [Spatiotemporal Self-supervised Learning for Point Clouds in the Wild](https://ar5iv.org/pdf/2303.16235.pdf)<br>:star:[code](https://github.com/YanhaoWu/STSSL)
- [ ]  [NerVE: Neural Volumetric Edges for Parametric Curve Extraction from Point Cloud](http://ar5iv.org/abs/2303.16465v1)<br>:star:[code](https://dongdu3.github.io/projects/2023/NerVE/)
- [ ]  [IterativePFN: True Iterative Point Cloud Filtering](http://ar5iv.org/abs/2304.01529v1)<br>:star:[code](https://github.com/ddsediri/IterativePFN)
- [ ]  [Fast Point Cloud Generation With Straight Flows](https://ar5iv.org/abs/2212.01747)
- [ ]  [GD-MAE: Generative Decoder for MAE Pre-Training on LiDAR Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_GD-MAE_Generative_Decoder_for_MAE_Pre-Training_on_LiDAR_Point_Clouds_CVPR_2023_paper.pdf)
- [ ]  3D点云
  - [ ]  [Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis](https://ar5iv.org/abs/2303.08134)<br>:star:[code](https://github.com/ZrrSkywalker/Point-NN)
  - [ ]  [ToThePoint: Efficient Contrastive Learning of 3D Point Clouds via Recycling](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ToThePoint_Efficient_Contrastive_Learning_of_3D_Point_Clouds_via_Recycling_CVPR_2023_paper.pdf)
  - [ ]  [PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models](https://ar5iv.org/abs/2212.01558)<br>:house:[project](https://colin97.github.io/PartSLIP_page/)
  - [ ]  [Starting From Non-Parametric Networks for 3D Point Cloud Analysis](https://ar5iv.org/abs/2303.08134)<br>:star:[code](https://github.com/ZrrSkywalker/Point-NN)
  - [ ]  [Learnable Skeleton-Aware 3D Point Cloud Sampling](https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_Learnable_Skeleton-Aware_3D_Point_Cloud_Sampling_CVPR_2023_paper.pdf)
  - [ ]  [GraVoS: Voxel Selection for 3D Point-Cloud Detection](https://ar5iv.org/abs/2208.08780)
  - [ ]  [MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_MarS3D_A_Plug-and-Play_Motion-Aware_Model_for_Semantic_Segmentation_on_Multi-Scan_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/CVMI-Lab/MarS3D)
  - [ ]  [NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-linear Interpolation](http://ar5iv.org/abs/2303.15126v1)<br>:star:[code](https://github.com/ispc-lab/NeuralPCI)<br>:star:[code](https://dyfcalid.github.io/NeuralPCI)
  - [ ]  [Rethinking the Approximation Error in 3D Surface Fitting for Point Cloud Normal Estimation](http://ar5iv.org/abs/2303.17167v1)<br>:star:[code](https://github.com/hikvision-research/3DVision)
- [ ]  点云实例分割
  - [ ]  [ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution](https://ar5iv.org/pdf/2303.00246.pdf)
- [ ]  点云分类
  - [ ]  [PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees](https://ar5iv.org/abs/2303.01959)
  - [ ]  [CAP: Robust Point Cloud Classification via Semantic and Structural Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_CAP_Robust_Point_Cloud_Classification_via_Semantic_and_Structural_Modeling_CVPR_2023_paper.pdf)
  - [ ]  [ViewNet: A Novel Projection-Based Backbone With View Pooling for Few-Shot Point Cloud Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ViewNet_A_Novel_Projection-Based_Backbone_With_View_Pooling_for_Few-Shot_CVPR_2023_paper.pdf)
- [ ]  点云补全
  - [ ]  [ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer](https://ar5iv.org/pdf/2302.14435.pdf)<br>:star:[code](https://github.com/I2-Multimedia-Lab/ProxyFormer)
  - [ ]  [Symmetric Shape-Preserving Autoencoder for Unsupervised Real Scene Point Cloud Completion](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Symmetric_Shape-Preserving_Autoencoder_for_Unsupervised_Real_Scene_Point_Cloud_Completion_CVPR_2023_paper.pdf)
  - [ ]  [ACL-SPC: Adaptive Closed-Loop system for Self-Supervised Point Cloud Completion](https://ar5iv.org/abs/2303.01979)<br>:star:[code](https://github.com/Sangminhong/ACL-SPC_PyTorch)
  - [ ]  [AnchorFormer: Point Cloud Completion From Discriminative Nodes](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_AnchorFormer_Point_Cloud_Completion_From_Discriminative_Nodes_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/chenzhik/AnchorFormer)
  - [ ]  [Hyperspherical Embedding for Point Cloud Completion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Hyperspherical_Embedding_for_Point_Cloud_Completion_CVPR_2023_paper.pdf)
- [ ]  点云配准
  - [ ]  [Deep Graph-based Spatial Consistency for Robust Non-rigid Point Cloud Registration](https://ar5iv.org/abs/2303.09950)<br>:star:[code](https://github.com/qinzheng93/GraphSCNet)
  - [ ]  [PEAL: Prior-Embedded Explicit Attention Learning for Low-Overlap Point Cloud Registration](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_PEAL_Prior-Embedded_Explicit_Attention_Learning_for_Low-Overlap_Point_Cloud_Registration_CVPR_2023_paper.pdf)
  - [ ]  [Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration](http://ar5iv.org/abs/2303.13290v1)
  - [ ]  [Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting](http://ar5iv.org/abs/2304.00467v1)<br>:star:[code](https://github.com/WHU-USI3DV/SGHR)
  - [ ]  [BUFFER: Balancing Accuracy, Efficiency, and Generalizability in Point Cloud Registration](https://openaccess.thecvf.com/content/CVPR2023/papers/Ao_BUFFER_Balancing_Accuracy_Efficiency_and_Generalizability_in_Point_Cloud_Registration_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/aosheng1996/BUFFER)
- [ ]  点云理解
  - [ ]  [Self-positioning Point-based Transformer for Point Cloud Understanding](http://ar5iv.org/abs/2303.16450v1)<br>:star:[code](https://github.com/mlvlab/SPoTr) 
- [ ]  点云重建  
  - [ ]  [Learning To Measure the Point Cloud Reconstruction Loss in a Representation Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Learning_To_Measure_the_Point_Cloud_Reconstruction_Loss_in_a_CVPR_2023_paper.pdf)
- [ ]  点云匹配
  - [ ]  [Neural Intrinsic Embedding for Non-Rigid Point Cloud Matching](https://ar5iv.org/abs/2303.01038)
- [ ]  点云分割
  - [ ] [Improving Graph Representation for Point Cloud Segmentation via Attentive Filtering](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Improving_Graph_Representation_for_Point_Cloud_Segmentation_via_Attentive_Filtering_CVPR_2023_paper.pdf)
- [ ]  点云压缩
  - [ ]  [Efficient Hierarchical Entropy Model for Learned Point Cloud Compression](https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Efficient_Hierarchical_Entropy_Model_for_Learned_Point_Cloud_Compression_CVPR_2023_paper.pdf)

<a name="6"/>

## 6.Object Tracking(目标跟踪)
- [ ]  [Data-Driven Feature Tracking for Event Cameras](http://ar5iv.org/abs/2211.12826)
- [ ]  [Autoregressive Visual Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Autoregressive_Visual_Tracking_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/MIV-XJTU/ARTrack)
- [ ]  [Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking](https://ar5iv.org/abs/2303.11791)<br>:house:[project](https://againstentropy.github.io/NLOS-Track/)
- [ ]  [Unifying Short and Long-Term Tracking With Graph Hierarchies](https://openaccess.thecvf.com/content/CVPR2023/papers/Cetintas_Unifying_Short_and_Long-Term_Tracking_With_Graph_Hierarchies_CVPR_2023_paper.pdf)<br>:house:[project](bit.ly/sushi-mot)
- [ ]  [VideoTrack: Learning To Track Objects via Video Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_VideoTrack_Learning_To_Track_Objects_via_Video_Transformer_CVPR_2023_paper.pdf)
- [ ]  [Tracking Through Containers and Occluders in the Wild](https://ar5iv.org/abs/2305.03052)<br>:house:[project](https://tcow.cs.columbia.edu/)
- [ ]  [Frame-Event Alignment and Fusion Network for High Frame Rate Tracking](https://ar5iv.org/abs/2305.15688)
- [ ]  [Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking](https://ar5iv.org/abs/2303.11791)<br>:star:[code](https://againstentropy.github.io/NLOS-Track/)
- [ ]  [Joint Visual Grounding and Tracking with Natural Language Specification](https://ar5iv.org/abs/2303.12027)<br>:star:[code](https://github.com/lizhou-cs/JointNLT)
- [ ]  [Generalized Relation Modeling for Transformer Tracking](http://ar5iv.org/abs/2303.16580v1)<br>:star:[code](https://github.com/Little-Podi/GRM)
- [ ]  [SeqTrack: Sequence to Sequence Learning for Visual Object Tracking](http://ar5iv.org/abs/2304.14394v1)
- [ ]  [Tracking through Containers and Occluders in the Wild](http://ar5iv.org/abs/2305.03052v1)<br>:house:[project](https://tcow.cs.columbia.edu/)
- [ ]  [DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks](http://ar5iv.org/abs/2304.00571v1)<br>:star:[code](https://github.com/jimmy-dq/DropMAE.git)
- [ ]  [CXTrack: Improving 3D Point Cloud Tracking With Contextual Information](https://ar5iv.org/abs/2211.08542)
- [ ]  [Representation Learning for Visual Object Tracking by Masked Appearance Transfer](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Representation_Learning_for_Visual_Object_Tracking_by_Masked_Appearance_Transfer_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/difhnp/MAT)
- [ ]  [3D-POP - An Automated Annotation Approach to Facilitate Markerless 2D-3D Tracking of Freely Moving Birds With Marker-Based Motion Capture](https://openaccess.thecvf.com/content/CVPR2023/papers/Naik_3D-POP_-_An_Automated_Annotation_Approach_to_Facilitate_Markerless_2D-3D_CVPR_2023_paper.pdf)
- [ ]  多目标跟踪
  - [ ]  [Referring Multi-Object Tracking](https://ar5iv.org/abs/2303.03366)<br>:star:[code](https://github.com/wudongming97/RMOT)
  - [ ]  [Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking](http://ar5iv.org/abs/2203.14360)
  - [ ]  [Simple Cues Lead to a Strong Multi-Object Tracker](https://openaccess.thecvf.com/content/CVPR2023/papers/Seidenschwarz_Simple_Cues_Lead_to_a_Strong_Multi-Object_Tracker_CVPR_2023_paper.pdf)
  - [ ]  [Tracking Multiple Deformable Objects in Egocentric Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tracking_Multiple_Deformable_Objects_in_Egocentric_Videos_CVPR_2023_paper.pdf)<br>:house:[project](https://mingzhenhuang.com/projects/detracker.html)
  - [ ]  [MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors](https://ar5iv.org/abs/2211.09791)<br>:star:[code](https://github.com/megvii-research/MOTRv2)
  - [ ]  [UTM: A Unified Multiple Object Tracking Model With Identity-Aware Feature Enhancement](https://openaccess.thecvf.com/content/CVPR2023/papers/You_UTM_A_Unified_Multiple_Object_Tracking_Model_With_Identity-Aware_Feature_CVPR_2023_paper.pdf)
  - [ ]  [Focus on Details: Online Multi-Object Tracking With Diverse Fine-Grained Representation](https://ar5iv.org/abs/2302.14589)
  - [ ]  [Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking](https://ar5iv.org/abs/2302.03802)<br>:star:[code](https://github.com/TRI-ML/PF-Track)
  - [ ]  [MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking](https://ar5iv.org/abs/2303.10404)
  - [ ]  [OVTrack: Open-Vocabulary Multiple Object Tracking](http://ar5iv.org/abs/2304.08408v1)<br>:house:[project](https://www.vis.xyz/pub/ovtrack/)
- [ ]  多模态跟踪
  - [ ]  [Visual Prompt Multi-Modal Tracking](https://ar5iv.org/abs/2303.10826)<br>:star:[code](https://github.com/jiawen-zhu/ViPT)
- [ ]  RGB-T tracking(可见光图像（RGB）和热红外图像（T）结合起来进行目标追踪)
  - [ ]  [Bridging Search Region Interaction With Template for RGB-T Tracking](https://openaccess.thecvf.com/content/CVPR2023/papers/Hui_Bridging_Search_Region_Interaction_With_Template_for_RGB-T_Tracking_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/RyanHTR/TBSI)
  - [ ]  [Efficient RGB-T Tracking via Cross-Modality Distillation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Efficient_RGB-T_Tracking_via_Cross-Modality_Distillation_CVPR_2023_paper.pdf)

<a name="5"/>

## 5.Object Detection(目标检测)
- [ ]  [Angelic Patches for Improving Third-Party Object Detector Performance](https://openaccess.thecvf.com/content/CVPR2023/papers/Si_Angelic_Patches_for_Improving_Third-Party_Object_Detector_Performance_CVPR_2023_paper.pdf)
- [ ]  [STDLens: Model Hijacking-Resilient Federated Learning for Object Detection](http://ar5iv.org/abs/2303.11511)
- [ ]  [Enhanced Training of Query-Based Object Detection via Selective Query Recollection](http://ar5iv.org/abs/2212.07593)
- [ ]  [The Differentiable Lens: Compound Lens Search Over Glass Surfaces and Materials for Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Cote_The_Differentiable_Lens_Compound_Lens_Search_Over_Glass_Surfaces_and_CVPR_2023_paper.pdf)
- [ ]  [Multi-view Adversarial Discriminator: Mine the Non-causal Factors for Object Detection in Unseen Domains](https://ar5iv.org/abs/2304.02950)<br>:star:[code](https://github.com/K2OKOH/MAD)
- [ ]  [Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding](https://ar5iv.org/abs/2206.03484)
- [ ]  [NeRF-RPN: A General Framework for Object Detection in NeRFs](https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_NeRF-RPN_A_General_Framework_for_Object_Detection_in_NeRFs_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/lyclyc52/NeRF_RPN)
- [ ]  [Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors](https://ar5iv.org/abs/2208.11356)
- [ ]  [Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration](https://openaccess.thecvf.com/content/CVPR2023/papers/Oksuz_Towards_Building_Self-Aware_Object_Detectors_via_Reliable_Uncertainty_Quantification_and_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/fiveai/saod)
- [ ]  [Gaussian Label Distribution Learning for Spherical Image Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Gaussian_Label_Distribution_Learning_for_Spherical_Image_Object_Detection_CVPR_2023_paper.pdf)
- [ ]  [Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments](https://ar5iv.org/abs/2210.16046)
- [ ]  [Towards Unsupervised Object Detection From LiDAR Point Clouds](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Unsupervised_Object_Detection_From_LiDAR_Point_Clouds_CVPR_2023_paper.pdf)<br>:house:[project](https://waabi.ai/research/oyster)
- [ ]  [Mask DINO: Towards a Unified Transformer-Based Framework for Object Detection and Segmentation](https://ar5iv.org/abs/2206.02777)<br>:star:[code](https://github.com/IDEACVR/MaskDINO)
- [ ]  [T-SEA: Transfer-Based Self-Ensemble Attack on Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_T-SEA_Transfer-Based_Self-Ensemble_Attack_on_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/VDIGPKU/T-SEA)
- [ ]  [Recurrent Vision Transformers for Object Detection With Event Cameras](https://ar5iv.org/abs/2212.05598)
- [ ]  [Learned Two-Plane Perspective Prior Based Image Resampling for Efficient Object Detection](https://ar5iv.org/abs/2303.14311)
- [ ]  [Normalizing Flow Based Feature Synthesis for Outlier-Aware Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_Normalizing_Flow_Based_Feature_Synthesis_for_Outlier-Aware_Object_Detection_CVPR_2023_paper.pdf)
- [ ]  [YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors](https://ar5iv.org/abs/2207.02696)<br>:star:[code](https://github.com/WongKinYiu/yolov7)
- [ ]  [MetaFusion: Infrared and Visible Image Fusion via Meta-Feature Embedding From Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_MetaFusion_Infrared_and_Visible_Image_Fusion_via_Meta-Feature_Embedding_From_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/wdzhao123/MetaFusion)
- [ ]  [Doubly Right Object Recognition: A Why Prompt for Visual Rationales](https://ar5iv.org/abs/2212.06202)
- [ ]  [Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection](https://ar5iv.org/abs/2211.06368)<br>:star:[code](https://github.com/open-mmlab/mmrotate)
- [ ]  [Unbalanced Optimal Transport: A Unified Framework for Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/De_Plaen_Unbalanced_Optimal_Transport_A_Unified_Framework_for_Object_Detection_CVPR_2023_paper.pdf)
- [ ]  [CLIP the Gap: A Single Domain Generalization Approach for Object Detection](https://ar5iv.org/abs/2301.05499)
- [ ]  [Learning Transformations To Reduce the Geometric Shift in Object Detection](https://ar5iv.org/abs/2301.05496)
- [ ]  [Object Detection With Self-Supervised Scene Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Object_Detection_With_Self-Supervised_Scene_Adaptation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/cvlab-stonybrook/scenes100)
- [ ]  [Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR](https://ar5iv.org/abs/2303.07335)<br>:star:[code](https://github.com/IDEA-Research/Lite-DETR)
- [ ]  [SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SAP-DETR_Bridging_the_Gap_Between_Salient_Points_and_Queries-Based_Transformer_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/liuyang-ict/SAP-DETR)
- [ ]  [Multiclass Confidence and Localization Calibration for Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Pathiraja_Multiclass_Confidence_and_Localization_Calibration_for_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/bimsarapathiraja/MCCL)
- [ ]  [Mobile User Interface Element Detection Via Adaptively Prompt Tuning](https://ar5iv.org/abs/2305.09699)
- [ ]  [DynamicDet: A Unified Dynamic Architecture for Object Detection](http://ar5iv.org/abs/2304.05552v1)<br>:star:[code](https://github.com/VDIGPKU/DynamicDet) 
- [ ]  [ZBS: Zero-shot Background Subtraction via Instance-level Background Modeling and Foreground Selection](http://ar5iv.org/abs/2303.14679v1)<br>:star:[code](https://github.com/CASIA-IVA-Lab/ZBS)
- [ ]  [Curricular Object Manipulation in LiDAR-based Object Detection](http://ar5iv.org/abs/2304.04248v1)<br>:star:[code](https://github.com/ZZY816/COM)
- [ ]  [STDLens: Model Hijacking-resilient Federated Learning for Object Detection](https://ar5iv.org/abs/2303.11511)<br>:star:[code](https://github.com/git-disl/STDLens)
- [ ]  [What Can Human Sketches Do for Object Detection?](http://ar5iv.org/abs/2303.15149v1)<br>:star:[code](https://pinakinathc.github.io/sketch-detect)
- [ ]  [Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects](http://ar5iv.org/abs/2303.13769v1)<br>:star:[code](https://github.com/Went-Liang/UnSniffer)
- [ ]  [Bridging Precision and Confidence: A Train-Time Loss for Calibrating Object Detection](http://ar5iv.org/abs/2303.14404v1)<br>:star:[code](https://github.com/akhtarvision/bpc_calibration)
- [ ]  [Learned Two-Plane Perspective Prior based Image Resampling for Efficient Object Detection](http://ar5iv.org/abs/2303.14311v1)
- [ ]  [T-SEA: Transfer-based Self-Ensemble Attack on Object Detection](https://ar5iv.org/pdf/2211.09773.pdf)<br>:star:[code](https://github.com/VDIGPKU/T-SEA)<br>:thumbsup:[CVPR 2023 | 北大提出T-SEA: 自集成策略实现更强的黑盒攻击迁移性](https://mp.weixin.qq.com/s/UPFnuHwHe1YqNOYCcHQ1rQ)
- [ ]  [Knowledge Combination to Learn Rotated Detection Without Rotated Annotation](http://ar5iv.org/abs/2304.02199v1)
- [ ]  [Universal Instance Perception as Object Discovery and Retrieval](https://ar5iv.org/abs/2303.06674)<br>:star:[code](https://github.com/MasterBin-IIAU/UNINEXT)
- [ ]  [Continual Detection Transformer for Incremental Object Detection](http://ar5iv.org/abs/2304.03110v1)目标检测
- [ ]  [Multi-view Adversarial Discriminator: Mine the Non-causal Factors for Object Detection in Unseen Domains](http://ar5iv.org/abs/2304.02950v1)<br>:star:[code](https://github.com/K2OKOH/MAD")目标检测
- [ ]  开放词汇目标检测
  - [ ]  [Aligning Bag of Regions for Open-Vocabulary Object Detection](https://ar5iv.org/abs/2302.13996)<br>:star:[code](https://github.com/wusize/ovdet)
  - [ ]  [Region-Aware Pretraining for Open-Vocabulary Object Detection With Vision Transformers](https://ar5iv.org/abs/2305.07011)
  - [ ]  [DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-Training via Word-Region Alignment](https://ar5iv.org/abs/2304.04514)
  - [ ]  [OvarNet: Towards Open-vocabulary Object Attribute Recognition](https://ar5iv.org/abs/2301.09506)<br>:thumbsup:[CVPR2023｜小红书提出 OvarNet 模型：开集预测的新SOTA，“万物识别”有了新玩法](https://mp.weixin.qq.com/s/EkYz5FO2PPpK_uVD7i2q-g)
  - [ ]  [Learning To Detect and Segment for Open Vocabulary Object Detection](https://ar5iv.org/abs/2212.12130)
  - [ ]  [Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers](http://ar5iv.org/abs/2305.07011v1)
  - [ ]  [Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection](https://ar5iv.org/abs/2303.05892)<br>:star:[code](https://github.com/LutingWang/OADP)
  - [ ]  [CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching](http://ar5iv.org/abs/2303.13076v1)
  - [ ]  [DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via Word-Region Alignment](http://ar5iv.org/abs/2304.04514v1)
- [ ]  开放世界目标检测
  - [ ]  [Annealing-Based Label-Transfer Learning for Open World Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Annealing-Based_Label-Transfer_Learning_for_Open_World_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/DIG-Beihang/ALLOW.git)
  - [ ]  [CapDet: Unifying Dense Captioning and Open-World Detection Pretraining](https://ar5iv.org/abs/2303.02489)
  - [ ]  [PROB: Probabilistic Objectness for Open World Object Detection](https://ar5iv.org/abs/2212.01424)<br>:star:[code](https://github.com/orrzohar/PROB)
  - [ ]  [CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection](https://ar5iv.org/abs/2301.01970)
  - [ ]  [Detecting Everything in the Open World: Towards Universal Object Detection](https://ar5iv.org/abs/2303.11749)<br>:star:[code](https://github.com/zhenyuw16/UniDetector)<br>:thumbsup:[CVPR 2023 | 标注500类，检测7000类！清华大学等提出通用目标检测算法UniDetector](https://zhuanlan.zhihu.com/p/616328874)
- [ ]  目标定位
  - [ ]  [LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding](https://ar5iv.org/abs/2303.09665)<br>:house:[project](https://reagan1311.github.io/locate/)
  - [ ]  [Egocentric Audio-Visual Object Localization](http://ar5iv.org/abs/2303.13471v1)
  - [ ]  [Unsupervised Object Localization: Observing the Background To Discover Objects](https://openaccess.thecvf.com/content/CVPR2023/papers/Simeoni_Unsupervised_Object_Localization_Observing_the_Background_To_Discover_Objects_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/valeoai/FOUND)
  - [ ]  [NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Min_NeurOCS_Neural_NOCS_Supervision_for_Monocular_3D_Object_Localization_CVPR_2023_paper.pdf)
- [ ]  3D OD
  - [ ]  [Virtual Sparse Convolution for Multimodal 3D Object Detection](https://ar5iv.org/abs/2303.02314)<br>:star:[code](https://github.com/hailanyi/VirConv)
  - [ ]  [Bi3D: Bi-Domain Active Learning for Cross-Domain 3D Object Detection](http://ar5iv.org/abs/2303.05886)
  - [ ]  [MSMDFusion: Fusing LiDAR and Camera at Multiple Scales With Multi-Depth Seeds for 3D Object Detection](http://ar5iv.org/abs/2209.03102)
  - [ ]  [BEVHeight: A Robust Framework for Vision-Based Roadside 3D Object Detection](http://ar5iv.org/abs/2303.08498)
  - [ ]  [UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_UniDistill_A_Universal_Cross-Modality_Knowledge_Distillation_Framework_for_3D_Object_CVPR_2023_paper.pdf)
  - [ ]  [PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D Detection](https://ar5iv.org/abs/2205.11098)<br>:star:[code](https://github.com/RunpeiDong/PointDistiller)
  - [ ]  [AShapeFormer: Semantics-Guided Object-Level Active Shape Encoding for 3D Object Detection via Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_AShapeFormer_Semantics-Guided_Object-Level_Active_Shape_Encoding_for_3D_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ZechuanLi/AShapeFormer)
  - [ ]  [BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks](https://openaccess.thecvf.com/content/CVPR2023/papers/Chi_BEV-SAN_Accurate_BEV_3D_Object_Detection_via_Slice_Attention_Networks_CVPR_2023_paper.pdf)
  - [ ]  [3D Video Object Detection With Learnable Object-Centric Global Optimization](https://ar5iv.org/abs/2303.15416)<br>:star:[code](https://github.com/jiaweihe1996/BA-Det)
  - [ ]  [ConQueR: Query Contrast Voxel-DETR for 3D Object Detection](https://ar5iv.org/abs/2212.07289)<br>:house:[project](https://benjin.me/projects/2022_conquer/)
  - [ ]  [Bi-LRFusion: Bi-Directional LiDAR-Radar Fusion for 3D Dynamic Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Bi-LRFusion_Bi-Directional_LiDAR-Radar_Fusion_for_3D_Dynamic_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/JessieW0806/Bi-LRFusion)
  - [ ]  [Uni3D: A Unified Baseline for Multi-Dataset 3D Object Detection](https://ar5iv.org/abs/2303.06880)<br>:star:[code](https://github.com/PJLab-ADG/3DTrans)
  - [ ]  [Distilling Focal Knowledge From Imperfect Expert for 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/OpenPerceptionX/BEVPerception-Survey-Recipe)
  - [ ]  [Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark](https://ar5iv.org/abs/2212.08914)<br>:star:[code](https://github.com/JeffWang987/ASAP)
  - [ ]  [Deep Dive Into Gradients: Better Optimization for 3D Object Detection With Gradient-Corrected IoU Supervision](https://openaccess.thecvf.com/content/CVPR2023/papers/Ming_Deep_Dive_Into_Gradients_Better_Optimization_for_3D_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ming71/GCIoU-loss)
  - [ ]  [AeDet: Azimuth-invariant Multi-view 3D Object Detection](https://ar5iv.org/abs/2211.12501)<br>:star:[code](https://fcjian.github.io/aedet)
  - [ ]  [FrustumFormer: Adaptive Instance-Aware Resampling for Multi-View 3D Detection](https://ar5iv.org/abs/2301.04467)<br>:star:[code](https://github.com/Robertwyq/Frustum)
  - [ ]  [PVT-SSD: Single-Stage 3D Object Detector With Point-Voxel Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_PVT-SSD_Single-Stage_3D_Object_Detector_With_Point-Voxel_Transformer_CVPR_2023_paper.pdf)
  - [ ]  [itKD: Interchange Transfer-Based Knowledge Distillation for 3D Object Detection](https://ar5iv.org/abs/2205.15531)
  - [ ]  [OcTr: Octree-Based Transformer for 3D Object Detection](https://ar5iv.org/abs/2303.12621)
  - [ ]  [MoDAR: Using Motion Forecasting for 3D Object Detection in Point Cloud Sequences](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MoDAR_Using_Motion_Forecasting_for_3D_Object_Detection_in_Point_CVPR_2023_paper.pdf)
  - [ ]  [Semi-Supervised Stereo-Based 3D Object Detection via Cross-View Consensus](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Semi-Supervised_Stereo-Based_3D_Object_Detection_via_Cross-View_Consensus_CVPR_2023_paper.pdf)
  - [ ]  [LinK: Linear Kernel for LiDAR-based 3D Perception](http://ar5iv.org/abs/2303.16094v1)<br>:star:[code](https://github.com/MCG-NJU/LinK)
  - [ ]  [PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds](http://ar5iv.org/abs/2305.04925v1)
  - [ ]  [PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer](http://ar5iv.org/abs/2305.06621v1)<br>:star:[code](https://github.com/Nightmare-n/PVT-SSD)
  - [ ]  [3D Video Object Detection with Learnable Object-Centric Global Optimization](http://ar5iv.org/abs/2303.15416v1)<br>:star:[code](https://github.com/jiaweihe1996/BA-Det)
  - [ ]  [Density-Insensitive Unsupervised Domain Adaption on 3D Object Detection](http://ar5iv.org/abs/2304.09446v1)<br>:star:[code](https://github.com/WoodwindHu/DTS)
  - [ ]  [X3KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection](https://ar5iv.org/abs/2303.02203)<br>:star:[code](https://youtu.be/1do9DPFmr38)
  - [ ]  [Understanding the Robustness of 3D Object Detection with Bird's-Eye-View Representations in Autonomous Driving](http://ar5iv.org/abs/2303.17297v1)
  - [ ]  [Weakly Supervised Monocular 3D Object Detection using Multi-View Projection and Direction Consistency](https://ar5iv.org/abs/2303.08686)<br>:star:[code](https://github.com/weakmono3d/weakmono3d)
  - [ ]  [Viewpoint Equivariance for Multi-View 3D Object Detection](http://ar5iv.org/abs/2303.14548v1)<br>:star:[code](https://github.com/TRI-ML/VEDet)
  - [ ]  [Benchmarking Robustness of 3D Object Detection to Common Corruptions in Autonomous Driving](https://ar5iv.org/abs/2303.11040)<br>:star:[code](https://github.com/kkkcx/3D_Corruptions_AD)
  - [ ]  [Collaboration Helps Camera Overtake LiDAR in 3D Detection](http://ar5iv.org/abs/2303.13560v1)<br>:star:[code](https://siheng-chen.github.io/dataset/CoPerception+)<br>:star:[code](https://github.com/MediaBrain-SJTU/CoCa3D)
  - [ ]  [OcTr: Octree-based Transformer for 3D Object Detection](http://ar5iv.org/abs/2303.12621v1)
  - [ ]  [MSF: Motion-guided Sequential Fusion for Efficient 3D Object Detection from Point Cloud Sequences](https://ar5iv.org/abs/2303.08316)<br>:star:[code](https://github.com/skyhehe123/MSF)
  - [ ]  [MonoATT: Online Monocular 3D Object Detection with Adaptive Token Transformer](http://ar5iv.org/abs/2303.13018v1)
  - [ ]  [MV-JAR: Masked Voxel Jigsaw and Reconstruction for LiDAR-Based Self-Supervised Pre-Training](http://ar5iv.org/abs/2303.13510v1)<br>:star:[code](https://github.com/SmartBot-PJLab/MV-JAR)
  - [ ]  [NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations](http://ar5iv.org/abs/2303.13483v1)
  - [ ]  [VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking](https://ar5iv.org/abs/2303.11301)<br>:star:[code](https://github.com/dvlab-research/VoxelNeXt)
  - [ ]  [Bi3D: Bi-domain Active Learning for Cross-domain 3D Object Detection](https://ar5iv.org/abs/2303.05886)<br>:star:[code](https://github.com/PJLabADG/3DTrans)
  - [ ]  [LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global Cross-Modal Fusion](https://ar5iv.org/abs/2303.03595)<br>:star:[code](https://github.com/sankin97/LoGoNet)
  - [ ]  [PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection](https://ar5iv.org/abs/2303.08129)<br>:star:[code](https://github.com/BLVLab/PiMAE)
  - [ ]  [CAPE: Camera View Position Embedding for Multi-View 3D Object Detection](https://ar5iv.org/abs/2303.10209)<br>:star:[code](https://github.com/kaixinbear/CAPE)
  - [ ]  [Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection](https://ar5iv.org/abs/2303.06880)<br>:star:[code](https://github.com/PJLab-ADG/3DTrans)
  - [ ]  [Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection](http://ar5iv.org/abs/2304.01464v1)<br>:star:[code](https://github.com/azhuantou/HSSDA)3D目标检测
- [ ]  端到端目标检测
  - [ ]  [Dense Distinct Query for End-to-End Object Detection](http://ar5iv.org/abs/2303.12776v1)<br>:star:[code](https://github.com/jshilong/DDQ)
- [ ]  半监督目标检测
  - [ ]  [Active Teacher for Semi-Supervised Object Detection](https://ar5iv.org/abs/2303.08348)<br>:star:[code](https://github.com/HunterJ-Lin/ActiveTeacher)
  - [ ]  [Semi-DETR: Semi-Supervised Object Detection With Detection Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Semi-DETR_Semi-Supervised_Object_Detection_With_Detection_Transformers_CVPR_2023_paper.pdf)
  - [ ]  [Consistent-Teacher: Towards Reducing Inconsistent Pseudo-Targets in Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Consistent-Teacher_Towards_Reducing_Inconsistent_Pseudo-Targets_in_Semi-Supervised_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Adamdad/ConsistentTeacher)
  - [ ]  [SOOD: Towards Semi-Supervised Oriented Object Detection](http://ar5iv.org/abs/2304.04515v1)<br>:star:[code](https://github.com/HamPerdredes/SOOD)
  - [ ]  [MixTeacher: Mining Promising Labels with Mixed Scale Teacher for Semi-Supervised Object Detection](https://ar5iv.org/abs/2303.09061)<br>:star:[code](https://github.com/lliuz/MixTeacher)
- [ ]  弱监督目标检测
  - [ ]  [DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection](http://ar5iv.org/abs/2304.07082v1)
- [ ]  小样本目标检测
  - [ ]  [NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging](https://ar5iv.org/abs/2303.04958)
  - [ ]  [Generating Features with Increased Crop-related Diversity for Few-Shot Object Detection](http://ar5iv.org/abs/2304.05096v1)
  - [ ]  [Meta-tuning Loss Functions and Data Augmentation for Few-shot Object Detection](http://ar5iv.org/abs/2304.12161v1)
  - [ ]  [DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection](https://ar5iv.org/abs/2303.09674)<br>:star:[code](https://github.com/Phoenix-V/DiGeo)
- [ ]  域适应目标检测
  - [ ]  [2PCNet: Two-Phase Consistency Training for Day-to-Night Unsupervised Domain Adaptive Object Detection](http://ar5iv.org/abs/2303.13853v1)
  - [ ]  [AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Hlings/AsyFOD)
  - [ ]  [CIGAR: Cross-Modality Graph Reasoning for Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_CIGAR_Cross-Modality_Graph_Reasoning_for_Domain_Adaptive_Object_Detection_CVPR_2023_paper.pdf)
  - [ ]  [Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection](https://ar5iv.org/abs/2203.15793)<br>:house:[project](https://viudomain.github.io/irg-sfda-web/)
  - [ ]  [Domain Adaptive Detection Transformer With Information Fusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DA-DETR_Domain_Adaptive_Detection_Transformer_With_Information_Fusion_CVPR_2023_paper.pdf)
  - [ ]  [Harmonious Teacher for Cross-Domain Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_Harmonious_Teacher_for_Cross-Domain_Object_Detection_CVPR_2023_paper.pdf)
  - [ ]  [Contrastive Mean Teacher for Domain Adaptive Object Detectors](http://ar5iv.org/abs/2305.03034v1)
- [ ]  弱样本目标检测
  - [ ]  [Weak-Shot Object Detection Through Mutual Knowledge Transfe](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Weak-Shot_Object_Detection_Through_Mutual_Knowledge_Transfer_CVPR_2023_paper.pdf)
- [ ]  显著目标检测
  - [ ]  [Sketch2Saliency: Learning to Detect Salient Objects from Human Drawings](https://ar5iv.org/abs/2303.11502)
  - [ ]  [Pixels, Regions, and Objects: Multiple Enhancement for Salient Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Pixels_Regions_and_Objects_Multiple_Enhancement_for_Salient_Object_Detection_CVPR_2023_paper.pdf)
  - [ ]  [Modeling the Distributional Uncertainty for Salient Object Detection Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Modeling_the_Distributional_Uncertainty_for_Salient_Object_Detection_Models_CVPR_2023_paper.pdf)<br>:star:[code](https://npucvr.github.io/Distributional_uncer/)
  - [ ]  [Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Veksler_Test_Time_Adaptation_With_Regularized_Loss_for_Weakly_Supervised_Salient_CVPR_2023_paper.pdf)
  - [ ]  [Texture-Guided Saliency Distilling for Unsupervised Salient Object Detection](http://ar5iv.org/abs/2207.05921)
- [ ]  红外目标检测
  - [ ]  [Physically Adversarial Infrared Patches with Learnable Shapes and Locations](http://ar5iv.org/abs/2303.13868v1)
  - [ ]  [TOPLight: Lightweight Neural Networks With Task-Oriented Pretraining for Visible-Infrared Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_TOPLight_Lightweight_Neural_Networks_With_Task-Oriented_Pretraining_for_Visible-Infrared_Recognition_CVPR_2023_paper.pdf)
- [ ]  伪装目标检测
  - [ ]  [Feature Shrinkage Pyramid for Camouflaged Object Detection with Transformers](http://ar5iv.org/abs/2303.14816v1)<br>:star:[code](https://tzxiang.github.io/project/COD-FSPNet/index.html)<br>:star:[code](https://github.com/ZhouHuang23/FSPNet)
  - [ ]  [Camouflaged Object Detection With Feature Decomposition and Edge Reconstruction](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Camouflaged_Object_Detection_With_Feature_Decomposition_and_Edge_Reconstruction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ChunmingHe/FEDER)
- [ ]  密集目标检测
  - [ ]  [Ambiguity-Resistant Semi-Supervised Learning for Dense Object Detection](http://ar5iv.org/abs/2303.14960v1)<br>:star:[code](https://github.com/PaddlePaddle/PaddleDetection)
- [ ]  协同目标检测
  - [ ]  [Discriminative Co-Saliency and Background Mining Transformer for Co-Salient Object Detection](https://ar5iv.org/abs/2305.00514)<br>:star:[code](https://github.com/dragonlee258079/DMT)
  - [ ]  [Co-Salient Object Detection With Uncertainty-Aware Group Exchange-Masking](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Co-Salient_Object_Detection_With_Uncertainty-Aware_Group_Exchange-Masking_CVPR_2023_paper.pdf)
- [ ]  点云目标检测
  - [ ]  [Open-Vocabulary Point-Cloud Object Detection Without 3D Annotation](https://ar5iv.org/abs/2304.00788)
- [ ]  目标发现
  - [ ]  [Object Discovery from Motion-Guided Tokens](http://ar5iv.org/abs/2303.15555v1)
- [ ]  视频目标检测
  - [ ]  [Feature Aggregated Queries for Transformer-Based Video Object Detectors](https://ar5iv.org/abs/2303.08319)
- [ ]  小目标检测
  - [ ]  [Dynamic Coarse-to-Fine Learning for Oriented Tiny Object Detection](http://ar5iv.org/abs/2304.08876v1)<br>:star:[code](https://github.com/Chasel-Tsui/mmrotate-dcfl)
  - [ ]  [Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection With Single Point Supervision](https://ar5iv.org/abs/2304.01484)<br>:star:[code](https://github.com/XinyiYing/LESPS)
  - [ ]  [Distilling Scale-Aware Knowledge in Small Object Detector](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_ScaleKD_Distilling_Scale-Aware_Knowledge_in_Small_Object_Detector_CVPR_2023_paper.pdf)
  - [ ]  [LSTFE-Net:Long Short-Term Feature Enhancement Network for Video Small Object Detection](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_LSTFE-NetLong_Short-Term_Feature_Enhancement_Network_for_Video_Small_Object_Detection_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xiaojs18/LSTFE-Net)
  - [ ]  红外小目标检测
    - [ ]  [Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection with Single Point Supervision](http://ar5iv.org/abs/2304.01484v1)<br>:star:[code](https://github.com/XinyiYing/LESPS)
- [ ]  线段检测
  - [ ]  [DeepLSD: Line Segment Detection and Refinement with Deep Image Gradients](https://ar5iv.org/abs/2212.07766)<br>:star:[code](https://github.com/cvg/DeepLSD)
- [ ]  目标导航
  - [ ]  [CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation](https://ar5iv.org/abs/2203.10421)

<a name="4"/>

## 4.Image Captioning(图像字幕生成)
- [ ]  视频字幕
  - [ ]  [Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning](https://ar5iv.org/pdf/2302.14115.pdf)<br>:house:[project](https://antoyang.github.io/vid2seq.html)
  - [ ]  [Text with Knowledge Graph Augmented Transformer for Video Captioning](http://ar5iv.org/abs/2303.12423v1)
  - [ ]  [Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation](http://ar5iv.org/abs/2303.12112v1)<br>:star:[code](https://github.com/aimagelab/pacscore)
- [ ]  图像字幕
  - [ ]  [Cross-Domain Image Captioning with Discriminative Finetuning](http://ar5iv.org/abs/2304.01662v1)
  - [ ]  [Crossing the Gap: Domain Generalization for Image Captioning](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Crossing_the_Gap_Domain_Generalization_for_Image_Captioning_CVPR_2023_paper.pdf)
  - [ ]  [Model-Agnostic Gender Debiased Image Captioning](http://ar5iv.org/abs/2304.03693v1)
  - [ ]  [A-CAP: Anticipation Captioning with Commonsense Knowledge](http://ar5iv.org/abs/2304.06602v1)字幕
  - [ ]  [Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation](https://ar5iv.org/abs/2303.12112)<br>:star:[code](https://github.com/aimagelab/pacscore)
  - [ ]  [HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning](https://openaccess.thecvf.com/content/CVPR2023/papers/Kuo_HAAV_Hierarchical_Aggregation_of_Augmented_Views_for_Image_Captioning_CVPR_2023_paper.pdf)
  - [ ]  [Semantic-Conditional Diffusion Networks for Image Captioning](https://ar5iv.org/abs/2212.03099)<br>:star:[code](https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet)
  - [ ]  [ConZIC: Controllable Zero-Shot Image Captioning by Sampling-Based Polishing](https://ar5iv.org/abs/2303.02437)
  - [ ]  [SmallCap: Lightweight Image Captioning Prompted With Retrieval Augmentation](https://ar5iv.org/abs/2209.15323)
- [ ]  story generation(视觉故事生成)
  - [ ]  [Make-A-Story: Visual Memory Conditioned Consistent Story Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Rahman_Make-a-Story_Visual_Memory_Conditioned_Consistent_Story_Generation_CVPR_2023_paper.pdf)  
- [ ]  3D密集字幕
  - [ ]  [End-to-End 3D Dense Captioning With Vote2Cap-DETR](https://ar5iv.org/abs/2301.02508) 
 
<a name="3"/>

## 3.Image Progress(低层图像处理、质量评价)
- [ ]  [Initialization Noise in Image Gradients and Saliency Maps](https://openaccess.thecvf.com/content/CVPR2023/papers/Woerl_Initialization_Noise_in_Image_Gradients_and_Saliency_Maps_CVPR_2023_paper.pdf)
- [ ]  [Learning a Practical SDR-to-HDRTV Up-conversion using New Dataset and Degradation Models](http://ar5iv.org/abs/2303.13031v1)<br>:star:[code](https://github.com/AndreGuo/HDRTVDM)
- [ ]  [Tunable Convolutions with Parametric Multi-Loss Optimization](http://ar5iv.org/abs/2304.00898v1)
- [ ]  图像着色
  - [ ]  [L-CoIns: Language-based Colorization with Instance Awareness](https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_L-CoIns_Language-Based_Colorization_With_Instance_Awareness_CVPR_2023_paper.pdf)
  - [ ]  色彩恢复
    - [ ]  [GamutMLP: A Lightweight MLP for Color Loss Recovery](https://ar5iv.org/abs/2304.11743)<br>:house:[project](https://gamut-mlp.github.io/)
- [ ]  阴影去除
  - [ ]  [ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal](https://ar5iv.org/abs/2212.04711)
  - [ ]  [Document Image Shadow Removal Guided by Color-Aware Background](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Document_Image_Shadow_Removal_Guided_by_Color-Aware_Background_CVPR_2023_paper.pdf)
  - [ ]  [DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering](http://ar5iv.org/abs/2303.15101v1)
- [ ]  图像恢复
  - [ ]  [Efficient and Explicit Modelling of Image Hierarchies for Image Restoration](https://ar5iv.org/pdf/2303.00748.pdf)<br>:star:[code](https://github.com/ofsoundof/GRL-Image-Restoration.git)
  - [ ]  [Visual Recognition-Driven Image Restoration for Multiple Degradation With Intrinsic Semantics Recovery](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Visual_Recognition-Driven_Image_Restoration_for_Multiple_Degradation_With_Intrinsic_Semantics_CVPR_2023_paper.pdf)
  - [ ]  [Learning Weather-General and Weather-Specific Features for Image Restoration Under Multiple Adverse Weather Conditions](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Learning_Weather-General_and_Weather-Specific_Features_for_Image_Restoration_Under_Multiple_CVPR_2023_paper.pdf)
  - [ ]  [Generating Aligned Pseudo-Supervision From Non-Aligned Data for Image Restoration in Under-Display Camera](https://ar5iv.org/abs/2304.06019)<br>:star:[code](https://github.com/jnjaby/AlignFormer)
  - [ ]  [Comprehensive and Delicate: An Efficient Transformer for Image Restoration](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Comprehensive_and_Delicate_An_Efficient_Transformer_for_Image_Restoration_CVPR_2023_paper.pdf)
  - [ ]  [Ingredient-Oriented Multi-Degradation Learning for Image Restoration](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Ingredient-Oriented_Multi-Degradation_Learning_for_Image_Restoration_CVPR_2023_paper.pdf)
  - [ ]  [All-in-One Image Restoration for Unknown Degradations Using Adaptive Discriminative Filters for Specific Degradations](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_All-in-One_Image_Restoration_for_Unknown_Degradations_Using_Adaptive_Discriminative_Filters_CVPR_2023_paper.pdf)
  - [ ]  [Contrastive Semi-Supervised Learning for Underwater Image Restoration via Reliable Bank](https://ar5iv.org/abs/2303.09101)<br>:star:[code](https://github.com/Huang-ShiRui/Semi-UIR)
  - [ ]  [Burstormer: Burst Image Restoration and Enhancement Transformer](http://ar5iv.org/abs/2304.01194v1)
  - [ ]  [Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera](http://ar5iv.org/abs/2304.06019v1)<br>:star:[code](https://github.com/jnjaby/AlignFormer)
  - [ ]  [Generative Diffusion Prior for Unified Image Restoration and Enhancement](http://ar5iv.org/abs/2304.01247v1)
  - [ ]  [Bitstream-Corrupted JPEG Images are Restorable: Two-stage Compensation and Alignment Framework for Image Restoration](http://ar5iv.org/abs/2304.06976v1)<br>:star:[code](https://github.com/wenyang001/Two-ACIR)
  - [ ]  [Learning Distortion Invariant Representation for Image Restoration From a Causality Perspective](https://ar5iv.org/abs/2303.06859)<br>:star:[code](https://github.com/lixinustc/Causal-IR-DIL)
  - [ ]  [Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack](https://ar5iv.org/abs/2304.11436)
  - [ ]  [Robust Unsupervised StyleGAN Image Restoration](https://ar5iv.org/abs/2302.06733)<br>:house:[project](https://lvsn.github.io/RobustUnsupervised/)
- [ ]  图像修复
  - [ ]  [NUWA-LIP: Language-guided Image Inpainting with Defect-free VQGAN](https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_NUWA-LIP_Language-Guided_Image_Inpainting_With_Defect-Free_VQGAN_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/kodenii/NUWA-LIP)
  - [ ]  [Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting](https://ar5iv.org/abs/2212.06909)
  - [ ]  [SmartBrush: Text and Shape Guided Object Inpainting With Diffusion Model](https://ar5iv.org/abs/2212.05034)
- [ ]  视频恢复
  - [ ]  [A Simple Baseline for Video Restoration With Grouped Spatial-Temporal Shift](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Simple_Baseline_for_Video_Restoration_With_Grouped_Spatial-Temporal_Shift_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dasongli1/Shift-Net)
- [ ]  视频修复
  - [ ]  [Deep Stereo Video Inpainting](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Deep_Stereo_Video_Inpainting_CVPR_2023_paper.pdf)
  - [ ]  [Semi-Supervised Video Inpainting With Cycle Consistency Constraints](https://ar5iv.org/abs/2208.06807)
- [ ]  图像照明
  - [ ]  [Controllable Light Diffusion for Portraits](http://ar5iv.org/abs/2305.04745v1)
- [ ]  图像质量评估
  - [ ]  [Quality-aware Pre-trained Models for Blind Image Quality Assessment](https://ar5iv.org/pdf/2303.00521.pdf)
  - [ ]  [Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective](http://ar5iv.org/abs/2303.14968v1)<br>:star:[code](https://github.com/zwx8981/LIQE)
  - [ ]  [Quality-Aware Pre-Trained Models for Blind Image Quality Assessment](https://ar5iv.org/abs/2303.00521)
  - [ ]  [An Image Quality Assessment Dataset for Portraits](https://ar5iv.org/abs/2304.05772)<br>:star:[code](https://github.com/DXOMARK-Research/PIQ2023)
  - [ ]  [Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild](http://ar5iv.org/abs/2304.00451v1)
- [ ]  去雾
  - [ ]  [Video Dehazing via a Multi-Range Temporal Alignment Network with Physical Prior](https://ar5iv.org/abs/2303.09757)<br>:star:[code](https://github.com/jiaqixuac/MAP-Net)
  - [ ]  [Curricular Contrastive Regularization for Physics-aware Single Image Dehazing](http://ar5iv.org/abs/2303.14218v1)
  - [ ]  [Curricular Contrastive Regularization for Physics-Aware Single Image Dehazing](https://ar5iv.org/abs/2303.14218)
  - [ ]  [Efficient Frequency Domain-Based Transformers for High-Quality Image Deblurring](https://ar5iv.org/abs/2211.12250)<br>:star:[code](https://github.com/kkkls/FFTformer)
  - [ ]  [RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors](http://ar5iv.org/abs/2304.03994v1)
- [ ]  去雨
  - [ ]  [Learning A Sparse Transformer Network for Effective Image Deraining](https://ar5iv.org/abs/2303.11950)<br>:star:[code](https://github.com/cschenxiang/DRSformer) 
  - [ ]  [SmartAssign: Learning a Smart Knowledge Assignment Strategy for Deraining and Desnowing](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_SmartAssign_Learning_a_Smart_Knowledge_Assignment_Strategy_for_Deraining_and_CVPR_2023_paper.pdf)<br>:house:[project](https://gitee.com/mindspore/models/tree/master/research/cv/SmartAssign)
- [ ]  去噪
  - [ ]  [Masked Image Training for Generalizable Deep Image Denoising](http://ar5iv.org/abs/2303.13132v1)
  - [ ]  [Real-Time Controllable Denoising for Image and Video](http://ar5iv.org/abs/2303.16425)
  - [ ]  [Patch-Craft Self-Supervised Training for Correlated Image Denoising](https://ar5iv.org/abs/2211.09919)
  - [ ]  [Polarized Color Image Denoising](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Polarized_Color_Image_Denoising_CVPR_2023_paper.pdf)
  - [ ]  [sRGB Real Noise Synthesizing With Neighboring Correlation-Aware Noise Model](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_sRGB_Real_Noise_Synthesizing_With_Neighboring_Correlation-Aware_Noise_Model_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xuan611/sRGB-Real-Noise-Synthesizing)
  - [ ]  [Zero-Shot Noise2Noise: Efficient Image Denoising Without Any Data](https://ar5iv.org/abs/2303.11253)<br>:house:[project](https://colab.research.google.com/drive/1i82nyizTdszyHkaHBuKPbWnTzao8HF9b)
  - [ ]  [HouseDiffusion: Vector Floorplan Generation via a Diffusion Model With Discrete and Continuous Denoising](https://ar5iv.org/abs/2211.13287)<br>:house:[project](https://aminshabani.github.io/housediffusion)
  - [ ]  [Structure Aggregation for Cross-Spectral Stereo Image Guided Denoising](https://openaccess.thecvf.com/content/CVPR2023/papers/Sheng_Structure_Aggregation_for_Cross-Spectral_Stereo_Image_Guided_Denoising_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/lustrouselixir/SANet)
  - [ ]  [Spatially Adaptive Self-Supervised Learning for Real-World Image Denoising](http://ar5iv.org/abs/2303.14934v1)<br>:star:[code](https://github.com/nagejacob/SpatiallyAdaptiveSSID) 
  - [ ]  [Spectral Enhanced Rectangle Transformer for Hyperspectral Image Denoising](https://ar5iv.org/abs/2304.00844)<br>:star:[code](https://github.com/MyuLi/SERT)
  - [ ]  [Real-time Controllable Denoising for Image and Video](http://ar5iv.org/abs/2303.16425v1)
  - [ ]  [LG-BPN: Local and Global Blind-Patch Network for Self-Supervised Real-World Denoising](http://ar5iv.org/abs/2304.00534v1)<br>:star:[code](https://github.com/Wang-XIaoDingdd/LGBPN)
  - [ ]  [Efficient View Synthesis and 3D-based Multi-Frame Denoising with Multiplane Feature Representations](http://ar5iv.org/abs/2303.18139v1)
  - [ ]  [Learning with Noisy labels via Self-supervised Adversarial Noisy Masking](https://ar5iv.org/abs/2302.06805)去噪
  - [ ]  [Learning from Noisy Labels with Decoupled Meta Label Purifier](https://ar5iv.org/abs/2302.06810)去噪
- [ ]  去模糊
  - [ ]  [HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering](http://ar5iv.org/abs/2304.01686v1)<br>:star:[code](https://github.com/VinAIResearch/HyperCUT.git)
  - [ ]  [Neumann Network With Recursive Kernels for Single Image Defocus Deblurring](https://openaccess.thecvf.com/content/CVPR2023/papers/Quan_Neumann_Network_With_Recursive_Kernels_for_Single_Image_Defocus_Deblurring_CVPR_2023_paper.pdf)
  - [ ]  [K3DN: Disparity-Aware Kernel Estimation for Dual-Pixel Defocus Deblurring](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_K3DN_Disparity-Aware_Kernel_Estimation_for_Dual-Pixel_Defocus_Deblurring_CVPR_2023_paper.pdf)
  - [ ]  [Uncertainty-Aware Unsupervised Image Deblurring With Deep Residual Prior](https://ar5iv.org/abs/2210.05361)
  - [ ]  [$\text{DC}^2$: Dual-Camera Defocus Control by Learning to Refocus](http://ar5iv.org/abs/2304.03285v1)<br>:star:[code](https://defocus-control.github.io)去模糊
  - [ ]  [Self-Supervised Non-Uniform Kernel Estimation With Flow-Based Motion Prior for Blind Image Deblurring](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_Self-Supervised_Non-Uniform_Kernel_Estimation_With_Flow-Based_Motion_Prior_for_Blind_CVPR_2023_paper.pdf)<br>:house:[project](https://see.xidian.edu.cn/faculty/wsdong/Projects/UFPNet.htm)
  - [ ]  [Joint Video Multi-Frame Interpolation and Deblurring Under Unknown Exposure Time](https://ar5iv.org/abs/2303.15043)<br>:star:[code](https://github.com/shangwei5/VIDUE)
  - [ ]  [Event-Based Frame Interpolation With Ad-Hoc Deblurring](https://ar5iv.org/abs/2301.05191)
  - [ ]  [Deep Discriminative Spatial and Temporal Network for Efficient Video Deblurring](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Deep_Discriminative_Spatial_and_Temporal_Network_for_Efficient_Video_Deblurring_CVPR_2023_paper.pdf)
- [ ]  去鬼影
  - [ ]  [A Unified HDR Imaging Method with Pixel and Patch Level](https://ar5iv.org/abs/2304.06943)
  - [ ]  [SMAE: Few-shot Learning for HDR Deghosting with Saturation-Aware Masked Autoencoders](http://ar5iv.org/abs/2304.06914v1)
- [ ]  去反射光斑
  - [ ]  [Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior](http://ar5iv.org/abs/2303.15046v1)<br>:star:[code](https://github.com/ykdai/BracketFlare)
- [ ]  image deweathering
  - [ ]  [WeatherStream: Light Transport Automation of Single Image Deweathering](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_WeatherStream_Light_Transport_Automation_of_Single_Image_Deweathering_CVPR_2023_paper.pdf)<br>:house:[project](http://visual.ee.ucla.edu/wstream.htm/)
- [ ]  图像缩放
  - [ ]  [HyperThumbnail: Real-time 6K Image Rescaling with Rate-distortion Optimization](http://ar5iv.org/abs/2304.01064v1)<br>:star:[code](https://github.com/AbnerVictor/HyperThumbnail)
  - [ ]  [Real-time 6K Image Rescaling with Rate-distortion Optimization](https://ar5iv.org/abs/2304.01064)<br>:star:[code](https://github.com/AbnerVictor/HyperThumbnail)
- [ ]  瞬间恢复与增强
  - [ ]  [Gated Multi-Resolution Transfer Network for Burst Restoration and Enhancement](http://ar5iv.org/abs/2304.06703v1)
- [ ]  图像增强
  - [ ]  [Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement](http://ar5iv.org/abs/2304.07039v1)
  - [ ]  [Realistic Saliency Guided Image Enhancement](https://openaccess.thecvf.com/content/CVPR2023/papers/Miangoleh_Realistic_Saliency_Guided_Image_Enhancement_CVPR_2023_paper.pdf)
  - [ ]  [Learning a Simple Low-Light Image Enhancer From Paired Low-Light Instances](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_a_Simple_Low-Light_Image_Enhancer_From_Paired_Low-Light_Instances_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zhenqifu/PairLIE)
  - [ ]  [Low-Light Image Enhancement via Structure Modeling and Guidance](https://ar5iv.org/abs/2305.05839)
  - [ ]  [You Do Not Need Additional Priors or Regularizers in Retinex-Based Low-Light Image Enhancement](https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_You_Do_Not_Need_Additional_Priors_or_Regularizers_in_Retinex-Based_CVPR_2023_paper.pdf)
- [ ]  图像和谐化
  - [ ]  [LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization](http://ar5iv.org/abs/2304.13166v1)
  - [ ]  [Semi-supervised Parametric Real-world Image Harmonization](https://ar5iv.org/abs/2303.00157)
  - [ ]  [PCT-Net: Full Resolution Image Harmonization Using Pixel-Wise Color Transformations](https://openaccess.thecvf.com/content/CVPR2023/papers/Guerreiro_PCT-Net_Full_Resolution_Image_Harmonization_Using_Pixel-Wise_Color_Transformations_CVPR_2023_paper.pdf)
- [ ]  图像曝光校正
  - [ ]  [Decoupling-and-Aggregating for Image Exposure Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Decoupling-and-Aggregating_for_Image_Exposure_Correction_CVPR_2023_paper.pdf)
- [ ]  物体移除
  - [ ]  [Automatic High Resolution Wire Segmentation and Removal](https://ar5iv.org/abs/2304.00221)<br>:star:[code](https://github.com/adobe-research/auto-wire-removal)
- [ ]  Image Decomposition 
  - [ ]  [Light Source Separation and Intrinsic Image Decomposition Under AC Illumination](https://openaccess.thecvf.com/content/CVPR2023/papers/Yoshida_Light_Source_Separation_and_Intrinsic_Image_Decomposition_Under_AC_Illumination_CVPR_2023_paper.pdf)
  - [ ]  [Context-aware Pretraining for Efficient Blind Image Decomposition](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Context-Aware_Pretraining_for_Efficient_Blind_Image_Decomposition_CVPR_2023_paper.pdf)
  - [ ]  [Unsupervised Intrinsic Image Decomposition With LiDAR Intensity](http://ar5iv.org/abs/2303.10820)
- [ ]  图像重建
  - [ ]  [Raw Image Reconstruction With Learned Compact Metadata](https://ar5iv.org/abs/2302.12995)<br>:star:[code](https://github.com/wyf0912/R2LCM)
  - [ ]  [Catch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder](https://ar5iv.org/abs/2305.02541)
  - [ ]  [High-Resolution Image Reconstruction With Latent Diffusion Models From Human Brain Activity](https://openaccess.thecvf.com/content/CVPR2023/papers/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.pdf)<br>:house:[project](https://sites.google.com/view/stablediffusion-with-brain/)
  - [ ]  [PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices](https://ar5iv.org/abs/2211.12562)<br>:house:[project](https://radualexandru.github.io/permuto_sdf)
- [ ]  文本驱动的图像处理
  - [ ]  [DeltaEdit: Exploring Text-Free Training for Text-Driven Image Manipulation](https://ar5iv.org/abs/2303.06285)<br>:star:[code](https://github.com/Yueming6568/DeltaEdit)
- [ ]  运动模糊
  - [ ]  [Self-supervised Blind Motion Deblurring with Deep Expectation Maximization](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Self-Supervised_Blind_Motion_Deblurring_With_Deep_Expectation_Maximization_CVPR_2023_paper.pdf)
- [ ]  图像裁剪
  - [ ]  [Image Cropping With Spatial-Aware Feature and Rank Consistency](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_Cropping_With_Spatial-Aware_Feature_and_Rank_Consistency_CVPR_2023_paper.pdf)
- [ ]  图像重照明
  - [ ]  [Weakly-supervised Single-view Image Relighting](https://ar5iv.org/abs/2303.13852)<br>:house:[project](https://renjiaoyi.github.io/relighting/)
  - [ ]  [SunStage: Portrait Reconstruction and Relighting Using the Sun as a Light Stage](http://ar5iv.org/abs/2204.03648)
- [ ]  模糊帧插值
  - [ ]  [Event-Based Blurry Frame Interpolation Under Blind Exposure](https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_Event-Based_Blurry_Frame_Interpolation_Under_Blind_Exposure_CVPR_2023_paper.pdf)

<a name="2"/>

## 2.Image Segmentation(图像分割)
- [ ]  [MED-VT: Multiscale Encoder-Decoder Video Transformer With Application To Object Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_MED-VT_Multiscale_Encoder-Decoder_Video_Transformer_With_Application_To_Object_Segmentation_CVPR_2023_paper.pdf)
- [ ]  [SimpSON: Simplifying Photo Cleanup With Single-Click Distracting Object Segmentation Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Huynh_SimpSON_Simplifying_Photo_Cleanup_With_Single-Click_Distracting_Object_Segmentation_Network_CVPR_2023_paper.pdf)
- [ ]  [Towards Open-World Segmentation of Parts](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Towards_Open-World_Segmentation_of_Parts_CVPR_2023_paper.pdf)
- [ ]  [Heat Diffusion Based Multi-Scale and Geometric Structure-Aware Transformer for Mesh Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wong_Heat_Diffusion_Based_Multi-Scale_and_Geometric_Structure-Aware_Transformer_for_Mesh_CVPR_2023_paper.pdf)
- [ ]  [MOVES: Manipulated Objects in Video Enable Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Higgins_MOVES_Manipulated_Objects_in_Video_Enable_Segmentation_CVPR_2023_paper.pdf)
- [ ]  [Decoupled Semantic Prototypes Enable Learning From Diverse Annotation Types for Semi-Weakly Segmentation in Expert-Driven Domains](https://openaccess.thecvf.com/content/CVPR2023/papers/Reiss_Decoupled_Semantic_Prototypes_Enable_Learning_From_Diverse_Annotation_Types_for_CVPR_2023_paper.pdf)
- [ ]  [Compositor: Bottom-Up Clustering and Compositing for Robust Part and Object Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Compositor_Bottom-Up_Clustering_and_Compositing_for_Robust_Part_and_Object_CVPR_2023_paper.pdf)
- [ ]  [VectorFloorSeg: Two-Stream Graph Attention Network for Vectorized Roughcast Floorplan Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_VectorFloorSeg_Two-Stream_Graph_Attention_Network_for_Vectorized_Roughcast_Floorplan_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/DrZiji/VecFloorSeg)
- [ ]  [Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervisio](https://ar5iv.org/abs/2303.03361)
- [ ]  [OneFormer: One Transformer To Rule Universal Image Segmentation](https://ar5iv.org/abs/2211.06220)<br>:house:[project](https://praeclarumjj3.github.io/oneformer)
- [ ]  [PanelNet: Understanding 360 Indoor Environment via Panel Representation](http://ar5iv.org/abs/2305.09078v1)
- [ ]  [AutoFocusFormer: Image Segmentation off the Grid](http://ar5iv.org/abs/2304.12406v1)
- [ ]  [MP-Former: Mask-Piloted Transformer for Image Segmentation](https://ar5iv.org/abs/2303.07336)<br>:star:[code](https://github.com/IDEA-Research/MP-Former)
- [ ]  [Explicit Visual Prompting for Low-Level Structure Segmentations](https://ar5iv.org/abs/2303.10883)<br>:star:[code](https://github.com/NiFangBaAGe/Explict-Visual-Prompt)
- [ ]  [Focused and Collaborative Feedback Integration for Interactive Image Segmentation](https://ar5iv.org/abs/2303.11880)<br>:star:[code](https://github.com/veizgyauzgyauz/FCFI)
- [ ]  [FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation](http://ar5iv.org/abs/2303.17225v1)<br>:house:[project](https://FreeSeg.github.io)<br>在 VIS、VOS、MOTS 三个下游视频分割任务的五个数据集上，将 InstMove 插入到现有 SOTA 模型可以进一步带来 1~5 个点的提升。
- [ ]  [MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation](http://ar5iv.org/abs/2304.05930v1)分割
- [ ]  零样本分割
  - [ ]  [Primitive Generation and Semantic-Related Alignment for Universal Zero-Shot Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Primitive_Generation_and_Semantic-Related_Alignment_for_Universal_Zero-Shot_Segmentation_CVPR_2023_paper.pdf)<br>:house:[project](https://henghuiding.github.io/PADing)<br>:thumbsup:[CVPR23 | 浙大、NTU提出零样本通用分割框架PADing](https://mp.weixin.qq.com/s/IngwwSYXKQbkAYOI7NaXJw)
- [ ]  3D分割
  - [ ]  [EFEM: Equivariant Neural Field Expectation Maximization for 3D Object Segmentation Without Scene Supervision](http://ar5iv.org/abs/2303.15440v1)<br>:house:[project](https://www.cis.upenn.edu/~leijh/projects/efem)
- [ ]  全景分割
  - [ ]  [CoMFormer: Continual Learning in Semantic and Panoptic Segmentation](https://ar5iv.org/abs/2211.13999)
  - [ ]  [Center Focusing Network for Real-Time LiDAR Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Center_Focusing_Network_for_Real-Time_LiDAR_Panoptic_Segmentation_CVPR_2023_paper.pdf)
  - [ ]  [Context-Aware Relative Object Queries To Unify Video Instance and Panoptic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Choudhuri_Context-Aware_Relative_Object_Queries_To_Unify_Video_Instance_and_Panoptic_CVPR_2023_paper.pdf)
  - [ ]  实时全景分割
    - [ ]  [You Only Segment Once: Towards Real-Time Panoptic Segmentation](http://ar5iv.org/abs/2303.14651v1)<br>:star:[code](https://github.com/hujiecpp/YOSO)
  - [ ]  域适应全景分割
    - [ ]  [UniDAformer: Unified Domain Adaptive Panoptic Segmentation Transformer via Hierarchical Mask Calibration](https://ar5iv.org/abs/2206.15083)
  - [ ]  开放词汇全景分割
    - [ ]  [Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models](https://ar5iv.org/abs/2303.04803)<br>:star:[code](https://github.com/NVlabs/ODISE)
- [ ]  实例分割
  - [ ]  [DynaMask: Dynamic Mask Selection for Instance Segmentation](https://ar5iv.org/abs/2303.07868)<br>:star:[code](https://github.com/lslrh/DynaMask)
  - [ ]  [Tree Instance Segmentation With Temporal Contour Graph](https://openaccess.thecvf.com/content/CVPR2023/papers/Firoze_Tree_Instance_Segmentation_With_Temporal_Contour_Graph_CVPR_2023_paper.pdf)
  - [ ]  [Hi4D: 4D Instance Segmentation of Close Human Interaction](http://ar5iv.org/abs/2303.15380)
  - [ ]  [Beyond mAP: Towards Better Evaluation of Instance Segmentation](https://ar5iv.org/abs/2207.01614)
  - [ ]  [Boosting Low-Data Instance Segmentation by Unsupervised Pre-Training With Saliency Prompt](https://ar5iv.org/abs/2302.01171)
  - [ ]  [Cut and Learn for Unsupervised Object Detection and Instance Segmentation](https://ar5iv.org/abs/2301.11320)<br>:star:[code](https://github.com/facebookresearch/CutLER)
  - [ ]  [PartDistillation: Learning Parts From Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_PartDistillation_Learning_Parts_From_Instance_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/facebookresearch/PartDistillation)
  - [ ]  [Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections](https://ar5iv.org/abs/2212.03022)<br>:star:[code](http://github.com/alexander-g/INBD)
  - [ ]  [AttentionShift: Iteratively Estimated Part-Based Attention Map for Pointly Supervised Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_AttentionShift_Iteratively_Estimated_Part-Based_Attention_Map_for_Pointly_Supervised_Instance_CVPR_2023_paper.pdf)
  - [ ]  [DoNet: Deep De-overlapping Network for Cytology Instance Segmentation](http://ar5iv.org/abs/2303.14373v1)<br>:star:[code](https://github.com/DeepDoNet/DoNet)
  - [ ]  [FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation](https://ar5iv.org/abs/2303.08594)<br>:star:[code](https://github.com/junjiehe96/FastInst)
  - [ ]  [Camouflaged Instance Segmentation via Explicit De-Camouflaging](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Camouflaged_Instance_Segmentation_via_Explicit_De-Camouflaging_CVPR_2023_paper.pdf)
  - [ ]  无监督实例分割
    - [ ]  [Exemplar-FreeSOLO: Enhancing Unsupervised Instance Segmentation With Exemplars](https://openaccess.thecvf.com/content/CVPR2023/papers/Ishtiak_Exemplar-FreeSOLO_Enhancing_Unsupervised_Instance_Segmentation_With_Exemplars_CVPR_2023_paper.pdf)
  - [ ]  弱监督实例分割
    - [ ]  [SIM: Semantic-aware Instance Mask Generation for Box-Supervised Instance Segmentation](https://ar5iv.org/abs/2303.08578)<br>:star:[code](https://github.com/lslrh/SIM)
    - [ ]  [BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation](https://ar5iv.org/abs/2210.05174)<br>:star:[code](https://github.com/hustvl/BoxTeacher)
    - [ ]  [The Devil is in the Points: Weakly Semi-Supervised Instance Segmentation via Point-Guided Mask Representation](http://ar5iv.org/abs/2303.15062v1)<br>:star:[code](https://github.com/clovaai/PointWSSIS)
  - [ ]  开放词汇实例分割
    - [ ]  [Mask-free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations](http://ar5iv.org/abs/2303.16891v1)<br>:star:[code](https://vibashan.github.io/ovis-web/)
  - [ ]  零样本实例分割
    - [ ]  [Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/He_Semantic-Promoted_Debiasing_and_Background_Disambiguation_for_Zero-Shot_Instance_Segmentation_CVPR_2023_paper.pdf)<br>:house:[project](https://henghuiding.github.io/D2Zero)
- [ ]  语义分割
  - [ ]  [IFSeg: Image-free Semantic Segmentation via Vision-Language Model](http://ar5iv.org/abs/2303.14396v1)<br>:star:[code](https://github.com/alinlab/ifseg)
  - [ ]  [Transformer Scale Gate for Semantic Segmentation](http://ar5iv.org/abs/2205.07056)
  - [ ]  [Towards Better Stability and Adaptability: Improve Online Self-Training for Model Adaptation in Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Towards_Better_Stability_and_Adaptability_Improve_Online_Self-Training_for_Model_CVPR_2023_paper.pdf)
  - [ ]  [BAEFormer: Bi-Directional and Early Interaction Transformers for Bird's Eye View Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_BAEFormer_Bi-Directional_and_Early_Interaction_Transformers_for_Birds_Eye_View_CVPR_2023_paper.pdf)
  - [ ]  [Combining Implicit-Explicit View Correlation for Light Field Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Cong_Combining_Implicit-Explicit_View_Correlation_for_Light_Field_Semantic_Segmentation_CVPR_2023_paper.pdf)
  - [ ]  [Pruning Parameterization With Bi-Level Optimization for Efficient Semantic Segmentation on the Edge](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Pruning_Parameterization_With_Bi-Level_Optimization_for_Efficient_Semantic_Segmentation_on_CVPR_2023_paper.pdf)
  - [ ]  [Less Is More: Reducing Task and Model Complexity for 3D Point Cloud Semantic Segmentation](http://ar5iv.org/abs/2303.11203)
  - [ ]  [SemiCVT: Semi-Supervised Convolutional Vision Transformer for Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_SemiCVT_Semi-Supervised_Convolutional_Vision_Transformer_for_Semantic_Segmentation_CVPR_2023_paper.pdf)
  - [ ]  [PIDNet: A Real-Time Semantic Segmentation Network Inspired by PID Controllers](https://ar5iv.org/abs/2206.02066)
  - [ ]  [Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions](https://ar5iv.org/abs/2303.14115)
  - [ ]  [PeakConv: Learning Peak Receptive Field for Radar Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PeakConv_Learning_Peak_Receptive_Field_for_Radar_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/zlw9161/PKC)
  - [ ]  [Understanding Imbalanced Semantic Segmentation Through Neural Collapse](https://ar5iv.org/abs/2301.01100)<br>:star:[code](https://github.com/dvlab-research/Imbalanced-Learning)
  - [ ]  [Geometry and Uncertainty-Aware 3D Point Cloud Class-Incremental Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Geometry_and_Uncertainty-Aware_3D_Point_Cloud_Class-Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/leolyj/3DPC-CISS)
  - [ ]  [Single Domain Generalization for LiDAR Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Single_Domain_Generalization_for_LiDAR_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/gzgzys9887/DGLSS)
  - [ ]  [FedSeg: Class-Heterogeneous Federated Learning for Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Miao_FedSeg_Class-Heterogeneous_Federated_Learning_for_Semantic_Segmentation_CVPR_2023_paper.pdf)
  - [ ]  [Proximal Splitting Adversarial Attack for Semantic Segmentation](https://ar5iv.org/abs/2206.07179)<br>:star:[code](https://github.com/jeromerony/alma_prox_segmentation)
  - [ ]  [On Calibrating Semantic Segmentation Models: Analyses and an Algorithm](https://ar5iv.org/abs/2212.12053)
  - [ ]  [Incrementer: Transformer for Class-Incremental Semantic Segmentation With Knowledge Distillation Focusing on Old Class](https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Incrementer_Transformer_for_Class-Incremental_Semantic_Segmentation_With_Knowledge_Distillation_Focusing_CVPR_2023_paper.pdf)
  - [ ]  [Content-Aware Token Sharing for Efficient Semantic Segmentation With Vision Transformers](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Content-Aware_Token_Sharing_for_Efficient_Semantic_Segmentation_With_Vision_Transformers_CVPR_2023_paper.pdf)
  - [ ]  [Endpoints Weight Fusion for Class Incremental Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Endpoints_Weight_Fusion_for_Class_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf)
  - [ ]  [Sparsely Annotated Semantic Segmentation With Adaptive Gaussian Mixtures](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Sparsely_Annotated_Semantic_Segmentation_With_Adaptive_Gaussian_Mixtures_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Luffy03/AGMM-SASS)
  - [ ]  [ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation](https://ar5iv.org/abs/2210.05944)
  - [ ]  [Improving Robustness of Semantic Segmentation to Motion-Blur Using Class-Centric Augmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Aakanksha_Improving_Robustness_of_Semantic_Segmentation_to_Motion-Blur_Using_Class-Centric_Augmentation_CVPR_2023_paper.pdf)
  - [ ]  [Dynamic Focus-Aware Positional Queries for Semantic Segmentation](https://ar5iv.org/abs/2204.01244)<br>:star:[code](https://github.com/ziplab/FASeg)
  - [ ]  [Continual Semantic Segmentation With Automatic Memory Sample Selection](https://ar5iv.org/abs/2304.05015)
  - [ ]  [Learning Open-Vocabulary Semantic Segmentation Models From Natural Language Supervision](https://ar5iv.org/abs/2301.09121)
  - [ ]  [Dynamically Instance-Guided Adaptation: A Backward-Free Approach for Test-Time Domain Adaptive Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dynamically_Instance-Guided_Adaptation_A_Backward-Free_Approach_for_Test-Time_Domain_Adaptive_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Waybaba/DIGA)
  - [ ]  [Federated Incremental Semantic Segmentation](http://ar5iv.org/abs/2304.04620v1)<br>:star:[code](https://github.com/JiahuaDong/FISS)
  - [ ]  [Delivering Arbitrary-Modal Semantic Segmentation](https://ar5iv.org/abs/2303.01480)<br>:star:[code](https://jamycheung.github.io/DELIVER.html)
  - [ ]  [Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation](https://ar5iv.org/pdf/2302.14250.pdf)
  - [ ]  [A Simple Framework for Text-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_A_Simple_Framework_for_Text-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/muyangyi/SimSeg)<br>在 PASCAL VOC 2012、PASCAL Context 和 COCO 数据集上的表现明显优于之前最先进的方法。
  - [ ]  [Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors](https://ar5iv.org/pdf/2302.14746.pdf)
  - [ ]  [Generative Semantic Segmentation](https://ar5iv.org/abs/2303.11316)<br>:star:[code](https://github.com/fudan-zvg/GSS)
  - [ ]  [Reliability in Semantic Segmentation: Are We on the Right Track?](https://ar5iv.org/abs/2303.11298)<br>:star:[code](https://github.com/naver/relis)
  - [ ]  [Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation](http://ar5iv.org/abs/2303.14360v1)
  - [ ]  [Less is More: Reducing Task and Model Complexity for 3D Point Cloud Semantic Segmentation](https://ar5iv.org/abs/2303.11203)<br>:star:[code](https://github.com/l1997i/lim3d;)
  - [ ]  [Instant Domain Augmentation for LiDAR Semantic Segmentation](http://ar5iv.org/abs/2303.14378v1)<br>:house:[project](http://cvlab.postech.ac.kr/research/LiDomAug)
  - [ ]  [Delving into Shape-aware Zero-shot Semantic Segmentation](http://ar5iv.org/abs/2304.08491v1)<br>:star:[code](https://github.com/Liuxinyv/SAZS)
  - [ ]  开放词汇语义分割
    - [ ]  [Open Vocabulary Semantic Segmentation With Patch Aligned Contrastive Learning](https://ar5iv.org/abs/2212.04994)
    - [ ]  [Open-Vocabulary Semantic Segmentation With Mask-Adapted CLIP](https://ar5iv.org/abs/2210.04150)<br>:house:[project](https://jeff-liangf.github.io/projects/ovseg)
    - [ ]  [Side Adapter Network for Open-Vocabulary Semantic Segmentation](https://ar5iv.org/abs/2302.12242)<br>:star:[code](https://github.com/MendelXu/SAN)<br>:thumbsup:[CVPR2023 Highlight | Side Adapter Network – 极致轻薄却性能强劲的开放词汇语义分割器](https://mp.weixin.qq.com/s/yBmBniIMF9JG0RG6GdSQng)
  - [ ]  开放世界语义分割
    - [ ]  [Learning to Generate Text-grounded Mask for Open-world Semantic Segmentation from Only Image-Text Pairs](https://ar5iv.org/abs/2212.00785)<br>:star:[code](https://github.com/kakaobrain/tcl)
  - [ ]  域适应语义分割
    - [ ]  [DiGA: Distil to Generalize and then Adapt for Domain Adaptive Semantic Segmentation](http://ar5iv.org/abs/2304.02222v1)<br>:star:[code](https://github.com/fy-vision/DiGA)
    - [ ]  [Weakly-Supervised Domain Adaptive Semantic Segmentation With Prototypical Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Das_Weakly-Supervised_Domain_Adaptive_Semantic_Segmentation_With_Prototypical_Contrastive_Learning_CVPR_2023_paper.pdf)
    - [ ]  [Continuous Pseudo-Label Rectified Domain Adaptive Semantic Segmentation With Implicit Neural Representations](https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_Continuous_Pseudo-Label_Rectified_Domain_Adaptive_Semantic_Segmentation_With_Implicit_Neural_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ETHRuiGong/IR2F)
  - [ ]  域泛化语义分割
    - [ ]  [HGFormer: Hierarchical Grouping Transformer for Domain Generalized Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dingjiansw101/HGFormer)
    - [ ]  [Style Projected Clustering for Domain Generalized Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Style_Projected_Clustering_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://gitee.com/mindspore/models/tree/master/research/cv/SPC-Net)
  - [ ]  无监督语义分割
     - [ ]  [Leveraging Hidden Positives for Unsupervised Semantic Segmentation](http://ar5iv.org/abs/2303.15014v1)<br>:star:[code](https://github.com/hynnsk/HP)
     - [ ]  [Network-Free, Unsupervised Semantic Segmentation With Synthetic Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Network-Free_Unsupervised_Semantic_Segmentation_With_Synthetic_Images_CVPR_2023_paper.pdf)
  - [ ]  半监督语义分割
    - [ ]  [Conflict-Based Cross-View Consistency for Semi-Supervised Semantic Segmentation](https://ar5iv.org/pdf/2303.01276.pdf)<br>:star:[code](https://github.com/xiaoyao3302/CCVC)  
    - [ ]  [Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation](http://ar5iv.org/abs/2208.09910)
    - [ ]  [Hunting Sparsity: Density-Guided Contrastive Learning for Semi-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Hunting_Sparsity_Density-Guided_Contrastive_Learning_for_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Gavinwxy/DGCL)
    - [ ]  [Instance-Specific and Model-Adaptive Supervision for Semi-Supervised Semantic Segmentation](https://ar5iv.org/abs/2211.11335)
    - [ ]  [LaserMix for Semi-Supervised LiDAR Semantic Segmentation](https://ar5iv.org/abs/2207.00026)<br>:star:[code](https://github.com/ldkong1205/LaserMix)
    - [ ]  [Augmentation Matters: A Simple-Yet-Effective Approach to Semi-Supervised Semantic Segmentation](https://ar5iv.org/abs/2212.04976)
    - [ ]  [Fuzzy Positive Learning for Semi-Supervised Semantic Segmentation](http://ar5iv.org/abs/2210.08519)
  - [ ]  弱监督语义分割
    - [ ]  [Token Contrast for Weakly-Supervised Semantic Segmentation](https://ar5iv.org/pdf/2303.01267.pdf)<br>:star:[code](https://github.com/rulixiang/ToCo)
    - [ ]  [CLIP Is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation](http://ar5iv.org/abs/2212.09506)
    - [ ]  [Boundary-Enhanced Co-Training for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Rong_Boundary-Enhanced_Co-Training_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/ShenghaiRong/BECO)
    - [ ]  [Out-of-Candidate Rectification for Weakly Supervised Semantic Segmentation](https://ar5iv.org/abs/2211.12268)
    - [ ]  [Weakly Supervised Semantic Segmentation via Adversarial Learning of Classifier and Reconstructor](https://openaccess.thecvf.com/content/CVPR2023/papers/Kweon_Weakly_Supervised_Semantic_Segmentation_via_Adversarial_Learning_of_Classifier_and_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sangrockEG/ACR)
  - [ ]  自监督语义分割
    - [ ]  [CLIP-S4: Language-Guided Self-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/He_CLIP-S4_Language-Guided_Self-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf)
  - [ ]  点云语义分割
    - [ ]  [Novel Class Discovery for 3D Point Cloud Semantic Segmentation](https://ar5iv.org/abs/2303.11610)<br>:star:[code](https://github.com/LuigiRiz/NOPS)
  - [ ]  零样本语义分割
    - [ ]  [Delving Into Shape-Aware Zero-Shot Semantic Segmentation](https://ar5iv.org/abs/2304.08491)<br>:star:[code](https://github.com/Liuxinyv/SAZS)
    - [ ]  [ZegCLIP: Towards Adapting CLIP for Zero-Shot Semantic Segmentation](http://ar5iv.org/abs/2212.03588)
  - [ ]  小样本语义分割
    - [ ]  [MIANet: Aggregating Unbiased Instance and General Information for Few-Shot Semantic Segmentation](https://ar5iv.org/abs/2305.13864)<br>:star:[code](https://github.com/Aldrich2y/MIANet)
    - [ ]  [A Strong Baseline for Generalized Few-Shot Semantic Segmentation](https://ar5iv.org/abs/2211.14126)<br>:star:[code](https://github.com/sinahmr/DIaM)
    - [ ]  [Learning Orthogonal Prototypes for Generalized Few-Shot Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Learning_Orthogonal_Prototypes_for_Generalized_Few-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf)
  - [ ]  长尾语义分割
    - [ ]  [Balancing Logit Variation for Long-Tailed Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Balancing_Logit_Variation_for_Long-Tailed_Semantic_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/grantword8/BLV)
  - [ ]  3D 语义分割
    - [ ]  [Seg3D: Multi-modal 3D Semantic Segmentation for Autonomous Driving](https://ar5iv.org/abs/2303.08600)<br>:star:[code](https://github.com/jialeli1/lidarseg3d)
    - [ ]  [3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds](http://ar5iv.org/abs/2304.00690v1)<br>:star:[code](https://github.com/xiaoaoran/SemanticSTF)
  - [ ]  开集语义分割
    - [ ]  [Open-Set Semantic Segmentation for Point Clouds via Adversarial Prototype Framework](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Open-Set_Semantic_Segmentation_for_Point_Clouds_via_Adversarial_Prototype_Framework_CVPR_2023_paper.pdf)
- [ ]  交互式分割
  - [ ]  [Interactive Segmentation as Gaussian Process Classification](https://ar5iv.org/pdf/2302.14578.pdf)<br>:star:[code](https://github.com/zmhhmz/GPCIS_CVPR2023)
  - [ ]  [Interactive Segmentation of Radiance Fields](https://ar5iv.org/abs/2212.13545)<br>:house:[project](https://rahul-goel.github.io/isrf/)
  - [ ]  [Efficient Mask Correction for Click-Based Interactive Image Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Efficient_Mask_Correction_for_Click-Based_Interactive_Image_Segmentation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/feiaxyt/EMC-Click)
- [ ]  小样本分割
  - [ ]  [Hierarchical Dense Correlation Distillation for Few-Shot Segmentation](http://ar5iv.org/abs/2303.14652v1)<br>:star:[code](https://github.com/Pbihao/HDMNet)
  - [ ]  [Rethinking the Correlation in Few-Shot Segmentation: A Buoys View](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Rethinking_the_Correlation_in_Few-Shot_Segmentation_A_Buoys_View_CVPR_2023_paper.pdf)
- [ ]  VSS
  - [ ]  [Efficient Semantic Segmentation by Altering Resolutions for Compressed Videos](https://ar5iv.org/abs/2303.07224)<br>:star:[code](https://github.com/THU-LYJ-Lab/AR-Seg)
  - [ ]  [Simultaneously Short- and Long-Term Temporal Modeling for Semi-Supervised Video Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Lao_Simultaneously_Short-_and_Long-Term_Temporal_Modeling_for_Semi-Supervised_Video_Semantic_CVPR_2023_paper.pdf)
  - [ ]  [Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation](http://ar5iv.org/abs/2303.14361v1)<br>:star:[code](https://github.com/shaoyuanlo/STPL)
- [ ]  VOS
  - [ ]  [InstMove: Instance Motion for Object-centric Video Segmentation](https://ar5iv.org/abs/2303.08132)<br>:star:[code](https://github.com/wjf5203/VNext)
  - [ ]  [Breaking the "Object" in Video Object Segmentation](https://ar5iv.org/abs/2212.06200)
  - [ ]  [Look Before You Match: Instance Understanding Matters in Video Object Segmentation](https://ar5iv.org/abs/2212.06826)
  - [ ]  [MobileVOS: Real-Time Video Object Segmentation Contrastive Learning meets Knowledge Distillation](https://ar5iv.org/abs/2303.07815)
  - [ ]  [Boosting Video Object Segmentation via Space-time Correspondence Learning](http://ar5iv.org/abs/2304.06211v1)<br>:star:[code](https://github.com/wenguanwang/VOS_Correspondence)
  - [ ]  [Bootstrapping Objectness from Videos by Relaxed Common Fate and Visual Grouping](http://ar5iv.org/abs/2304.08025v1)VOS
  - [ ]  [Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation](https://ar5iv.org/abs/2303.10100)<br>:star:[code](https://github.com/0liliulei/Mask-VOS)
  - [ ]  [Two-shot Video Object Segmentation](https://ar5iv.org/abs/2303.12078)<br>:star:[code](https://github.com/yk-pku/Two-shot-Video-Object-Segmentation)
  - [ ]  [Bootstrapping Objectness from Videos by Relaxed Common Fate and Visual Grouping](https://ar5iv.org/abs/2304.08025)
 - [ ]  VIS
   - [ ]  [Mask-Free Video Instance Segmentation](http://ar5iv.org/abs/2303.15904v1)<br>:star:[code](https://github.com/SysCV/MaskFreeVis)<br>:house:[project](http://vis.xyz/pub/maskfreevis)<br>:star:[code](https://github.com/SysCV/MaskFreeVis)
   - [ ]  [MDQE: Mining Discriminative Query Embeddings to Segment Occluded Instances on Challenging Videos](https://ar5iv.org/abs/2303.14395)<br>:star:[code](https://github.com/MinghanLi/MDQE_CVPR2023)
   - [ ]  [A Generalized Framework for Video Instance Segmentation](https://ar5iv.org/abs/2211.08834)<br>:star:[code](https://github.com/miranheo/GenVIS)
- [ ]  场景理解
  - [ ]  [FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding](http://ar5iv.org/abs/2304.02135v1)
  - [ ]  [SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo and Text](https://ar5iv.org/abs/2204.11964)<br>:house:[project](http://www.pinakinathc.me/scenetrilogy)
  - [ ]  [Movies2Scenes: Using Movie Metadata To Learn Scene Representation](http://ar5iv.org/abs/2202.10650)
  - [ ]  [Seeing With Sound: Long-range Acoustic Beamforming for Multimodal Scene Understanding](https://openaccess.thecvf.com/content/CVPR2023/papers/Chakravarthula_Seeing_With_Sound_Long-range_Acoustic_Beamforming_for_Multimodal_Scene_Understanding_CVPR_2023_paper.pdf)
  - [ ]  [Single View Scene Scale Estimation Using Scale Field](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf)
  - [ ]  [Neural Part Priors: Learning To Optimize Part-Based Object Completion in RGB-D Scans](https://openaccess.thecvf.com/content/CVPR2023/papers/Bokhovkin_Neural_Part_Priors_Learning_To_Optimize_Part-Based_Object_Completion_in_CVPR_2023_paper.pdf)
  - [ ]  3D 场景理解
    - [ ]  [OpenScene: 3D Scene Understanding With Open Vocabularies](http://ar5iv.org/abs/2211.15654)
    - [ ]  [Long Range Pooling for 3D Large-Scale Scene Understanding](https://ar5iv.org/abs/2301.06962)
    - [ ]  [Panoptic Lifting for 3D Scene Understanding With Neural Fields](https://openaccess.thecvf.com/content/CVPR2023/papers/Siddiqui_Panoptic_Lifting_for_3D_Scene_Understanding_With_Neural_Fields_CVPR_2023_paper.pdf)<br>:house:[project](nihalsid.github.io/panoptic-lifting/)
    - [ ]  [FAC: 3D Representation Learning via Foreground Aware Feature Contrast](https://ar5iv.org/abs/2303.06388)
    - [ ]  [Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding](http://ar5iv.org/abs/2305.05026v1)
    - [ ]  [CLIP2Scene: Towards Label-Efficient 3D Scene Understanding by CLIP](https://ar5iv.org/abs/2301.04926)<br>:star:[code](https://github.com/runnanchen/CLIP2Scene)
    - [ ]  [PLA:Language-driven Open-Vocabulary 3D Scene Understanding](https://ar5iv.org/pdf/2211.16312.pdf)<br>:star:[code](https://github.com/CVMI-Lab/PLA)<br>:house:[project](https://dingry.github.io/projects/PLA.html)
    - [ ]  [MM-3DScene: 3D Scene Understanding by Customizing Masked Modeling With Informative-Preserved Reconstruction and Self-Distilled Consistency](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_MM-3DScene_3D_Scene_Understanding_by_Customizing_Masked_Modeling_With_Informative-Preserved_CVPR_2023_paper.pdf)
- [ ]  抠图
  - [ ]  [Adaptive Human Matting for Dynamic Videos](http://ar5iv.org/abs/2304.06018v1)<br>:star:[code](https://github.com/microsoft/AdaM)
  - [ ]  [Mask-Guided Matting in the Wild](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Mask-Guided_Matting_in_the_Wild_CVPR_2023_paper.pdf)
  - [ ]  [Ultrahigh Resolution Image/Video Matting With Spatio-Temporal Sparsity](https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Ultrahigh_Resolution_ImageVideo_Matting_With_Spatio-Temporal_Sparsity_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/nowsyn/SparseMat.git)
  - [ ]  [End-to-End Video Matting With Trimap Propagation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_End-to-End_Video_Matting_With_Trimap_Propagation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/csvt32745/FTP-VM)
  - [ ]  [Referring Image Matting](https://ar5iv.org/abs/2206.05149)<br>:star:[code](https://github.com/JizhiziLi/RIM)
- [ ]  指代图像分割
  - [ ]  [PolyFormer: Referring Image Segmentation As Sequential Polygon Generation](https://ar5iv.org/abs/2302.07387)<br>:house:[project](https://polyformer.github.io/)
  - [ ]  [Zero-shot Referring Image Segmentation with Global-Local Context Features](http://ar5iv.org/abs/2303.17811v1)<br>:star:[code](https://github.com/Seonghoon-Yu/Zero-shot-RIS)
  - [ ]  [Contrastive Grouping With Transformer for Referring Image Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Contrastive_Grouping_With_Transformer_for_Referring_Image_Segmentation_CVPR_2023_paper.pdf)
- [ ]  引用表达分割  
  - [ ]  [GRES: Generalized Referring Expression Segmentation](https://ar5iv.org/abs/2306.00968)<br>:house:[project](https://henghuiding.github.io/GRES/)<br>:thumbsup:[CVPR23 Highlight 多模态新任务、新数据集：NTU提出广义引用分割问题GRES](https://mp.weixin.qq.com/s/YoL_8a_8OPHovFrfJSXm4A)
  - [ ]  [Meta Compositional Referring Expression Segmentation](http://ar5iv.org/abs/2304.04415v1)
  - [ ]  [Learning to Segment Every Referring Object Point by Point](https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Learning_To_Segment_Every_Referring_Object_Point_by_Point_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/qumengxue/Partial-RES.git.)
- [ ]  运动分割
  - [ ]  [Unsupervised Space-Time Network for Temporally-Consistent Segmentation of Multiple Motions](https://openaccess.thecvf.com/content/CVPR2023/papers/Meunier_Unsupervised_Space-Time_Network_for_Temporally-Consistent_Segmentation_of_Multiple_Motions_CVPR_2023_paper.pdf)
- [ ]  视频分割
  - [ ]  [TarViS: A Unified Approach for Target-Based Video Segmentation](https://ar5iv.org/abs/2301.02657)<br>:star:[code](https://github.com/Ali2500/TarViS)
  - [ ]  [Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation](http://ar5iv.org/abs/2303.10100)
- [ ]  动作分割
  - [ ]  [ASPnet: Action Segmentation With Shared-Private Representation of Multiple Data Sources](https://openaccess.thecvf.com/content/CVPR2023/papers/van_Amsterdam_ASPnet_Action_Segmentation_With_Shared-Private_Representation_of_Multiple_Data_Sources_CVPR_2023_paper.pdf)
  - [ ]  [Reducing the Label Bias for Timestamp Supervised Temporal Action Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Reducing_the_Label_Bias_for_Timestamp_Supervised_Temporal_Action_Segmentation_CVPR_2023_paper.pdf)

<a name="1"/>

## 1.other(其它,待分类)
- [ ]  [CIRCLE: Capture in Rich Contextual Environments](https://openaccess.thecvf.com/content/CVPR2023/papers/Araujo_CIRCLE_Capture_in_Rich_Contextual_Environments_CVPR_2023_paper.pdf)
- [ ]  [Trainable Projected Gradient Method for Robust Fine-Tuning](http://ar5iv.org/abs/2303.10720)
- [ ]  [HDR Imaging With Spatially Varying Signal-to-Noise Ratios](http://ar5iv.org/abs/2303.17253)
- [ ]  [Are Deep Neural Networks SMARTer Than Second Graders?](http://ar5iv.org/abs/2212.09993)
- [ ]  [Blowing in the Wind: CycleNet for Human Cinemagraphs From Still Images](http://ar5iv.org/abs/2303.08639)
- [ ]  [Uncertainty-Aware Vision-Based Metric Cross-View Geolocalization](http://ar5iv.org/abs/2211.12145)
- [ ]  [pCON: Polarimetric Coordinate Networks for Neural Scene Representations](https://openaccess.thecvf.com/content/CVPR2023/papers/Peters_pCON_Polarimetric_Coordinate_Networks_for_Neural_Scene_Representations_CVPR_2023_paper.pdf)
- [ ]  [Two-Stage Co-Segmentation Network Based on Discriminative Representation for Recovering Human Mesh From Videos](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Two-Stage_Co-Segmentation_Network_Based_on_Discriminative_Representation_for_Recovering_Human_CVPR_2023_paper.pdf)
- [ ]  [Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate](http://ar5iv.org/abs/2304.00049)
- [ ]  [Implicit View-Time Interpolation of Stereo Videos Using Multi-Plane Disparities and Non-Uniform Coordinates](http://ar5iv.org/abs/2303.17181)
- [ ]  [LayoutFormer++: Conditional Graphic Layout Generation via Constraint Serialization and Decoding Space Restriction](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_LayoutFormer_Conditional_Graphic_Layout_Generation_via_Constraint_Serialization_and_Decoding_CVPR_2023_paper.pdf)
- [ ]  [Stare at What You See: Masked Image Modeling Without Reconstruction](http://ar5iv.org/abs/2211.08887)
- [ ]  [Neural Kaleidoscopic Space Sculpting](https://openaccess.thecvf.com/content/CVPR2023/papers/Ahn_Neural_Kaleidoscopic_Space_Sculpting_CVPR_2023_paper.pdf)
- [ ]  [HyperCUT: Video Sequence From a Single Blurry Image Using Unsupervised Ordering](http://ar5iv.org/abs/2304.01686)
- [ ]  [Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders](https://openaccess.thecvf.com/content/CVPR2023/papers/Sha_Cant_Steal_Cont-Steal_Contrastive_Stealing_Attacks_Against_Image_Encoders_CVPR_2023_paper.pdf)
- [ ]  [Edges to Shapes to Concepts: Adversarial Augmentation for Robust Vision](https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_Edges_to_Shapes_to_Concepts_Adversarial_Augmentation_for_Robust_Vision_CVPR_2023_paper.pdf)
- [ ]  [Improved Distribution Matching for Dataset Condensation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Improved_Distribution_Matching_for_Dataset_Condensation_CVPR_2023_paper.pdf)
- [ ]  [Slimmable Dataset Condensation](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Slimmable_Dataset_Condensation_CVPR_2023_paper.pdf)
- [ ]  [LEGO-Net: Learning Regular Rearrangements of Objects in Rooms](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_LEGO-Net_Learning_Regular_Rearrangements_of_Objects_in_Rooms_CVPR_2023_paper.pdf)
- [ ]  [Neuralizer: General Neuroimage Analysis Without Re-Training](http://ar5iv.org/abs/2305.02644)
- [ ]  [DETRs With Hybrid Matching](https://ar5iv.org/abs/2207.13080)<br>:star:[code](https://github.com/HDETR)
- [ ]  [A Rotation-Translation-Decoupled Solution for Robust and Efficient Visual-Inertial Initialization](https://openaccess.thecvf.com/content/CVPR2023/papers/He_A_Rotation-Translation-Decoupled_Solution_for_Robust_and_Efficient_Visual-Inertial_Initialization_CVPR_2023_paper.pdf)
- [ ]  [A-La-Carte Prompt Tuning (APT): Combining Distinct Data via Composable Prompting](https://openaccess.thecvf.com/content/CVPR2023/papers/Bowman_A-La-Carte_Prompt_Tuning_APT_Combining_Distinct_Data_via_Composable_Prompting_CVPR_2023_paper.pdf)
- [ ]  [Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction](http://ar5iv.org/abs/2301.10034)
- [ ]  [Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery From Sparse Image Ensemble](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Hi-LASSIE_High-Fidelity_Articulated_Shape_and_Skeleton_Discovery_From_Sparse_Image_CVPR_2023_paper.pdf)
- [ ]  [Decentralized Learning With Multi-Headed Distillation](http://ar5iv.org/abs/2211.15774)
- [ ]  [On the Convergence of IRLS and Its Variants in Outlier-Robust Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_On_the_Convergence_of_IRLS_and_Its_Variants_in_Outlier-Robust_CVPR_2023_paper.pdf)
- [ ]  [Learning Joint Latent Space EBM Prior Model for Multi-Layer Generator](https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Learning_Joint_Latent_Space_EBM_Prior_Model_for_Multi-Layer_Generator_CVPR_2023_paper.pdf)
- [ ]  [Knowledge Combination To Learn Rotated Detection Without Rotated Annotation](http://ar5iv.org/abs/2304.02199)
- [ ]  [FlowGrad: Controlling the Output of Generative ODEs With Gradients](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_FlowGrad_Controlling_the_Output_of_Generative_ODEs_With_Gradients_CVPR_2023_paper.pdf)
- [ ]  [Recurrent Homography Estimation Using Homography-Guided Image Warping and Focus Transformer](https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Recurrent_Homography_Estimation_Using_Homography-Guided_Image_Warping_and_Focus_Transformer_CVPR_2023_paper.pdf)
- [ ]  [Multi-View Inverse Rendering for Large-Scale Real-World Indoor Scenes](http://ar5iv.org/abs/2211.10206)
- [ ]  [Boosting Transductive Few-Shot Fine-Tuning With Margin-Based Uncertainty Weighting and Probability Regularization](https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_Boosting_Transductive_Few-Shot_Fine-Tuning_With_Margin-Based_Uncertainty_Weighting_and_Probability_CVPR_2023_paper.pdf)
- [ ]  [BiasAdv: Bias-Adversarial Augmentation for Model Debiasing](https://openaccess.thecvf.com/content/CVPR2023/papers/Lim_BiasAdv_Bias-Adversarial_Augmentation_for_Model_Debiasing_CVPR_2023_paper.pdf)
- [ ]  [CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion](http://ar5iv.org/abs/2211.14461)
- [ ]  [Why Is the Winner the Best?](http://ar5iv.org/abs/2303.17719)
- [ ]  [HGNet: Learning Hierarchical Geometry From Points, Edges, and Surfaces](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_HGNet_Learning_Hierarchical_Geometry_From_Points_Edges_and_Surfaces_CVPR_2023_paper.pdf)
- [ ]  [Revisiting the P3P Problem](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Revisiting_the_P3P_Problem_CVPR_2023_paper.pdf)
- [ ]  [RiDDLE: Reversible and Diversified De-Identification With Latent Encryptor](https://ar5iv.org/abs/2303.05171)
- [ ]  [BASiS: Batch Aligned Spectral Embedding Space](https://ar5iv.org/abs/2211.16960)
- [ ]  [CRAFT: Concept Recursive Activation FacTorization for Explainability](https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_CRAFT_Concept_Recursive_Activation_FacTorization_for_Explainability_CVPR_2023_paper.pdf)
- [ ]  [Infinite Photorealistic Worlds using Procedural Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Raistrick_Infinite_Photorealistic_Worlds_Using_Procedural_Generation_CVPR_2023_paper.pdf)
- [ ]  [All-in-Focus Imaging From Event Focal Stack](https://openaccess.thecvf.com/content/CVPR2023/papers/Lou_All-in-Focus_Imaging_From_Event_Focal_Stack_CVPR_2023_paper.pdf)
- [ ]  [Learning 3D Scene Priors With 2D Supervision](http://ar5iv.org/abs/2211.14157)
- [ ]  [NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation](http://ar5iv.org/abs/2212.00613)
- [ ]  [CLIPPO: Image-and-Language Understanding from Pixels Only](https://ar5iv.org/abs/2212.08045)<br>:star:[code](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/clippo/README.md)
- [ ]  [Towards Bridging the Performance Gaps of Joint Energy-Based Models](http://ar5iv.org/abs/2209.07959)
- [ ]  [expOSE: Accurate Initialization-Free Projective Factorization Using Exponential Regularization](https://openaccess.thecvf.com/content/CVPR2023/papers/Iglesias_expOSE_Accurate_Initialization-Free_Projective_Factorization_Using_Exponential_Regularization_CVPR_2023_paper.pdf)
- [ ]  [Learning Debiased Representations via Conditional Attribute Interpolation](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Debiased_Representations_via_Conditional_Attribute_Interpolation_CVPR_2023_paper.pdf)
- [ ]  [Learning Neural Volumetric Representations of Dynamic Humans in Minutes](http://ar5iv.org/abs/2302.12237)
- [ ]  [Bayesian Posterior Approximation With Stochastic Ensembles](http://ar5iv.org/abs/2212.08123)
- [ ]  [RILS: Masked Visual Reconstruction in Language Semantic Space](http://ar5iv.org/abs/2301.06958)
- [ ]  [RepMode: Learning to Re-Parameterize Diverse Experts for Subcellular Structure Prediction](https://ar5iv.org/abs/2212.10066)
- [ ]  [Zero-Shot Model Diagnosis](https://ar5iv.org/abs/2303.15441)
- [ ]  [Improving Visual Grounding by Encouraging Consistent Gradient-Based Explanations](https://ar5iv.org/abs/2206.15462)<br>:star:[code](https://github.com/uvavision/AMC-grounding)
- [ ]  [AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning With Masked Autoencoders](http://ar5iv.org/abs/2211.09120)
- [ ]  [Understanding and Improving Visual Prompting: A Label-Mapping Perspective](http://ar5iv.org/abs/2211.11635)
- [ ]  [DegAE: A New Pretraining Paradigm for Low-Level Vision](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_DegAE_A_New_Pretraining_Paradigm_for_Low-Level_Vision_CVPR_2023_paper.pdf)
- [ ]  [LiDAR-in-the-Loop Hyperparameter Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Goudreault_LiDAR-in-the-Loop_Hyperparameter_Optimization_CVPR_2023_paper.pdf)
- [ ]  [Understanding Deep Generative Models With Generalized Empirical Likelihoods](https://openaccess.thecvf.com/content/CVPR2023/papers/Ravuri_Understanding_Deep_Generative_Models_With_Generalized_Empirical_Likelihoods_CVPR_2023_paper.pdf)
- [ ]  [Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning](https://ar5iv.org/abs/2212.03220)
- [ ]  [Compressing Volumetric Radiance Fields to 1 MB](https://ar5iv.org/abs/2211.16386)<br>:star:[code](https://github.com/AlgoHunt/VQRF)
- [ ]  [Label Information Bottleneck for Label Enhancement](https://ar5iv.org/abs/2303.06836)<br>:star:[code](https://github.com/qinghai-zheng/LIBLE)
- [ ]  [DNF: Decouple and Feedback Network for Seeing in the Dark](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_DNF_Decouple_and_Feedback_Network_for_Seeing_in_the_Dark_CVPR_2023_paper.pdf)
- [ ]  [Cloud-Device Collaborative Adaptation to Continual Changing Environments in the Real-World](https://ar5iv.org/abs/2212.00972)
- [ ]  [How To Prevent the Continuous Damage of Noises To Model Training?](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_How_To_Prevent_the_Continuous_Damage_of_Noises_To_Model_CVPR_2023_paper.pdf)
- [ ]  [ActMAD: Activation Matching To Align Distributions for Test-Time-Training](https://ar5iv.org/abs/2211.12870)<br>:house:[project](https://jmiemirza.github.io/ActMAD/)
- [ ]  [Leveraging Temporal Context in Low Representational Power Regimes](https://openaccess.thecvf.com/content/CVPR2023/papers/Fosco_Leveraging_Temporal_Context_in_Low_Representational_Power_Regimes_CVPR_2023_paper.pdf)<br>:house:[project](https://camilofosco.com/etm_website)
- [ ]  [Guided Recommendation for Model Fine-Tuning](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Guided_Recommendation_for_Model_Fine-Tuning_CVPR_2023_paper.pdf)
- [ ]  [OT-Filter: An Optimal Transport Filter for Learning With Noisy Labels](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_OT-Filter_An_Optimal_Transport_Filter_for_Learning_With_Noisy_Labels_CVPR_2023_paper.pdf)
- [ ]  [E2PN: Efficient SE(3)-Equivariant Point Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_E2PN_Efficient_SE3-Equivariant_Point_Network_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/minghanz/E2PN)
- [ ]  [Understanding Masked Image Modeling via Learning Occlusion Invariant Feature](https://ar5iv.org/abs/2208.04164)
- [ ]  [Fine-Tuned CLIP Models Are Efficient Video Learners](https://ar5iv.org/abs/2212.03640)<br>:star:[code](https://github.com/muzairkhattak/ViFi-CLIP)
- [ ]  [Visual Recognition by Request](https://ar5iv.org/abs/2207.14227)
- [ ]  [Stitchable Neural Networks](https://ar5iv.org/abs/2302.06586)<br>:house:[project](https://snnet.github.io/)
- [ ]  [RUST: Latent Neural Scene Representations From Unposed Imagery](https://ar5iv.org/abs/2211.14306)<br>:star:[code](https://rust-paper.github.io/)
- [ ]  [Spatio-Focal Bidirectional Disparity Estimation From a Dual-Pixel Image](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Spatio-Focal_Bidirectional_Disparity_Estimation_From_a_Dual-Pixel_Image_CVPR_2023_paper.pdf)
- [ ]  [Four-View Geometry With Unknown Radial Distortion](https://openaccess.thecvf.com/content/CVPR2023/papers/Hruby_Four-View_Geometry_With_Unknown_Radial_Distortion_CVPR_2023_paper.pdf)
- [ ]  [Learning Optical Expansion From Scale Matching](https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_Learning_Optical_Expansion_From_Scale_Matching_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/HanLingsgjk/TPCV)
- [ ]  [Don't Lie to Me! Robust and Efficient Explainability With Verified Perturbation Analysis](https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_Dont_Lie_to_Me_Robust_and_Efficient_Explainability_With_Verified_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/deel-ai/formal-explainability)
- [ ]  [Learning Transformation-Predictive Representations for Detection and Description of Local Features](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_Transformation-Predictive_Representations_for_Detection_and_Description_of_Local_Features_CVPR_2023_paper.pdf)
- [ ]  [Two-Way Multi-Label Loss](https://openaccess.thecvf.com/content/CVPR2023/papers/Kobayashi_Two-Way_Multi-Label_Loss_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tk1980/TwowayMultiLabelLoss)
- [ ]  [Where is my Wallet? Modeling Object Proposal Sets for Egocentric Visual Query Localization](https://ar5iv.org/abs/2211.10528)<br>:star:[code](https://github.com/facebookresearch/vq2d_cvpr)
- [ ]  [Dionysus: Recovering Scene Structures by Dividing Into Semantic Pieces](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dionysus_Recovering_Scene_Structures_by_Dividing_Into_Semantic_Pieces_CVPR_2023_paper.pdf)
- [ ]  [Noisy Correspondence Learning With Meta Similarity Correction](https://ar5iv.org/abs/2304.06275)
- [ ]  [HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics](https://ar5iv.org/abs/2212.07242)
- [ ]  [Modeling Entities As Semantic Points for Visual Information Extraction in the Wild](https://ar5iv.org/abs/2303.13095)<br>:house:[project](https://www.modelscope.cn/datasets/damo/SIBR/summary)
- [ ]  [NeAT: Learning Neural Implicit Surfaces With Arbitrary Topologies From Multi-View Images](https://ar5iv.org/abs/2303.12012)
- [ ]  [Learning a Deep Color Difference Metric for Photographic Images](https://ar5iv.org/abs/2303.14964)
- [ ]  [DINN360: Deformable Invertible Neural Network for Latitude-Aware 360deg Image Rescaling](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_DINN360_Deformable_Invertible_Neural_Network_for_Latitude-Aware_360deg_Image_Rescaling_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/gyc9709/DINN360)
- [ ]  [Finetune Like You Pretrain: Improved Finetuning of Zero-Shot Vision Models](https://ar5iv.org/abs/2212.00638)<br>:star:[code](https://github.com/locuslab/FLYP)
- [ ]  [Learning a Practical SDR-to-HDRTV Up-Conversion Using New Dataset and Degradation Models](https://ar5iv.org/abs/2303.13031)<br>:star:[code](https://github.com/AndreGuo/HDRTVDM)
- [ ]  [DynaFed: Tackling Client Data Heterogeneity With Global Dynamics](https://ar5iv.org/abs/2211.10878)
- [ ]  [CUF: Continuous Upsampling Filters](https://ar5iv.org/abs/2210.06965)
- [ ]  [Learning Decorrelated Representations Efficiently Using Fast Fourier Transform](https://ar5iv.org/abs/2301.01569)
- [ ]  [Practical Network Acceleration With Tiny Sets](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Practical_Network_Acceleration_With_Tiny_Sets_CVPR_2023_paper.pdf)
- [ ]  [AstroNet: When Astrocyte Meets Artificial Neural Network](https://openaccess.thecvf.com/content/CVPR2023/papers/Han_AstroNet_When_Astrocyte_Meets_Artificial_Neural_Network_CVPR_2023_paper.pdf)
- [ ]  [NeuralLift-360: Lifting an In-the-Wild 2D Photo to a 3D Object With 360deg Views](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.pdf)<br>:star:[code](https://vita-group.github.io/NeuralLift-360/)
- [ ]  [Command-Driven Articulated Object Understanding and Manipulation](https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_Command-Driven_Articulated_Object_Understanding_and_Manipulation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/dvlab-research/Cart)
- [ ]  [HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes With Iterative Intertwined Regularization](https://ar5iv.org/abs/2302.14340)<br>:star:[code](https://github.com/Gorilla-Lab-SCUT/HelixSurf)
- [ ]  [Joint Appearance and Motion Learning for Efficient Rolling Shutter Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_Joint_Appearance_and_Motion_Learning_for_Efficient_Rolling_Shutter_Correction_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/GitCVfb/JAMNet)
- [ ]  [Gradient-Based Uncertainty Attribution for Explainable Bayesian Deep Learning](https://ar5iv.org/abs/2304.04824)
- [ ]  [Class Adaptive Network Calibration](https://ar5iv.org/abs/2211.15088)<br>:star:[code](https://github.com/by-liu/CALS)
- [ ]  [OCTET: Object-Aware Counterfactual Explanations](https://ar5iv.org/abs/2211.12380)<br>:star:[code](https://github.com/valeoai/OCTET)
- [ ]  [DNeRV: Modeling Inherent Dynamics via Difference Neural Representation for Videos](https://ar5iv.org/abs/2304.06544)
- [ ]  [FFF: Fragment-Guided Flexible Fitting for Building Complete Protein Structures](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_FFF_Fragment-Guided_Flexible_Fitting_for_Building_Complete_Protein_Structures_CVPR_2023_paper.pdf)
- [ ]  [Open-Set Representation Learning Through Combinatorial Embedding](https://ar5iv.org/abs/2106.15278)
- [ ]  [A Unified HDR Imaging Method With Pixel and Patch Level](https://ar5iv.org/abs/2304.06943)
- [ ]  [Accelerated Coordinate Encoding: Learning to Relocalize in Minutes Using RGB and Poses](https://openaccess.thecvf.com/content/CVPR2023/papers/Brachmann_Accelerated_Coordinate_Encoding_Learning_to_Relocalize_in_Minutes_Using_RGB_CVPR_2023_paper.pdf)<br>:star:[code](https://nianticlabs.github.io/ace)
- [ ]  [Switchable Representation Learning Framework With Self-Compatibility](https://ar5iv.org/abs/2206.08289)
- [ ]  [Exploring and Utilizing Pattern Imbalance](https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Exploring_and_Utilizing_Pattern_Imbalance_CVPR_2023_paper.pdf)
- [ ]  [Top-Down Visual Attention From Analysis by Synthesis](https://ar5iv.org/abs/2303.13043)<br>:house:[project](https://sites.google.com/view/absvit)
- [ ]  [Interactive Cartoonization With Controllable Perceptual Factors](https://ar5iv.org/abs/2212.09555)
- [ ]  [Regularize Implicit Neural Representation by Itself](https://ar5iv.org/abs/2303.15484)
- [ ]  [Delving Into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling](https://ar5iv.org/abs/2304.03937)
- [ ]  [Re-Basin via Implicit Sinkhorn Differentiation](https://openaccess.thecvf.com/content/CVPR2023/papers/Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/fagp/sinkhorn-rebasin)
- [ ]  [Towards Effective Visual Representations for Partial-Label Learning](https://ar5iv.org/abs/2305.06080)
- [ ]  [Samples With Low Loss Curvature Improve Data Efficiency](https://openaccess.thecvf.com/content/CVPR2023/papers/Garg_Samples_With_Low_Loss_Curvature_Improve_Data_Efficiency_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/isha-garg/SLo-Curves)
- [ ]  [Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares](https://openaccess.thecvf.com/content/CVPR2023/papers/Muhle_Learning_Correspondence_Uncertainty_via_Differentiable_Nonlinear_Least_Squares_CVPR_2023_paper.pdf)
- [ ]  [Tunable Convolutions With Parametric Multi-Loss Optimization](https://ar5iv.org/abs/2304.00898)
- [ ]  [RelightableHands: Efficient Neural Relighting of Articulated Hand Models](https://ar5iv.org/abs/2302.04866)<br>:house:[project](https://sh8.io/#/relightable_hands)
- [ ]  [DyNCA: Real-Time Dynamic Texture Synthesis Using Neural Cellular Automata](https://ar5iv.org/abs/2211.11417)<br>:house:[project](https://dynca.github.io/)
- [ ]  [Token Turing Machines](https://ar5iv.org/abs/2211.09119)<br>:star:[code](https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing)
- [ ]  [Probabilistic Debiasing of Scene Graphs](https://ar5iv.org/abs/2211.06444)<br>:star:[code](https://github.com/bashirulazam/within-triplet-debias)
- [ ]  [Few-Shot Non-Line-of-Sight Imaging With Signal-Surface Collaborative Regularization](https://ar5iv.org/abs/2211.15367)
- [ ]  [The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_The_Dark_Side_of_Dynamic_Routing_Neural_Networks_Towards_Efficiency_CVPR_2023_paper.pdf)
- [ ]  [Generalized Decoding for Pixel, Image, and Language](https://ar5iv.org/abs/2212.11270)<br>:house:[project](https://x-decoder-vl.github.io/)
- [ ]  [EC2: Emergent Communication for Embodied Control](https://openaccess.thecvf.com/content/CVPR2023/papers/Mu_EC2_Emergent_Communication_for_Embodied_Control_CVPR_2023_paper.pdf)
- [ ]  [Generalizable Local Feature Pre-Training for Deformable Shape Analysis](https://ar5iv.org/abs/2303.15104)<br>:star:[code](https://github.com/pvnieo/vader)
- [ ]  [On-the-Fly Category Discovery](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_On-the-Fly_Category_Discovery_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/PRIS-CV/On-the-fly-Category-Discovery)
- [ ]  [PyramidFlow: High-Resolution Defect Contrastive Localization Using Pyramid Normalizing Flow](https://ar5iv.org/abs/2303.02595)
- [ ]  [Efficient Verification of Neural Networks Against LVM-Based Specifications](https://openaccess.thecvf.com/content/CVPR2023/papers/Hanspal_Efficient_Verification_of_Neural_Networks_Against_LVM-Based_Specifications_CVPR_2023_paper.pdf)
- [ ]  [TensoIR: Tensorial Inverse Rendering](https://ar5iv.org/abs/2304.12461)<br>:house:[project](https://haian-jin.github.io/TensoIR)
- [ ]  [Learning From Unique Perspectives: User-Aware Saliency Modeling](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf)
- [ ]  [LargeKernel3D: Scaling Up Kernels in 3D Sparse CNNs](https://ar5iv.org/abs/2206.10555)<br>:star:[code](https://github.com/dvlab-research/LargeKernel3D)
- [ ]  [Learning Transferable Spatiotemporal Representations From Natural Script Knowledge](https://ar5iv.org/abs/2209.15280)<br>:star:[code](https://github.com/TencentARC/TVTS)
- [ ]  [FFCV: Accelerating Training by Removing Data Bottlenecks](https://openaccess.thecvf.com/content/CVPR2023/papers/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.pdf)<br>:house:[project](https://ffcv.io/)
- [ ]  [Semidefinite Relaxations for Robust Multiview Triangulation](https://openaccess.thecvf.com/content/CVPR2023/papers/Harenstam-Nielsen_Semidefinite_Relaxations_for_Robust_Multiview_Triangulation_CVPR_2023_paper.pdf)
- [ ]  [GradICON: Approximate Diffeomorphisms via Gradient Inverse Consistency](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_GradICON_Approximate_Diffeomorphisms_via_Gradient_Inverse_Consistency_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/uncbiag/ICON)
- [ ]  [Polynomial Implicit Neural Representations for Large Diverse Datasets](https://ar5iv.org/abs/2303.11424)<br>:star:[code](https://github.com/Rajhans0/Poly_INR)
- [ ]  [Back to the Source: Diffusion-Driven Adaptation To Test-Time Corruption](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Back_to_the_Source_Diffusion-Driven_Adaptation_To_Test-Time_Corruption_CVPR_2023_paper.pdf)
- [ ]  [Learning To Zoom and Unzoom](https://ar5iv.org/abs/2303.15390)<br>:house:[project](https://tchittesh.github.io/lzu/)
- [ ]  [Masked Image Modeling With Local Multi-Scale Reconstruction](https://ar5iv.org/abs/2303.05251)
- [ ]  [Neural Vector Fields: Implicit Representation by Explicit Learning](https://ar5iv.org/abs/2303.04341)<br>:star:[code](https://github.com/Wi-sc/NVF)
- [ ]  [Rate Gradient Approximation Attack Threats Deep Spiking Neural Networks](https://openaccess.thecvf.com/content/CVPR2023/papers/Bu_Rate_Gradient_Approximation_Attack_Threats_Deep_Spiking_Neural_Networks_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/putshua/SNN_attack_RGA)
- [ ]  [Critical Learning Periods for Multisensory Integration in Deep Networks](https://ar5iv.org/abs/2210.04643)
- [ ]  [Imitation Learning as State Matching via Differentiable Physics](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Imitation_Learning_As_State_Matching_via_Differentiable_Physics_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sail-sg/ILD)
- [ ]  [Probing Sentiment-Oriented Pre-Training Inspired by Human Sentiment Perception Mechanism](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Probing_Sentiment-Oriented_Pre-Training_Inspired_by_Human_Sentiment_Perception_Mechanism_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tinglyfeng/sentiment_pretraining)
- [ ]  [Relightable Neural Human Assets From Multi-View Gradient Illuminations](https://ar5iv.org/abs/2212.07648)<br>:star:[code](https://miaoing.github.io/RNHA)
- [ ]  [DINER: Disorder-Invariant Implicit Neural Representation](https://ar5iv.org/abs/2211.07871)
- [ ]  [Robust Mean Teacher for Continual and Gradual Test-Time Adaptation](https://openaccess.thecvf.com/content/CVPR2023/papers/Dobler_Robust_Mean_Teacher_for_Continual_and_Gradual_Test-Time_Adaptation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/mariodoebler/test-time-adaptation)
- [ ]  [A Probabilistic Framework for Lifelong Test-Time Adaptation](https://ar5iv.org/abs/2212.09713)<br>:star:[code](https://github.com/dhanajitb/petal)
- [ ]  [Probing Neural Representations of Scene Perception in a Hippocampally Dependent Task Using Artificial Neural Networks](https://ar5iv.org/abs/2303.06367)
- [ ]  [Decoupling Human and Camera Motion From Videos in the Wild](https://ar5iv.org/abs/2302.12827)<br>:house:[project](https://vye16.github.io/slahmr)
- [ ]  [DISC: Learning From Noisy Labels via Dynamic Instance-Specific Selection and Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DISC_Learning_From_Noisy_Labels_via_Dynamic_Instance-Specific_Selection_and_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/JackYFL/DISC)
- [ ]  [DC2: Dual-Camera Defocus Control by Learning To Refocus](https://openaccess.thecvf.com/content/CVPR2023/papers/Alzayer_DC2_Dual-Camera_Defocus_Control_by_Learning_To_Refocus_CVPR_2023_paper.pdf)
- [ ]  [FJMP: Factorized Joint Multi-Agent Motion Prediction over Learned Directed Acyclic Interaction Graphs](https://ar5iv.org/abs/2211.16197)
- [ ]  ["Seeing" Electric Network Frequency From Events](https://ar5iv.org/abs/2305.02597)<br>:house:[project](https://xlx-creater.github.io/E-ENF)
- [ ]  [Confidential and Private Decentralized Learning Based on Encryption-Friendly Distillation Loss](https://openaccess.thecvf.com/content/CVPR2023/papers/Tastan_CaPriDe_Learning_Confidential_and_Private_Decentralized_Learning_Based_on_Encryption-Friendly_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/tnurbek/capride-learning)
- [ ]  [Revealing the Dark Secrets of Masked Image Modeling](https://ar5iv.org/abs/2205.13543)
- [ ]  [RIFormer: Keep Your Vision Backbone Effective but Removing Token Mixer](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_RIFormer_Keep_Your_Vision_Backbone_Effective_but_Removing_Token_Mixer_CVPR_2023_paper.pdf)
- [ ]  [Adaptive Graph Convolutional Subspace Clustering](https://ar5iv.org/abs/2305.03414)
- [ ]  [Graph Representation for Order-Aware Visual Transformation](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_Graph_Representation_for_Order-Aware_Visual_Transformation_CVPR_2023_paper.pdf)
- [ ]  [Train-Once-for-All Personalization](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf)
- [ ]  [Learning Sample Relationship for Exposure Correction](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Learning_Sample_Relationship_for_Exposure_Correction_CVPR_2023_paper.pdf)
- [ ]  [EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata](https://ar5iv.org/abs/2301.04647)<br>:house:[project](http://hellomuffin.github.io/exif-as-language)
- [ ]  [Gradient norm aware minimization seeks first-order flatness and improves generalization](https://ar5iv.org/abs/2303.03108)<br>:star:[code](https://github.com/xxgege/GAM)<br>:thumbsup:[CVPR2023｜清华大学提出GAM：神经网络“一阶平滑优化器”，显著提升模型“泛化能力”](https://mp.weixin.qq.com/s/TJ5tX1jOPAY_9S2KvgMOsA)
- [ ]  [EXIF As Language: Learning Cross-Modal Associations Between Images and Camera Metadata](https://ar5iv.org/abs/2301.04647)<br>:house:[project](http://hellomuffin.github.io/exif-as-language)
- [ ]  [InstantAvatar: Learning Avatars From Monocular Video in 60 Seconds](https://ar5iv.org/abs/2212.10550)
- [ ]  [GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts](https://ar5iv.org/abs/2211.05272)
- [ ]  [Deep Deterministic Uncertainty: A New Simple Baseline](https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Deep_Deterministic_Uncertainty_A_New_Simple_Baseline_CVPR_2023_paper.pdf)
- [ ]  [WIRE: Wavelet Implicit Neural Representations](https://ar5iv.org/abs/2301.05187)
- [ ]  [Learning From Noisy Labels With Decoupled Meta Label Purifier](https://ar5iv.org/abs/2302.06810)
- [ ]  [Architectural Backdoors in Neural Networks](https://ar5iv.org/abs/2206.07840)
- [ ]  [Event-Based Shape From Polarization](https://ar5iv.org/abs/2301.06855)
- [ ]  [Deep Hashing With Minimal-Distance-Separated Hash Centers](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Hashing_With_Minimal-Distance-Separated_Hash_Centers_CVPR_2023_paper.pdf)
- [ ]  [Progressive Spatio-Temporal Alignment for Efficient Event-Based Motion Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Progressive_Spatio-Temporal_Alignment_for_Efficient_Event-Based_Motion_Estimation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/huangxueyan/PEME)
- [ ]  [Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation](https://ar5iv.org/abs/2212.00774)<br>:house:[project](https://pals.ttic.edu/p/score-jacobian-chaining)
- [ ]  [MetaCLUE: Towards Comprehensive Visual Metaphors Research](https://ar5iv.org/abs/2212.09898)<br>:house:[project](https://metaclue.github.io/)
- [ ]  [EVA: Exploring the Limits of Masked Visual Representation Learning at Scale](https://ar5iv.org/abs/2211.07636)<br>:star:[code](https://github.com/baaivision/EVA)
- [ ]  [Sliced Optimal Partial Transport](https://ar5iv.org/abs/2212.08049)
- [ ]  [Deep Learning of Partial Graph Matching via Differentiable Top-K](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Learning_of_Partial_Graph_Matching_via_Differentiable_Top-K_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Thinklab-SJTU/ThinkMatch)
- [ ]  [Unsupervised Volumetric Animation](https://ar5iv.org/abs/2301.11326)<br>:house:[project](https://snap-research.github.io/unsupervised-volumetric-animation)
- [ ]  [Passive Micron-Scale Time-of-Flight With Sunlight Interferometry](https://ar5iv.org/abs/2211.10732)
- [ ]  [Generalizable Implicit Neural Representations via Instance Pattern Composers](https://ar5iv.org/abs/2211.13223)<br>:star:[code](https://github.com/kakaobrain/ginr-ipc)
- [ ]  [On the Pitfall of Mixup for Uncertainty Calibration](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_On_the_Pitfall_of_Mixup_for_Uncertainty_Calibration_CVPR_2023_paper.pdf)
- [ ]  [UMat: Uncertainty-Aware Single Image High Resolution Material Capture](https://openaccess.thecvf.com/content/CVPR2023/papers/Rodriguez-Pardo_UMat_Uncertainty-Aware_Single_Image_High_Resolution_Material_Capture_CVPR_2023_paper.pdf)
- [ ]  [On Data Scaling in Masked Image Modeling](https://ar5iv.org/abs/2206.04664)
- [ ]  [End-to-End Vectorized HD-Map Construction With Piecewise Bezier Curve](https://openaccess.thecvf.com/content/CVPR2023/papers/Qiao_End-to-End_Vectorized_HD-Map_Construction_With_Piecewise_Bezier_Curve_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/er-muyue/BeMapNet)
- [ ]  [Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Boundary_Unlearning_Rapid_Forgetting_of_Deep_Networks_via_Shifting_the_CVPR_2023_paper.pdf)
- [ ]  [MobileOne: An Improved One millisecond Mobile Backbone](https://ar5iv.org/abs/2206.04040)<br>:star:[code](https://github.com/apple/ml-mobileone)
- [ ]  [Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization](https://ar5iv.org/abs/2211.12624)
- [ ]  [Shepherding Slots to Objects: Towards Stable and Robust Object-Centric Learning](https://ar5iv.org/abs/2303.17842)<br>:star:[code](https://github.com/object-understanding/SLASH)
- [ ]  [Residual Degradation Learning Unfolding Framework With Mixing Priors Across Spectral and Spatial for Compressive Spectral Imaging](https://ar5iv.org/abs/2211.06891)
- [ ]  [Robust and Scalable Gaussian Process Regression and Its Applications](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Robust_and_Scalable_Gaussian_Process_Regression_and_Its_Applications_CVPR_2023_paper.pdf)<br>:star:[code](github.com/YifanLu2000/Robust-Scalable-GPR)
- [ ]  [NeuralUDF: Learning Unsigned Distance Fields for Multi-View Reconstruction of Surfaces With Arbitrary Topologies](https://ar5iv.org/abs/2211.14173)<br>:house:[project](https://www.xxlong.site/NeuralUDF/)
- [ ]  [Shortcomings of Top-Down Randomization-Based Sanity Checks for Evaluations of Deep Neural Network Explanations](https://ar5iv.org/abs/2211.12486)
- [ ]  [Alias-Free Convnets: Fractional Shift Invariance via Polynomial Activations](https://ar5iv.org/abs/2303.08085)<br>:star:[code](https://github.com/hmichaeli/alias_free_convnets/)
- [ ]  [Multiplicative Fourier Level of Detail](https://openaccess.thecvf.com/content/CVPR2023/papers/Dou_Multiplicative_Fourier_Level_of_Detail_CVPR_2023_paper.pdf)
- [ ]  [VGFlow: Visibility guided Flow Network for Human Reposing](https://ar5iv.org/abs/2211.08540)
- [ ]  [Neural Dependencies Emerging From Learning Massive Categories](https://ar5iv.org/abs/2211.12339)
- [ ]  [MaLP: Manipulation Localization Using a Proactive Scheme](https://ar5iv.org/abs/2303.16976)<br>:house:[project](http://www.github.com/vishal3477/pro_loc)
- [ ]  [Efficient Robust Principal Component Analysis via Block Krylov Iteration and CUR Decomposition](https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_Efficient_Robust_Principal_Component_Analysis_via_Block_Krylov_Iteration_and_CVPR_2023_paper.pdf)
- [ ]  [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://ar5iv.org/abs/2301.00808)<br>:star:[code](https://github.com/facebookresearch/ConvNeXt-V2)
- [ ]  [Learning 3D Representations From 2D Pre-Trained Models via Image-to-Point Masked Autoencoders](https://ar5iv.org/abs/2212.06785)<br>:star:[code](https://github.com/ZrrSkywalker/I2P-MAE)
- [ ]  [MEGANE: Morphable Eyeglass and Avatar Network](https://ar5iv.org/abs/2302.04868)<br>:house:[project](https://junxuan-li.github.io/megane/)
- [ ]  [Solving relaxations of MAP-MRF problems: Combinatorial in-face Frank-Wolfe directions](https://ar5iv.org/abs/2010.09567)
- [ ]  [EXCALIBUR: Encouraging and Evaluating Embodied Exploration](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_EXCALIBUR_Encouraging_and_Evaluating_Embodied_Exploration_CVPR_2023_paper.pdf)
- [ ]  [Learning To Predict Scene-Level Implicit 3D From Posed RGBD Data](https://openaccess.thecvf.com/content/CVPR2023/papers/Kulkarni_Learning_To_Predict_Scene-Level_Implicit_3D_From_Posed_RGBD_Data_CVPR_2023_paper.pdf)
- [ ]  [SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries](https://ar5iv.org/abs/2302.12828)<br>:house:[project](http://bit.ly/splinecam)
- [ ]  [Learning Neural Parametric Head Models](https://ar5iv.org/abs/2212.02761)<br>:house:[project](https://simongiebenhain.github.io/NPHM)
- [ ]  [Integral Neural Networks](https://openaccess.thecvf.com/content/CVPR2023/papers/Solodskikh_Integral_Neural_Networks_CVPR_2023_paper.pdf)
- [ ]  [Simulated Annealing in Early Layers Leads to Better Generalization](https://ar5iv.org/abs/2304.04858)
- [ ]  [Fresnel Microfacet BRDF: Unification of Polari-Radiometric Surface-Body Reflection](https://ar5iv.org/abs/2212.04483)
- [ ]  [Improving Visual Representation Learning Through Perceptual Understanding](https://ar5iv.org/abs/2212.14504)
- [ ]  [Probability-Based Global Cross-Modal Upsampling for Pansharpening](https://ar5iv.org/abs/2303.13659)<br>:star:[code](https://github.com/Zeyu-Zhu/PGCU)
- [ ]  [SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SCConv_Spatial_and_Channel_Reconstruction_Convolution_for_Feature_Redundancy_CVPR_2023_paper.pdf)
- [ ]  [Megahertz Light Steering Without Moving Parts](https://openaccess.thecvf.com/content/CVPR2023/papers/Pediredla_Megahertz_Light_Steering_Without_Moving_Parts_CVPR_2023_paper.pdf)
- [ ]  [TempSAL - Uncovering Temporal Information for Deep Saliency Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Aydemir_TempSAL_-_Uncovering_Temporal_Information_for_Deep_Saliency_Prediction_CVPR_2023_paper.pdf)<br>:house:[project](https://ivrl.github.io/Tempsal/)
- [ ]  [Affection: Learning Affective Explanations for Real-World Visual Data](https://ar5iv.org/abs/2210.01946)<br>:house:[project](https://affective-explanations.org/)
- [ ]  [Metadata-Based RAW Reconstruction via Implicit Neural Functions](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Metadata-Based_RAW_Reconstruction_via_Implicit_Neural_Functions_CVPR_2023_paper.pdf)
- [ ]  [Coaching a Teachable Student](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Coaching_a_Teachable_Student_CVPR_2023_paper.pdf)
- [ ]  [Progressive Transformation Learning for Leveraging Virtual Images in Training](https://ar5iv.org/abs/2211.01778)
- [ ]  [NIRVANA: Neural Implicit Representations of Videos with Adaptive Networks and Autoregressive Patch-wise Modeling](https://ar5iv.org/abs/2212.14593)
- [ ]  [Spatial-Temporal Concept Based Explanation of 3D ConvNets](https://ar5iv.org/abs/2206.05275)
- [ ]  [Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability](https://ar5iv.org/abs/2207.09615)<br>:star:[code](https://github.com/princetonvisualai/OverlookedFactors)
- [ ]  [Neural Fourier Filter Bank](https://ar5iv.org/abs/2212.01735)<br>:star:[code](https://github.com/ubc-vision/NFFB)
- [ ]  [ECON: Explicit Clothed Humans Optimized via Normal Integration](https://ar5iv.org/abs/2212.07422)<br>:star:[code](https://github.com/YuliangXiu/ECON)
- [ ]  [Autonomous Manipulation Learning for Similar Deformable Objects via Only One Demonstration](https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Autonomous_Manipulation_Learning_for_Similar_Deformable_Objects_via_Only_One_CVPR_2023_paper.pdf)
- [ ]  [Plateau-Reduced Differentiable Path Tracing](https://ar5iv.org/abs/2211.17263)<br>:house:[project](https://mfischer-ucl.github.io/prdpt/)
- [ ]  [Test Time Adaptation With Transformation Invariance](https://openaccess.thecvf.com/content/CVPR2023/papers/Nguyen_TIPI_Test_Time_Adaptation_With_Transformation_Invariance_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/atuannguyen/TIPI)
- [ ]  [Learning To Exploit the Sequence-Specific Prior Knowledge for Image Processing Pipelines Optimization](https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Learning_To_Exploit_the_Sequence-Specific_Prior_Knowledge_for_Image_Processing_CVPR_2023_paper.pdf)
- [ ]  [Deep Fair Clustering via Maximizing and Minimizing Mutual Information: Theory, Algorithm and Metric](https://ar5iv.org/abs/2209.12396)<br>:house:[project](https://pengxi.me/)
- [ ]  [CUDA: Convolution-based Unlearnable Datasets](https://ar5iv.org/abs/2303.04278)
- [ ]  [Efficient On-Device Training via Gradient Filtering](https://ar5iv.org/abs/2301.00330)
- [ ]  [Transfer Knowledge From Head to Tail: Uncertainty Calibration Under Long-Tailed Distribution](https://ar5iv.org/abs/2304.06537)
- [ ]  [Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning](https://ar5iv.org/abs/2206.12126)
- [ ]  [Disentangled Representation Learning for Unsupervised Neural Quantization](https://openaccess.thecvf.com/content/CVPR2023/papers/Noh_Disentangled_Representation_Learning_for_Unsupervised_Neural_Quantization_CVPR_2023_paper.pdf)
- [ ]  [DA Wand: Distortion-Aware Selection Using Neural Mesh Parameterization](https://ar5iv.org/abs/2212.06344)<br>:star:[code](https://github.com/threedle/DA-Wand)<br>:house:[project](https://threedle.github.io/DA-Wand/)
- [ ]  [On Distillation of Guided Diffusion Models](https://ar5iv.org/abs/2210.03142)
- [ ]  [Putting People in Their Place: Affordance-Aware Human Insertion Into Scenes](https://ar5iv.org/abs/2304.14406)<br>:star:[code](https://sumith1896.github.io/affordance-insertion/)
- [ ]  [K-Planes: Explicit Radiance Fields in Space, Time, and Appearance](https://openaccess.thecvf.com/content/CVPR2023/papers/Fridovich-Keil_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance_CVPR_2023_paper.pdf)<br>:house:[project](sarafridov.github.io/K-Plane)
- [ ]  [Understanding Masked Autoencoders via Hierarchical Latent Variable Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Understanding_Masked_Autoencoders_via_Hierarchical_Latent_Variable_Models_CVPR_2023_paper.pdf)
- [ ]  [Co-Training 2L Submodels for Visual Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Touvron_Co-Training_2L_Submodels_for_Visual_Recognition_CVPR_2023_paper.pdf)
- [ ]  [Masked Images Are Counterfactual Samples for Robust Fine-Tuning](https://ar5iv.org/abs/2303.03052)<br>:star:[code](https://github.com/Coxy7/robust-finetuning)
- [ ]  [Learning Customized Visual Models With Retrieval-Augmented Knowledge](https://ar5iv.org/abs/2301.07094)
- [ ]  [A Unified Spatial-Angular Structured Light for Single-View Acquisition of Shape and Reflectance](https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_A_Unified_Spatial-Angular_Structured_Light_for_Single-View_Acquisition_of_Shape_CVPR_2023_paper.pdf)
- [ ]  [PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery](https://ar5iv.org/abs/2212.05590)<br>:star:[code](https://github.com/sheng-eatamath/PromptCAL)
- [ ]  [Reproducible Scaling Laws for Contrastive Language-Image Learning](https://ar5iv.org/abs/2212.07143)<br>:star:[code](https://github.com/LAION-AI/scaling-laws-openclip)
- [ ]  [Intrinsic Physical Concepts Discovery With Object-Centric Predictive Models](https://ar5iv.org/abs/2303.01869)
- [ ]  [Invertible Neural Skinning](https://ar5iv.org/abs/2302.09227)<br>:house:[project](https://yashkant.github.io/invertible-neural-skinning/)
- [ ]  [Multi-Object Manipulation via Object-Centric Neural Scattering Functions](https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Multi-Object_Manipulation_via_Object-Centric_Neural_Scattering_Functions_CVPR_2023_paper.pdf)
- [ ]  [Fair Scratch Tickets: Finding Fair Sparse Networks Without Weight Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Fair_Scratch_Tickets_Finding_Fair_Sparse_Networks_Without_Weight_Training_CVPR_2023_paper.pdf)
- [ ]  [Backdoor Cleansing With Unlabeled Data](https://ar5iv.org/abs/2211.12044)<br>:star:[code](https://github.com/luluppang/BCU)
- [ ]  [Full or Weak Annotations? An Adaptive Strategy for Budget-Constrained Annotation Campaigns](https://ar5iv.org/abs/2303.11678)
- [ ]  [Extracting Class Activation Maps From Non-Discriminative Features As Well](https://ar5iv.org/abs/2303.10334)
- [ ]  [Executing Your Commands via Motion Diffusion in Latent Space](https://ar5iv.org/abs/2212.04048)
- [ ]  [Chat2Map: Efficient Scene Mapping From Multi-Ego Conversations](https://ar5iv.org/abs/2301.02184)<br>:house:[project](http://vision.cs.utexas.edu/projects/chat2map)
- [ ]  [Learning To Generate Image Embeddings With User-Level Differential Privacy](https://ar5iv.org/abs/2211.10844)
- [ ]  [Revisiting the Stack-Based Inverse Tone Mapping](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Revisiting_the_Stack-Based_Inverse_Tone_Mapping_CVPR_2023_paper.pdf)
- [ ]  [PACO: Parts and Attributes of Common Objects](https://ar5iv.org/abs/2301.01795)<br>:star:[code](https://github.com/facebookresearch/paco)
- [ ]  [Teacher-Generated Spatial-Attention Labels Boost Robustness and Accuracy of Contrastive Models](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf)
- [ ]  [A General Regret Bound of Preconditioned Gradient Method for DNN Training](https://openaccess.thecvf.com/content/CVPR2023/papers/Yong_A_General_Regret_Bound_of_Preconditioned_Gradient_Method_for_DNN_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Yonghongwei/AdaBK)
- [ ]  [A Practical Upper Bound for the Worst-Case Attribution Deviations](https://ar5iv.org/abs/2303.00340)
- [ ]  [Perception and Semantic Aware Regularization for Sequential Confidence Calibration](https://ar5iv.org/abs/2305.19498)<br>:star:[code](https://github.com/husterpzh/PSSR)
- [ ]  [Deep Random Projector: Accelerated Deep Image Prior](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Deep_Random_Projector_Accelerated_Deep_Image_Prior_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/sun- umn/DeepRandom-Projector)
- [ ]  [Bias Mimicking: A Simple Sampling Approach for Bias Mitigation](https://ar5iv.org/abs/2209.15605)<br>:star:[code](https://github.com/mqraitem/Bias-Mimicking)
- [ ]  [DeCo: Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-To-Fine Contrastive Ranking](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_DeCo_Decomposition_and_Reconstruction_for_Compositional_Temporal_Grounding_via_Coarse-To-Fine_CVPR_2023_paper.pdf)
- [ ]  [Structured Kernel Estimation for Photon-Limited Deconvolution](https://ar5iv.org/abs/2303.03472)<br>:star:[code](https://github.com/sanghviyashiitb/structured-kernel-cvpr23)
- [ ]  [FlexiViT: One Model for All Patch Sizes](https://ar5iv.org/abs/2212.08013)<br>:star:[code](https://github.com/google-research/big_vision)
- [ ]  [BiasBed - Rigorous Texture Bias Evaluation](https://openaccess.thecvf.com/content/CVPR2023/papers/Kalischek_BiasBed_-_Rigorous_Texture_Bias_Evaluation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/D1noFuzi/BiasBed)
- [ ]  [GeoLayoutLM: Geometric Pre-Training for Visual Information Extraction](https://ar5iv.org/abs/2304.10759)<br>:star:[code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/GeoLayoutLM)
- [ ]  [Finding Geometric Models by Clustering in the Consensus Space](https://openaccess.thecvf.com/content/CVPR2023/papers/Barath_Finding_Geometric_Models_by_Clustering_in_the_Consensus_Space_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/danini/clustering-in-consensus-space)
- [ ]  [Hierarchical Neural Memory Network for Low Latency Event Processing](https://openaccess.thecvf.com/content/CVPR2023/papers/Hamaguchi_Hierarchical_Neural_Memory_Network_for_Low_Latency_Event_Processing_CVPR_2023_paper.pdf)<br>:house:[project](https://hamarh.github.io/)
- [ ]  [Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries](https://ar5iv.org/abs/2211.15658)<br>:star:[code](https://github.com/ywyue/RoomFormer)
- [ ]  [PointConvFormer: Revenge of the Point-Based Convolution](https://ar5iv.org/abs/2208.02879)
- [ ]  [A Practical Stereo Depth System for Smart Glasses](https://ar5iv.org/abs/2211.10551)
- [ ]  [Differentiable Shadow Mapping for Efficient Inverse Graphics](https://openaccess.thecvf.com/content/CVPR2023/papers/Worchel_Differentiable_Shadow_Mapping_for_Efficient_Inverse_Graphics_CVPR_2023_paper.pdf)
- [ ]  [Multi Domain Learning for Motion Magnification](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_Multi_Domain_Learning_for_Motion_Magnification_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/jasdeep-singh-007/Multi-Domain-Learning-for-Motion-Magnification)
- [ ]  [Re-Thinking Model Inversion Attacks Against Deep Neural Networks](https://ar5iv.org/abs/2304.01669)<br>:star:[code](https://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/)
- [ ]  [DexArt: Benchmarking Generalizable Dexterous Manipulation With Articulated Objects](https://ar5iv.org/abs/2305.05706)<br>:house:[project](https://www.chenbao.tech/dexart/)
- [ ]  [Two-View Geometry Scoring Without Correspondences](https://openaccess.thecvf.com/content/CVPR2023/papers/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.pdf)<br>:house:[project](http://www.github.com/nianticlabs/scoring-without-correspondences)
- [ ]  [ScanDMM: A Deep Markov Model of Scanpath Prediction for 360deg Images](https://openaccess.thecvf.com/content/CVPR2023/papers/Sui_ScanDMM_A_Deep_Markov_Model_of_Scanpath_Prediction_for_360deg_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/xiangjieSui/ScanDMM)
- [ ]  [Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation](https://ar5iv.org/abs/2303.01311)
- [ ]  [Analyzing Physical Impacts Using Transient Surface Wave Imaging](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Analyzing_Physical_Impacts_Using_Transient_Surface_Wave_Imaging_CVPR_2023_paper.pdf)
- [ ]  [Adaptive Global Decay Process for Event Cameras](https://openaccess.thecvf.com/content/CVPR2023/papers/Nunes_Adaptive_Global_Decay_Process_for_Event_Cameras_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/neuromorphic-paris/event)
- [ ]  [Leveraging Inter-Rater Agreement for Classification in the Presence of Noisy Labels](https://openaccess.thecvf.com/content/CVPR2023/papers/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.pdf)
- [ ]  [Towards Better Gradient Consistency for Neural Signed Distance Functions via Level Set Alignment](http://ar5iv.org/abs/2305.11601v1)<br>:star:[code](https://github.com/mabaorui/TowardsBetterGradient)
- [ ]  [Swept-Angle Synthetic Wavelength Interferometry](https://ar5iv.org/abs/2205.10655)
- [ ]  [Shape, Pose, and Appearance From a Single Image via Bootstrapped Radiance Field Inversion](https://ar5iv.org/abs/2211.11674)<br>:house:[project](https://github.com/google-research/nerf-from-image)
- [ ]  [Unlearnable Clusters: Towards Label-Agnostic Unlearnable Examples](https://ar5iv.org/abs/2301.01217)<br>:star:[code](https://github.com/jiamingzhang94/Unlearnable-Clusters)
- [ ]  [3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification](https://ar5iv.org/abs/2212.00338)
- [ ]  [EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization](https://ar5iv.org/abs/2303.01904)
- [ ]  [Text-Guided Unsupervised Latent Transformation for Multi-Attribute Image Manipulation](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Text-Guided_Unsupervised_Latent_Transformation_for_Multi-Attribute_Image_Manipulation_CVPR_2023_paper.pdf)
- [ ]  [Minimizing the Accumulated Trajectory Error To Improve Dataset Distillation](https://ar5iv.org/abs/2211.11004)<br>:star:[code](https://github.com/AngusDujw/FTD-distillation) 
- [ ]  [DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-Aware Scene Synthesis](https://ar5iv.org/abs/2212.11984)<br>:house:[project](https://snap-research.github.io/discoscene/)
- [ ]  [Virtual Occlusions Through Implicit Depth](http://ar5iv.org/abs/2305.07014v1)
- [ ]  [StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator](http://ar5iv.org/abs/2305.05445v1)<br>:star:[code](https://hangz-nju-cuhk.github.io/projects/StyleSync)
- [ ]  [Putting People in Their Place: Affordance-Aware Human Insertion into Scenes](http://ar5iv.org/abs/2304.14406v1)<br>:star:[code](https://sumith1896.github.io/affordance-insertion/)
- [ ]  [Inverting the Imaging Process by Learning an Implicit Camera Model](http://ar5iv.org/abs/2304.12748v1)<br>:star:[code](https://xhuangcv.github.io/neucam/)
- [ ]  [Visual DNA: Representing and Comparing Images using Distributions of Neuron Activations](http://ar5iv.org/abs/2304.10036v1)<br>:star:[code](https://bramtoula.github.io/vdna/)
- [ ]  [GeoLayoutLM: Geometric Pre-training for Visual Information Extraction](http://ar5iv.org/abs/2304.10759v1)<br>:star:[code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/GeoLayoutLM)
- [ ]  [Gradient-based Uncertainty Attribution for Explainable Bayesian Deep Learning](http://ar5iv.org/abs/2304.04824v1)
- [ ]  [Noisy Correspondence Learning with Meta Similarity Correction](http://ar5iv.org/abs/2304.06275v1)
- [ ]  [Efficient Multimodal Fusion via Interactive Prompting](http://ar5iv.org/abs/2304.06306v1)
- [ ]  [Representing Volumetric Videos as Dynamic MLP Maps](http://ar5iv.org/abs/2304.06717v1)<br>:star:[code](https://zju3dv.github.io/mlp_maps/)
- [ ]  [Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation](https://ar5iv.org/pdf/2303.00914.pdf)
- [ ]  [Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness](https://ar5iv.org/abs/2303.00971)
- [ ]  [DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks](https://ar5iv.org/pdf/2302.14685.pdf)
- [ ]  [EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization](https://ar5iv.org/abs/2303.01904)
- [ ]  [Intrinsic Physical Concepts Discovery with Object-Centric Predictive Models](https://ar5iv.org/pdf/2303.01869.pdf)
- [ ]  [A Meta-Learning Approach to Predicting Performance and Data Requirements](https://ar5iv.org/abs/2303.01598)
- [ ]  [Multimodal Prompting with Missing Modalities for Visual Recognition](https://ar5iv.org/abs/2303.03369)<br>:star:[code](https://github.com/YiLunLee/Missing_aware_prompts)
- [ ]  [Masked Images Are Counterfactual Samples for Robust Fine-tuning](https://ar5iv.org/abs/2303.03052)
- [ ]  [UniHCP: A Unified Model for Human-Centric Perceptions](https://ar5iv.org/abs/2303.02936)<br>:star:[code](https://github.com/OpenGVLab/UniHCP)
- [ ]  [DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network](https://ar5iv.org/abs/2303.02165)<br>:star:[code](https://github.com/alibaba/lightweight-neural-architecture-search)
- [ ]  [Progressive Open Space Expansion for Open-Set Model Attribution](https://ar5iv.org/abs/2303.06877)<br>:star:[code](https://github.com/TianyunYoung/POSE)
- [ ]  [TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets](https://ar5iv.org/abs/2303.05762)<br>:star:[code](https://github.com/chenweixin107/TrojDiff)
- [ ]  [HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining](https://ar5iv.org/abs/2303.05675)<br>:star:[code](https://github.com/OpenGVLab/HumanBench)
- [ ]  [3D Cinemagraphy from a Single Image](https://ar5iv.org/abs/2303.05724)<br>:house:[project](https://xingyi-li.github.io/3d-cinemagraphy)
- [ ]  [Masked Image Modeling with Local Multi-Scale Reconstruction](https://ar5iv.org/abs/2303.05251)<br>:star:[code](https://gitee.com/mindspore/hub/blob/fa2a3270aa36673f835e524fa55c5a4c67262eb2/mshub_res/assets/noah-cvlab/gpu/1.8/localmim_v1.0_imagenet2012.md)
- [ ]  [Revisiting Rotation Averaging: Uncertainties and Robust Losses](https://ar5iv.org/abs/2303.05195)<br>:star:[code](https://github.com/zhangganlin/GlobalSfMpy)
- [ ]  [Unifying Layout Generation with a Decoupled Diffusion Model](https://ar5iv.org/abs/2303.05049)
- [ ]  [Adversarial Counterfactual Visual Explanations](https://ar5iv.org/abs/2303.09962)<br>:star:[code](https://github.com/guillaumejs2403/ACE)
- [ ]  [Trainable Projected Gradient Method for Robust Fine-tuning](https://ar5iv.org/abs/2303.10720)<br>:star:[code](https://github.com/PotatoTian/TPGM)
- [ ]  [Partial Network Cloning](https://ar5iv.org/abs/2303.10597)<br>:star:[code](https://github.com/JngwenYe/PNCloning)
- [ ]  [Extracting Class Activation Maps from Non-Discriminative Features as well](https://ar5iv.org/abs/2303.10334)<br>:star:[code](https://github.com/zhaozhengChen/LPCAM)
- [ ]  [TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization](https://ar5iv.org/abs/2303.11135)<br>:star:[code](https://github.com/ziquanliu/CVPR2023-TWINS)
- [ ]  [Visibility Constrained Wide-band Illumination Spectrum Design for Seeing-in-the-Dark](https://ar5iv.org/abs/2303.11642)<br>:star:[code](https://github.com/MyNiuuu/VCSD)
- [ ]  [PRISE: Demystifying Deep Lucas-Kanade with Strongly Star-Convex Constraints for Multimodel Image Alignment](https://ar5iv.org/abs/2303.11526)<br>:star:[code](https://github.com/Zhang-VISLab)
- [ ]  [Boundary Unlearning](https://ar5iv.org/abs/2303.11570)<br>:house:[project](https://www.dropbox.com/s/bwu543qsdy4s32i/Boundary-Unlearning-Code.zip?dl=0)
- [ ]  [ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals](https://ar5iv.org/abs/2303.12071)
- [ ]  [VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions](http://ar5iv.org/abs/2303.12675v1)
- [ ]  [Learning a Depth Covariance Function](http://ar5iv.org/abs/2303.12157v1)<br>:star:[code](https://edexheim.github.io/depth_cov/)
- [ ]  [A Bag-of-Prototypes Representation for Dataset-Level Applications](http://ar5iv.org/abs/2303.13251v1)
- [ ]  [CrOC: Cross-View Online Clustering for Dense Visual Representation Learning](http://ar5iv.org/abs/2303.13245v1)<br>:star:[code](https://github.com/stegmuel/CrOC)
- [ ]  [Exploring Structured Semantic Prior for Multi Label Recognition with Incomplete Labels](http://ar5iv.org/abs/2303.13223v1)<br>:star:[code](https://github.com/jameslahm/SCPNet)
- [ ]  [Marching-Primitives: Shape Abstraction from Signed Distance Function](http://ar5iv.org/abs/2303.13190v1)<br>:star:[code](https://github.com/ChirikjianLab/Marching-Primitives.git)
- [ ]  [Robust Generalization against Photon-Limited Corruptions via Worst-Case Sharpness Minimization](http://ar5iv.org/abs/2303.13087v1)
- [ ]  [Robust Test-Time Adaptation in Dynamic Scenarios](http://ar5iv.org/abs/2303.13899v1)<br>:star:[code](https://github.com/BIT-DA/RoTTA)
- [ ]  [Enhancing Multiple Reliability Measures via Nuisance-extended Information Bottleneck](http://ar5iv.org/abs/2303.14096v1)<br>:star:[code](https://github.com/jh-jeong/nuisance_ib)
- [ ]  [IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients](http://ar5iv.org/abs/2303.14242v1)
- [ ]  [Compacting Binary Neural Networks by Sparse Kernel Selection](http://ar5iv.org/abs/2303.14470v1)
- [ ]  [PDPP:Projected Diffusion for Procedure Planning in Instructional Videos](http://ar5iv.org/abs/2303.14676v1)<br>:star:[code](https://github.com/MCG-NJU/PDPP)
- [ ]  [Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph](http://ar5iv.org/abs/2303.15266v1)<br>:star:[code](https://github.com/zhourixin/bronze-Ding)
- [ ]  [Quantum Multi-Model Fitting](http://ar5iv.org/abs/2303.15444v1)<br>:star:[code](https://github.com/FarinaMatteo/qmmf)
- [ ]  [Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe Based Motion Interpolation](http://ar5iv.org/abs/2303.14926v1)
- [ ]  [PMatch: Paired Masked Image Modeling for Dense Geometric Matching](http://ar5iv.org/abs/2303.17342v1)<br>:star:[code](https://github.com/ShngJZ/PMatch)
- [ ]  [ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing](http://ar5iv.org/abs/2303.17096v1)<br>:star:[code](https://github.com/alibaba/easyrobust)
- [ ]  [Single Image Depth Prediction Made Better: A Multivariate Gaussian Take](http://ar5iv.org/abs/2303.18164v1)
- [ ]  [Why is the winner the best?](http://ar5iv.org/abs/2303.17719v1)
- [ ]  [Disorder-invariant Implicit Neural Representation](http://ar5iv.org/abs/2304.00837v1)<br>:star:[code](https://ezio77.github.io/DINER-website/)
- [ ]  [HypLiLoc: Towards Effective LiDAR Pose Regression with Hyperbolic Fusion](http://ar5iv.org/abs/2304.00932v1)<br>:star:[code](https://github.com/sijieaaa/HypLiLoc)
- [ ]  [Enhancing Deformable Local Features by Jointly Learning to Detect and Describe Keypoints](http://ar5iv.org/abs/2304.00583v1)<br>:house:[project](https://verlab.dcc.ufmg.br/descriptors/dalf_cvpr23)
- [ ]  [SMPConv: Self-moving Point Representations for Continuous Convolution](http://ar5iv.org/abs/2304.02330v1)<br>:star:[code](https://github.com/sangnekim/SMPConv)
- [ ]  [VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution](http://ar5iv.org/abs/2304.01434v1)<br>:star:[code](https://github.com/jaeill/CVPR23-VNE)
- [ ]  [Delving into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling](http://ar5iv.org/abs/2304.03937v1)
- [ ]  [Wide-Angle Rectification via Content-Aware Conformal Mapping](https://ar5iv.org/abs/2303.16624)<br>:house:[project](https://astr2023.github.io/)
- [ ]  [Large-capacity and Flexible Video Steganography via Invertible Neural Network](http://ar5iv.org/abs/2304.12300v1)<br>:star:[code](https://github.com/MC-E/LF-VSN)
- [ ]  [SketchXAI: A First Look at Explainability for Human Sketches](http://ar5iv.org/abs/2304.11744v1)<br>:star:[code](https://sketchxai.github.io)
- [ ]  [Hard Patches Mining for Masked Image Modeling](http://ar5iv.org/abs/2304.05919v1)<br>:thumbsup:[CVPR 2023 | HPM：在掩码学习中挖掘困难样本，带来稳固性能提升！](https://mp.weixin.qq.com/s/I9XGSDwkkmkmIE4tBaIK3g)
- [ ]  [Learning Geometry-aware Representations by Sketching](http://ar5iv.org/abs/2304.08204v1)
- [ ]  [DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training](http://ar5iv.org/abs/2304.08480v1)<br>:star:[code](https://github.com/IDEA-Research/DisCo-CLIP)
- [ ]  [Investigating the Nature of 3D Generalization in Deep Neural Networks](https://ar5iv.org/abs/2304.09358)<br>:star:[code](https://github.com/shoaibahmed/investigating_3d_generalization.git)
- [ ]  [EC^2: Emergent Communication for Embodied Control](http://ar5iv.org/abs/2304.09448v1)
- [ ]  [Generalizing Dataset Distillation via Deep Generative Prior](https://ar5iv.org/abs/2305.01649)<br>:star:[code](https://github.com/GeorgeCazenavette/glad)<br>:house:[project](https://georgecazenavette.github.io/glad)
- [ ]  [Learning Locally Editable Virtual Humans](https://ar5iv.org/abs/2305.00121)<br>:house:[project](https://custom-humans.github.io)
- [ ]  [Class-Balancing Diffusion Models](https://ar5iv.org/abs/2305.00562)
- [ ]  [SFD2: Semantic-guided Feature Detection and Description](https://ar5iv.org/abs/2304.14845)<br>:star:[code](https://github.com/feixue94/sfd2)
- [ ]  [Computational Flash Photography Through Intrinsics](https://openaccess.thecvf.com/content/CVPR2023/papers/Maralan_Computational_Flash_Photography_Through_Intrinsics_CVPR_2023_paper.pdf)
- [ ]  [Deep Graph Reprogramming](https://ar5iv.org/abs/2304.14593)
- [ ]  [LayoutDM: Transformer-based Diffusion Model for Layout Generation](http://ar5iv.org/abs/2305.02567v1)
- [ ]  [MetaViewer: Towards a Unified Multi-View Representation](https://ar5iv.org/abs/2303.06329)
- [ ]  [Learning Compact Representations for LiDAR Completion and Generation](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Learning_Compact_Representations_for_LiDAR_Completion_and_Generation_CVPR_2023_paper.pdf)<br>:house:[project](https://waabi.ai/research/ultralidar/)
- [ ]  多模态
  - [ ]  [Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning](https://ar5iv.org/abs/2303.05952)
  - [ ]  [PMR: Prototypical Modal Rebalance for Multimodal Learning](http://ar5iv.org/abs/2211.07089)
  - [ ]  [Multi-Modal Learning With Missing Modality via Shared-Specific Feature Modelling](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multi-Modal_Learning_With_Missing_Modality_via_Shared-Specific_Feature_Modelling_CVPR_2023_paper.pdf)
  - [ ]  [Towards Flexible Multi-Modal Document Models](http://ar5iv.org/abs/2303.18248)
  - [ ]  [Multi-Modal Representation Learning With Text-Driven Soft Masks](http://ar5iv.org/abs/2304.00719)
  - [ ]  [Align and Attend: Multimodal Summarization With Dual Contrastive Losses](https://ar5iv.org/abs/2303.07284)<br>:house:[project](https://boheumd.github.io/A2Summ/)
  - [ ]  [Improving Zero-Shot Generalization and Robustness of Multi-Modal Models](https://ar5iv.org/abs/2212.01758)<br>:star:[code](https://github.com/gyhandy/Hierarchy-CLIP)
  - [ ]  [BEV-Guided Multi-Modality Fusion for Driving Perception](https://openaccess.thecvf.com/content/CVPR2023/papers/Man_BEV-Guided_Multi-Modality_Fusion_for_Driving_Perception_CVPR_2023_paper.pdf)<br>:star:[code](https://yunzeman.github.io/BEVGuide)
  - [ ]  [BiCro: Noisy Correspondence Rectification for Multi-modality Data via Bi-directional Cross-modal Similarity Consistency](http://ar5iv.org/abs/2303.12419v1)
  - [ ]  [Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information](https://ar5iv.org/abs/2211.09807)<br>:star:[code](https://github.com/OpenGVLab/M3I-Pretraining)
  - [ ]  [Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-commerce](http://ar5iv.org/abs/2304.02853v1)多模态预训练
  - [ ]  [MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning](https://ar5iv.org/abs/2304.08028)<br>:star:[code](https://github.com/shicaiwei123/MMANet)
- [ ]  Affordance Learning(启示学习)
  - [ ]  [Leverage Interactive Affinity for Affordance Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Leverage_Interactive_Affinity_for_Affordance_Learning_CVPR_2023_paper.pdf)<br>:star:[code](github.com/lhc1224/PIAL-Net)
- [ ]  Feature Matching(特征匹配)
  - [ ]  [PATS: Patch Area Transportation with Subdivision for Local Feature Matching](https://ar5iv.org/abs/2303.07700)<br>:house:[project](https://zju3dv.github.io/pats/)
  - [ ]  [Adaptive Spot-Guided Transformer for Consistent Local Feature Matching](http://ar5iv.org/abs/2303.16624v1)<br>:star:[code](https://astr2023.github.io)<br>:star:[code](https://astr2023.github.io/)
  - [ ]  [Adaptive Assignment for Geometry Aware Local Feature Matching](https://ar5iv.org/abs/2207.08427)<br>:star:[code](https://github.com/AbyssGaze/AdaMatcher)特征匹配
  - [ ]  [DKM: Dense Kernelized Feature Matching for Geometry Estimation](https://openaccess.thecvf.com/content/CVPR2023/papers/Edstedt_DKM_Dense_Kernelized_Feature_Matching_for_Geometry_Estimation_CVPR_2023_paper.pdf)<br>:star:[code](https://github.com/Parskatt/DKM)
- [ ]  紫外线预测
  - [ ]  [Normal-Guided Garment UV Prediction for Human Re-Texturing](https://ar5iv.org/abs/2303.06504)
- [ ]  vector quantization(矢量量化)
  - [ ]  [Vector Quantization With Self-Attention for Quality-Independent Representation Learning](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vector_Quantization_With_Self-Attention_for_Quality-Independent_Representation_Learning_CVPR_2023_paper.pdf)<br>:house:[project](https://see.xidian.edu.cn/faculty/wsdong/Projects/VQSA.htm)
### 扫码CV君微信(注明：CVPR)入微信交流群：
![9475fa20fd5e95235d9fa23ae9587a2](https://user-images.githubusercontent.com/62801906/156720309-de92964f-a6da-464a-b21f-cfb270c13e27.png)

